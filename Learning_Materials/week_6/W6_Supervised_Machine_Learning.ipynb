{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervised Machine Learning\n",
    "### Artificial Intelligence 1, Week 6\n",
    "\n",
    "\n",
    "### Learning models for **classification** or **regression** \n",
    "from a set of labelled instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# This week\n",
    "Learning outcomes:\n",
    "\n",
    "- Identify formulate and apply the basic processes of supervised machine learning\n",
    "- Understand the role of data in estimating accuracy \n",
    "\n",
    "Videos:\n",
    "- Basic model building process: train and test \n",
    "- Types of model: instance-based ( e.g. kNN) vs explicit (e.g. decision trees,rules, ...) \n",
    "- Example:   greedy rule induction as compared to expert system\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning Paradigm\n",
    "- Completely different paradigm to symbolic AI\n",
    "- Create a system with the ability to learn\n",
    "- Present the system with series of examples\n",
    "- System builds up its own model of the world\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<img src=\"figures/PersonThinkingAboutDogs.png\" style=\"float:left\"><img src=\"figures/idealisedDog.png\" style=\"float:right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Video (6:52): Hello World of Machine Learning Recipes\n",
    "\n",
    "\n",
    "https://youtu.be/cKxRvEZd3Mw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## It's all about the data\n",
    "- Computers cannot experience artefacts of the real world directly\n",
    "- Instead they just deal with a few variables that represent them\n",
    "- ML algorithms learn from a “training set” containing digital representations of examples to learn from\n",
    "- Outcomes depend entirely on:\n",
    " - What you choose to measure\n",
    " - And how representative your training set is\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More formally\n",
    "\n",
    "We have a set of *n* examples., and for each one  we have: \n",
    "- a value for each of *f* features \n",
    "- a label\n",
    "\n",
    "The data set *X* is usually 2-D array of *n* rows and *f* columns.   \n",
    "\n",
    "The label set *y* is usually a 1-D array with *n* entries.   \n",
    "\n",
    "For now we'll assume the features are *continuous* (e.g. floating point values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If the label comes from a discrete unordered set of *m* values, e.g.  (\"Orange\",\"Apple\" \"Banana\"): \n",
    "- we have a **Classification** problem. $M: \\mathcal{R}^f \\rightarrow \\{1,\\ldots,m\\}$\n",
    "- The learned model *M*  is a mapping from a *f*-dimensional continuous space (the feature values) onto a finite set\n",
    " \n",
    "\n",
    "If the label is an ordinal value (integer,    floating point):\n",
    "- we have a **Regression** problem. $ M:\\mathcal{R}^f \\rightarrow \\mathcal{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The  Supervised Learning Workflow\n",
    "<div>\n",
    "<div width=40% style=\"float:left\">\n",
    "    <p>This diagram assumes you are trying out:</p>\n",
    "<ul>\n",
    "    <li> more than one type of algorithm </li>\n",
    "    <li> or a choice of parameter settings</li>\n",
    "    <li> or when to stop training <br>\n",
    "        your algorithm</li>\n",
    "    </ul>\n",
    "<p>If you are just trying one algorithm<br>  \n",
    "    you can skip the validation phase</p>\n",
    "    </div>\n",
    "\n",
    "<div width=45% style=\"float:right\">    \n",
    "<img src=\"figures/ML_workflow.png\" style= \"float:right\">\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example:  Iris flowers <img src=\"figures/Iris-image.png\" style=\"float:right;width:300px\">\n",
    "- Classic Machine Learning Data set\n",
    "- 4 measurements: sepal and petal width and length\n",
    "- 50 examples  from each 3 sub-species for iris flowers\n",
    "- three class problem:\n",
    " - so for some types of algorithm have to decide whether to make  \n",
    "   a 3-way classifier or nested 1-vs-rest classifers\n",
    "- most ML classifiers can get over 90%\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "import  week6_utils as w6utils\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "iris_x,iris_y = sklearn.datasets.load_iris(return_X_y=True)\n",
    "iris_features= (\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\")\n",
    "iris_names= ['setosa','versicolor','virginica']\n",
    "title=\"Scatterplots of 2D slices through the 4D Iris data\"\n",
    "w6utils.show_scatterplot_matrix(iris_x,iris_y,iris_features,title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap so far\n",
    "Machine Learning is about learning patterns from data.  \n",
    "In supervised ML this means: \n",
    "1. **Training Data**: set of labelled examples, each characterised by values for *f* features  \n",
    "   - **X**: data - usually a 2D array with one row per example, one column for each feature  \n",
    "     (even images can be 'flattened' into this format).   \n",
    "   - **y** : the labels/target \n",
    "\n",
    "2. A supervised Machine Learning **Algorithm**\n",
    "\n",
    "3. A **performance criteria (quality)**: used to drive training and estimate quality of model.  \n",
    "Depending on the **context** this might be accuracy,  precision, recall, error rates...\n",
    "\n",
    "\n",
    "4. A **test set** to estimate the performance of the model on unseen data.  \n",
    "   If this is not available separately, have to take out some data from the training set\n",
    "   - crude way; single 70:30 train:test split,  \n",
    "     making sure you preserve the proportions of different classes\n",
    "   - better way: *N-Fold Cross Validation* [Description on Wikipedia](https://en.wikipedia.org/wiki/Cross-validation_(statistics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Important Idea!  Decision Surfaces in feature space\n",
    "<div>\n",
    "<div width=70% style=\"float:left\">\n",
    "<p>    \n",
    "Each feature defines a dimension in <em>feature space</em>.<br>  \n",
    "Each example has specific values for each feature<br>\n",
    "    - so it occupies <b>one point</b> in feature space</p>\n",
    "\n",
    "<p>The aim of our model is to let us predict labels for any item:</p>\n",
    "    <ul><li> so it puts decision boundaries into feature space </li>\n",
    "        <li>  to divide it into regions with a common label</li></ul>\n",
    "\n",
    "<p>Symbolic Reasoning:\n",
    "<ul><li> boundaries defined by our 'knowledge' </li>\n",
    "<li> so can plot without needing data! e.g. image on right</li>\n",
    "</ul></p>\n",
    "\n",
    "<p>Machine Learning: \n",
    "<ul><li> uses the training data to <b>estimate</b> where the boundaries should be.</li>\n",
    "    <li> usually by <b>searching for the set of decision boundaries</b> that minimises the number of mis-predictions</li>\n",
    "<li> So we can plot model's prediction for lots of points over a grid <br> \n",
    "    to visualise the decision surface and boundaries </li></ul></p>\n",
    "</div>\n",
    "<div  width=35% style=\"float:right\">    \n",
    "    <figure>\n",
    "        <img src=\"figures/decisionRegions.png\" style=\"width:300px;float:right\">\n",
    "        <figcaption>Decision Model for predicting outcomes<br> using  pre-2024 assessment.</figcaption>\n",
    "    </figure>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Learning Algorithms\n",
    "Typically a ML method consists of:\n",
    "\n",
    "1: A  representation for the decision boundaries\n",
    " - Each different arrangement of boundaries defines a unique model\n",
    " - Each unique model is defined by the set of values for variables specifying where the boundaries are.\n",
    " - So in the language we used in topic one, each unique model is a <b> candidate solution</b>.\n",
    " - Different types of models will have different variables.\n",
    " \n",
    "2: A learning algorithm to deciding how to change those variable values to move between models.\n",
    "- This is an <b>informed search process</b> \n",
    "    - driven by a heuristic that estimates performance in the *``real world''*.\n",
    "    - Typically this is the <b>loss</b> (total of errors) on the training set.\n",
    " - Last week we saw how the KMeans clustering algorithm uses \"local search with random restarts\"\n",
    " - Most `standard' ML algorithms use some variant of local search\n",
    "\n",
    "ML Algorithms build models in different ways\n",
    "- but they don’t care what it is they are grouping\n",
    "- and it is **meaningless** to say they “understand”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some example ML methods\n",
    "The field of ML is fast growing and contains many complex methods and representations.   \n",
    "This module focusses on a few simple ideas to give you a feel for what is out there.  \n",
    "- Instance-based learning (k-Nearest Neighbours) - this week\n",
    "- Decision trees and rule induction algorithms- this week\n",
    "- Artificial Neural Networks - weeks 7 and 8 \n",
    "\n",
    "Next year: \n",
    "- Artificial Intelligence 2:  15 credits, semester 1 (AI and \"General\" pathways) \n",
    "and in particular \n",
    "- Machine Learning: 15 credits, semester 2     ( AI pathway)\n",
    "\n",
    "will cover more algorithms in greater depth.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Instance-based Methods: Nearest Neighbour Methods\n",
    "- Do not explicitly represent class boundaries  \n",
    "  Construct them “on-the-fly” when queried\n",
    "- Store the set of training examples  \n",
    "  More efficient methods may not store all points\n",
    "- Use a metric to calculate distance between two points  \n",
    "  e.g. Euclidean (continuous), Hamming (binary), ...\n",
    "\n",
    "<img src=\"figures/kNN-steps.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-Nearest Neighbour Classification \n",
    "\n",
    "<div>\n",
    "<div width-50% style=\"float:left\">\n",
    "    <p style=\"background:lightgreen;color:black\"><b>init(neighbours=k, distance metric =d)</b>:<br>  \n",
    "        Specify <i>k</i> and a distance metric <i>d(i,j)</i> </p>\n",
    "    <p style=\"background:beige\"><b>fit(trainingData)</b>:<br>  \n",
    "      Store a local copy of the training data as two arrays:<br>  \n",
    "       model_x of shape (numTrainingItems , numFeatures),  <br>\n",
    "        model_y of shape( numTrainingItems)</p>  \n",
    "    <p style=\"background:lightblue\"><b>predict(newItems)</b>:\n",
    "        <ol style=\"background:lightblue\">\n",
    "        <li>Make 2D array <i>distances</i> of shape (num_newItems , numTrainingItems)<br>  \n",
    "            FOREACH COMBINATION of newItem i  and trainingitem j <br> \n",
    "            ...SET <i>distances[i][j] = d (i,j)</i> \n",
    "        </li>\n",
    "        <li> Make 2D array <i>votes</i> of shape(num_newItems, k)<br> \n",
    "                FOREACH newItem i <br>\n",
    "            ...Find the <i>k</i> columns of the row <i>distances[i]</i> with the smallest values<br>\n",
    "                ...Put the corresponding <i>k</i> labels from model_y into <i>votes[i]</i> \n",
    "        </li>\n",
    "        <li>Store majority votes in a  1D array <i>y_pred</i> of size <i>numToPredict</i><br>\n",
    "                FOREACH  newItem i<br>\n",
    "                ...SET<i> y_pred[i] = most_common_value(votes[i]) </i>\n",
    "        </li>\n",
    "        <li>RETURN y_pred</li>\n",
    "            </ol>\n",
    "</div>    \n",
    "<div width=35% style=\"float:right\">    \n",
    "<img src=\"figures/voronoi.png\" style=\"float:right\" width = 400 title=\"https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor\">\n",
    "<p><a href= https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor>Image of Vornoi tesselation</a> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example for K = 1 \n",
    "from sklearn.base import ClassifierMixin\n",
    "\n",
    "class Simple1NNClassifier(ClassifierMixin):\n",
    "    \"\"\" \n",
    "    Simple example class for 1-Nearest Neighbours algorithm.\n",
    "    Assumes numpy is imported as np and uses euclidean distance\n",
    "    \"\"\"    \n",
    "    def dist_a_b(self,a:np.array,b:np.array)->float:\n",
    "        \"\"\" euclidean distance between same-size vectors a and b\"\"\"\n",
    "        assert a.shape==b.shape, 'vectors not same size calculating distance'\n",
    "        return np.linalg.norm(a-b) \n",
    "    \n",
    "    def fit(self,x:np.ndarray,y:np.array):\n",
    "        \"\"\" just stores the data for k-nerarest neighbour\"\"\"\n",
    "        self.num_training_items = x.shape[0]\n",
    "        self.num_features = x.shape[1]\n",
    "        self.model_x = x\n",
    "        self.model_y = y\n",
    "        self.is_fitted_ = True\n",
    "        self.classes_=np.unique(y)\n",
    "        \n",
    "    def predict(self,new_items:np.ndarray):\n",
    "        \"\"\" makes predictions for an array of new items\"\"\"\n",
    "        num_to_predict = new_items.shape[0]\n",
    "        y_pred = np.zeros((num_to_predict),dtype=int)\n",
    "        \n",
    "        # measure distances - creates an array with numToPredict rows and num_trainItems columns\n",
    "        dist = np.zeros((num_to_predict,self.num_training_items))\n",
    "        for new_item in range(num_to_predict):\n",
    "            for stored_example in range(self.num_training_items):\n",
    "                dist[new_item][stored_example]= self.dist_a_b(new_items[new_item],\n",
    "                                                              self.model_x[stored_example ])\n",
    "        #make predictions: \n",
    "        for item_idx in range(num_to_predict):\n",
    "            y_pred[item_idx] = self.predict_one(item_idx, dist)\n",
    "        return y_pred\n",
    "    \n",
    "    def predict_one(self,item_idx:int,distances:np.ndarray):\n",
    "        \"\"\" makes a class prediction for a single new item\n",
    "        This version is just for 1 Nearest Neighbour\n",
    "        Parameters\n",
    "        ----------\n",
    "        item_idx (int): item to make prediction for - i.e. idx of row in distances matrix\n",
    "        dist (numpy ndarray): array of distances between new items (rows) and training set records(columns)\n",
    "        \"\"\"\n",
    "        # we're going to use numpy's argmin method (google it)\n",
    "        # which gives us the  get indexes of column with lowest value in an array\n",
    "        idx_of_nearest_neighbour = np.argmin (distances[item_idx])\n",
    "        return self.model_y[ idx_of_nearest_neighbour]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How does K-nearest Neighbours do on the Iris data?\n",
    "\n",
    "We'll use a function from sklearn to do our train/test split here.\n",
    "\n",
    "This is handy because it shuffles the data and has options to make sure that we keep the same proportion of different classes in our training and testing data.\n",
    "\n",
    "\n",
    "            \n",
    "           \n",
    "We'll also make a **confusion matrix** to examine the predictions it makes\n",
    "rows = target labels,  columns = predicted labels\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make train/test split \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(iris_x, iris_y, test_size=0.33,stratify=iris_y)\n",
    "\n",
    "\n",
    "my_KNN_model = Simple1NNClassifier()\n",
    "my_KNN_model.fit(train_x,train_y)\n",
    "y_pred = my_KNN_model.predict(test_x)\n",
    "print(y_pred.T) #.t turns column to row so it shows onscreen better "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## how good are these results?\n",
    "We can use a neat numpy trick to find out if the predictions are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print ( (test_y==y_pred))\n",
    "accuracy = 100* ( test_y == y_pred).sum() / test_y.shape[0]\n",
    "print(f\"Overall Accuracy = {accuracy} %\")\n",
    "\n",
    "# sklearn's built-in Confusion matrix method\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "_ = ConfusionMatrixDisplay.from_predictions(test_y, y_pred,display_labels= iris_names )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualising 1-NN if we just learn from the sepal features\n",
    "\n",
    "\n",
    "- same labels (y)\n",
    "- but training inputs  only has the first two features about the sepals\n",
    "\n",
    "we use what we learned about *numpy slicing* to  pick out\n",
    "every row but just the first two  columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# new model\n",
    "my_KNN_model2= Simple1NNClassifier()\n",
    "\n",
    "# fit the petals data\n",
    "my_KNN_model2.fit(train_x[:,:2],train_y)\n",
    "\n",
    "#make predictions for petals version of test data\n",
    "y_pred2 = my_KNN_model2.predict(test_x[:,:2])\n",
    "\n",
    "accuracy = 100* ( test_y == y_pred).sum() / test_y.shape[0]\n",
    "print(f\"Overall Accuracy in 2D = {accuracy} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig,axs= plt.subplots(ncols=2,figsize=(10,5))\n",
    "#make the visualisation of the decision boundary\n",
    "disp = DecisionBoundaryDisplay.from_estimator(\n",
    "    my_KNN_model2, train_x[:,:2], alpha=0.5,ax=axs[0],eps=0.1\n",
    ")\n",
    "disp2 = DecisionBoundaryDisplay.from_estimator(\n",
    "    my_KNN_model2, train_x[:,:2], alpha=0.5,ax=axs[1],eps=0.1\n",
    ")\n",
    "#overlay the dataset records -black/red edges for train/test data\n",
    "disp.ax_.scatter(train_x[:, 0], train_x[:,1], c=train_y,linewidth=1, edgecolor=\"k\")\n",
    "disp2.ax_.scatter(test_x[:, 0], test_x[:,1], c=test_y, edgecolor=\"r\",linewidth=1)\n",
    "disp.ax_.set_title(\"train\")\n",
    "disp2.ax_.set_title(\"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Rule Induction Algorithms\n",
    "\n",
    "## Principles\n",
    "\n",
    "### Rule Representation\n",
    "<div>\n",
    "<div width=50% style=\"float:left\">\n",
    "<p>Week 1 we introduced the idea of rules. <p>\n",
    "<ul><li>Topic 3 is about <em>Knowledge-Based systems</em><br>  \n",
    "    where <b>humans provide the rules</b> for a situation. </li>\n",
    "    <li> ML Rule Induction algorithms automate learning rules</li>\n",
    "    </ul>    \n",
    "<p> In both cases a rule has the form:</p>\n",
    "<ul>\n",
    "    <li><b>if</b> feature_n <em> comparison</em> threshold <b>then</b> prediction.</li>\n",
    "<li> with comparison  one of less than, equals, more than etc.</li>\n",
    "    <li> Illustrated on the right for the iris data</li></ul>\n",
    "    </div>\n",
    "<div width=40% style=\"float:right\">\n",
    "<img src=\"figures/rule-representation.png\" style=\"float:right\">\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "   \n",
    "### Rule Matching\n",
    "We say that a rule *covers* a training example (features, label) if\n",
    "- the example features meet the rule's _condition_\n",
    "- the rule's _action_ (prediction) matches the example's label.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Boundaries and Default Classes\n",
    "Most existing algorithms tend to use  rules built up of lots of axis-perpendicular decisions.   \n",
    "-  For example the (useless) rule  *If( petal_length > 0.3) THEN (\"Setosa\")*   \n",
    "  Draws a line through feature space, at right angles to the petal_length axis, crossing it at 0.3.  \n",
    "  Puts the label \"setosa\" on one side, nothing on the other\n",
    "\n",
    "- As more rules are added, the model effectively builds labelled (hyper) boxes in space.  \n",
    "  \n",
    "- Rest of 'decision space' is given with the default (majority) label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Making a Prediction with a rule based model\n",
    "Lets assume we have 3 rules (so this example fits on a slide)\n",
    "and we want to make a predcition for a new item\n",
    "\n",
    "```python\n",
    "  if matches(rule0_condition, new_item):\n",
    "     return rule0_prediction\n",
    "  elif if matches(rule1_condition, new_item):\n",
    "     return rule1_prediction\n",
    "  elif if matches(rule2_condition, new_item):\n",
    "     return rule2_prediction\n",
    "  else:\n",
    "     return default class \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Relationship of model fitting to search\n",
    "This learning or fitting a model to the data can be seen as a search process\n",
    "\n",
    "*italics below refer to the search framework we used last topic*\n",
    "\n",
    "To do that we need to have:\n",
    "1. A representation for rules  \n",
    "   *`mycandidatesolution.variable_values` is a list of rules*   \n",
    "   *and each rule is a tuple of four values*\n",
    "2. A way of assigning \"goodness\" to (sets of) rules.  \n",
    "   *a dataset would be held in an instance of a `RuleInductionProblem`*  \n",
    "   *evaluate() breaks into two stages: make predictions, then score them  against actual values*\n",
    "3. A way of algorithmically generating possible rules  \n",
    "   We have fixed sets of features,operators,outputs,  \n",
    "   We can **discretize** the thresholds for each feature    \n",
    "   So we can use nested loops (or a neat python trick) to create all possible rules.  \n",
    "   *this would be the value_set for the problem instance*\n",
    "   \n",
    "Then we could use our local search algorithm to learn a set of rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Greedy rule induction: keep choosing the next best rule\n",
    "\n",
    "We can exploit the ability to generate rules algorithmically to make a simple Machine Learning algorithm that **automatically** learns rules, using a greedy constructive hill climbing approach:  \n",
    "\n",
    "This is a **generate-and-test** approach for search the space of all possible models, that repeatedly takes the \"next-best\" rule to create a rule-set.     \n",
    "- Note that this method can be easily out-performed by more sophisticated approaches.\n",
    "\n",
    "In our framework `RuleInductionProblem.evaluate()`:\n",
    "- takes a list of rules **not including the default rule**\n",
    "- uses them to make a predictions for the training data\n",
    "- then compares each prediction to the actual label for that training item\n",
    " - returns:\n",
    "   - -1 if the model makes any incorrect predictions\n",
    "   - otherwise the number of correct predictions\n",
    "Note that this does not assume we can correctly classify each trainig item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees can capture rules and more\n",
    "<div>\n",
    "    <p> <b>Basic idea:</b> divide input space using a set of axis-parallel lines \n",
    "        and <b>\"grow\"</b> a tree via these steps.</p> \n",
    "    <ol>\n",
    "        <li>Start with single node that predicts majority class label.</li>  \n",
    "        <li>Loop over every leaf node:\n",
    "        <ul>  \n",
    "          <li>measure (in some way) the  \"information content\"  \n",
    "             of the data that arrives at that node</li>  \n",
    "         <li>for each possible each way of splitting data  you could put into that node\n",
    "          <ul>\n",
    "               <li>  measure and add the \"information content\" of the child nodes created by the split </li>\n",
    "               <li> subtract information content of parent</li>\n",
    "               <li> result is the <i> gain</i> in information content given by split</li>\n",
    "              <li> update stored \"best split\" if appropriate</li>\n",
    "           </ul>\n",
    "        </li>\n",
    "        <li> If the  \"best\" split is above some threshold then change the leaf node to an interior node with the <i>best</i> condition</li>   \n",
    "            <li> If <i>termination criteria</i> not met goto step 2 </li>\n",
    "                     </ol>\n",
    "<p>This criteria for adding nodes is different to the rule induction algorithm, and gives you different trees</p>\n",
    "<p> <b>Interior nodes</b> are equivalent to conditions in a rule  </p>\n",
    "            <p><b>Leaf Nodes</b> are the outputs (actions of a rule):</p>\n",
    "            <ul>\n",
    "                <li>class labels (classification tree), </li>\n",
    "                <li>equation for predicting values (regression tree)</li>\n",
    "            </ul>\n",
    "        </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision trees for our example datasets\n",
    "using code from sklearn   \n",
    "`class sklearn.tree.DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None,  min_samples_leaf=1, ...)`\n",
    "\n",
    "Like all sklearn models it implements a fit() and predict() method\n",
    "\n",
    "Note the default criteria for splitting is the 'gini' index = there are many available, this is a popular one.   \n",
    "Note 2: Using *best* as the splitter means the search algorithm doing greedy constructive local search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn import tree\n",
    "\n",
    "fig,ax = plt.subplots(1,3,figsize=(15,5))\n",
    "fig.suptitle(\"Illustration of how Decision Trees select and insert nodes to increase data purity\")\n",
    "for depth in range (1,4):\n",
    "    my_dt_model = DecisionTreeClassifier(random_state=1234, max_depth=depth,min_samples_leaf=1)\n",
    "    my_dt_model.fit(train_x,train_y)\n",
    "    _ = tree.plot_tree(my_dt_model, feature_names=iris_features, class_names= iris_names,filled=True,ax=ax[depth-1])\n",
    "    ax[depth-1].set_title(\"Depth \"+str(depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Making a new tree using just the sepal features and visualising the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#make,  fit and evaluate new model\n",
    "two_D_DT_model = DecisionTreeClassifier(max_depth=4,min_samples_leaf=2)\n",
    "two_D_DT_model.fit(train_x[:,:2],train_y)\n",
    "train_accuracy=two_D_DT_model.score(train_x[:,:2],train_y)\n",
    "test_accuracy=two_D_DT_model.score(test_x[:,:2],test_y)\n",
    "\n",
    "#call sklearn's built in visualisation\n",
    "fig2,axs2=plt.subplots(ncols=2,figsize=(15,5))\n",
    "_ = tree.plot_tree(two_D_DT_model, \n",
    "                   feature_names=iris_features, \n",
    "                   class_names= iris_names,\n",
    "                   filled=True,ax=axs2[0])\n",
    "\n",
    "#make the visualisation of the decision boundary\n",
    "disp3 = DecisionBoundaryDisplay.from_estimator(\n",
    "    two_D_DT_model, train_x[:,:2], ax=axs2[1],\n",
    "    alpha=0.5,eps=0.5\n",
    ")\n",
    "#overlay the dataset records -black/red edges for train/test data\n",
    "disp3.ax_.scatter(train_x[:, 0], train_x[:,1], \n",
    "                  c=train_y, edgecolor=\"k\",linewidth=1,label='train')\n",
    "disp3.ax_.scatter(test_x[:, 0], test_x[:,1], \n",
    "                  c=test_y, edgecolor=\"r\",linewidth=1,label='test')\n",
    "disp3.ax_.legend()\n",
    "axs2[0].set_title(f'Train Accuracy {train_accuracy*100}%')\n",
    "axs2[1].set_title(f'Test Accuracy {test_accuracy*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## So how do  we learn models?\n",
    "**Construction**:  add boundaries to make models more complex\n",
    "- Add examples to kNN\n",
    "- Repeatedly add nodes to trees, splitting on new variables\n",
    "- Repeatedly add rules that classify as-yet unclassified data\n",
    "- Add nodes to an artifical neural network\n",
    " \n",
    "**Perturbation**: Move existing boundaries to change model\n",
    "- Change value of K or distance function in kNN\n",
    "- Change rule/treenode thresholds: *if  exam < 40*  &rarr; *if exam < 38*\n",
    "- Change operators in rules/ tree nodes:  *if exam < 38* &rarr; *if exam &leq; 38*\n",
    "- Change variables considered in rules/tree nodes: *if exam < 38* &rarr; *if coursework < 38*\n",
    "- Change weights in MLP, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "Supervised Machine Learning is concerned with learning predictive models from datasets\n",
    "- Different algorithms use different representations of decision boundaries\n",
    "- Regions inside the boundaries contain **Labels**\n",
    "   - Class names (classification)\n",
    "   - (formulas leading to) continuous values (regression)\n",
    "\n",
    "Algorithms **fit** models to data by repeatedly:\n",
    "  - making and testing small changes,  \n",
    "  - and then selecting the ones that improve accuracy on the training set,\n",
    "  - until some stop criteria is met\n",
    "\n",
    "They do this by either adding complexity or changing the parameters of an existing model\n",
    "- values for parameters that define unique set of boundaries and labels &hArr; **Candidate Solution**\n",
    "- *Fitting* a model to data &hArr; **searching** for a model that  minimises  loss  on that dataset.\n",
    "\n",
    "Once the model has been learned (fit) we leave it unchanged  \n",
    "  - and use it to **predict** the labels for new data points.\n",
    "\n",
    "Next week:   Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "aienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
