{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aims of this tutorial\n",
    "The aim of this tutorial is to illustrate how Perceptrons can be combined into Neural Networks to solve problems that are not linearly separable, such as XOR.  \n",
    "We will look at the key differences between the two algorithms and also consider how network architecture and training parameters affects the outcome.\n",
    "\n",
    "## Learning Objectives:\n",
    "1. Understand the key differences between the Neural Network and Perceptron algorithms:\n",
    "- Non-linear activation functions.\n",
    "- Using Backpropagation to update (learn) the weights.\n",
    "- configuring MLP with more than one output node when there are more than two different output labels (multi-class learning)\n",
    "2. Understand how different nodes learn different aspects of the problem.\n",
    "\n",
    "3. Consider the need for different network architectures and learning parameters for different problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview:\n",
    "<img src=\"figures/ANN-2-Node.png\" style=\"float:right\" width= 500>\n",
    "\n",
    "As we have seen, Perceptrons are only capable of solving linearly separable problems.   \n",
    "To overcome this limitation, we can connect Perceptrons together into a network.  \n",
    "Each one becomes a Node in the network, and they are connected together into Layers. \n",
    "\n",
    "In standard Artificial Neural Network (ANN) architecture there is one input, one output and one or more hidden layers.  \n",
    "- Though the term *input layer* is a bit misleading, it doesn't actually do any computation, it is just the inputs to the network.\n",
    "- So, outputs of hidden layers become the inputs to subsequent hidden layers, or the final output layer. \n",
    "- Hidden nodes tend to learn different aspects of the problem space, building more complex decision boundaries and are therefore able to solve more complex problems.\n",
    "\n",
    "Note: \n",
    "- The number of nodes in the input layer must equal the number of inputs/features in the data. \n",
    "- One output node can discriminate between two classes (classification problems),  \n",
    "  or predict a value for one continuous variable (regression problems).  \n",
    "  If your data  has more than two classes (or variables to predict),  \n",
    "  the number of output nodes must equal the number of classes/regression variables. \n",
    "- The number of hidden layers and nodes in the layers is arbitrary, and selecting this architecture is part of building an ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Training Algorithm  \n",
    "Similar to Perceptrons, ANN are trained in two 'phases'. \n",
    "- The forward pass, where data is input into the network to produce an output. \n",
    "- The backward pass, where the error in output is used to update the weights using Backpropagation and Gradient Descent.\n",
    "  - note that to calculate what the sum of  inputs was going *in* to a node we apply the *sigmoid derivative* to the signal coming *out* of that node \n",
    "\n",
    "<img src=\"figures/ann-pseudocode.png\" style=\"float:center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-warning\" style=\"color:black\"><h1> Part 1: Solving XOR</h1></div>\n",
    "As an introduction to the ANN algorithm, and to give you an intuition for how different nodes and layers in the network learn different aspects of the problem space, we are going to look at how a small network can solve the XOR problem.\n",
    "\n",
    "Running the code will train an ANN to solve the XOR problem and produces a visualisation to show how different nodes have learned different aspects of the problem to create a more complex decision boundary (in this case different logical functions.\n",
    "\n",
    "- You do not need to understand *how* the graphs/visualisations are produced.\n",
    "\n",
    "- You should try and understand *what* the graphs/visualisations output means.\n",
    "\n",
    "\n",
    "**Run the next  cells below** to import the libraries and define the function that plots the decision surface.\n",
    "- If the first cell reports an error trying to import VisualiseNN, make sure you have downloaded the file VisualiseNN.py and it is in the same directory as this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basics for manipulating and outputting arrays etc\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from random import random\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "## MLP specific stuff\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import VisualiseNN as VisNN\n",
    "\n",
    "import workbook9_utils as wb9\n",
    "from workbook9_utils import plotDecisionSurface\n",
    "\n",
    "# useful sklearn functions for preprocessing data and sahowing results\n",
    "from  sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "#the iris data\n",
    "from sklearn.datasets import load_iris\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"color:black\">\n",
    " <h2> Activity 1.1 Investigating repeatability as different sized networks learn to model the simple XOR problem</h2>\n",
    "    \n",
    "<b>First</b> run the first cell below: it will try and learn the XOR problem and show you a plot of how the error rate changes over *time* measured in epochs. <br>\n",
    "    As there are only four cases, we do not have any test data for this problem - we are just looking at how reliably different sized networks can learn a simple problem.\n",
    "<ul>\n",
    "    <li> One epoch means that all the training data is shown to the system once and the weights are updated. </li>\n",
    "    <li> We know that <it>in theory</it> it should be able to learn XOR with 2 hidden nodes. <br>\n",
    "         But is there a difference between theory and what happens in practice? </li>\n",
    "    <li>Each time you run the cell it starts the whole process from new, so the error curve will be different and you might get different final accuracy scores.</li>\n",
    "    </ul><br>\n",
    "<b>Then</b><ol>\n",
    "    <li> Uncomment the variable declarations for the lists hidden_layer_width and xor_successes on lines 8 and 9</li>\n",
    "    <li> Edit the cell by putting in two loops around lines 16--24 (where we create, train and test the MLP for a given configuration): <ul> \n",
    "        <li>The outer loop should set the value of numHiddenNodes to each of the values in the list hidden_layer_width</li>\n",
    "        <li>The inner loop should run ten times for each network size: <br>counting the number of runs that reach 100% training accuracy,<br> and storing that value in the appropriate place in the list xor_success</li>\n",
    "<li> Now that you have run your experiment and recorded the xor_success values for different network sizes, run the second cell below. <br>\n",
    "    This will to produce a <b>sensitivity analysis</b>: a plot showing how much your results depend on a network parameter - the number of hidden nodes.</li>\n",
    "    </ol>\n",
    "    </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the four input cases form our training data\n",
    "train_X = np.array( [[0,0],[0,1],[1,0],[1,1]])\n",
    "# and her eare the labels our network should learn for the XOR problem\n",
    "xor_y = np.array([0,1,1,0])\n",
    "train_y= xor_y\n",
    "\n",
    "# these arrays will store the success rate for different numbers of nodes\n",
    "#hidden_layer_width = [1,2,3,4,5,6,7,8,9]\n",
    "#xor_success = np.zeros(9)\n",
    "\n",
    "# at present jsut do one run for with two nodes in the hidden layer\n",
    "numHiddenNodes = 2\n",
    "\n",
    "\n",
    "# one hidden layer with one hidden layer of numHiddenNodes neurons with logistic (sigmoid) activation and Stochastic Gradient Descent (backprop)\n",
    "xorMLP =  MLPClassifier(hidden_layer_sizes=(numHiddenNodes,), max_iter=1000, alpha=1e-4,\n",
    "                    solver='sgd', verbose=0, \n",
    "                    learning_rate_init=.1)\n",
    "\n",
    "\n",
    "xorMLP.fit(train_X, train_y)\n",
    "\n",
    "training_accuracy = 100* xorMLP.score(train_X, train_y)\n",
    "print(f\"Training set accuracy: {training_accuracy}%\")\n",
    "\n",
    "lossplot=plt.plot(xorMLP.loss_curve_) \n",
    "plt.xlabel('training epochs')\n",
    "plt.ylabel('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit the  array xor_success to  replace the 'dummy' values(1,6,4,8,10) with your results, i.e.  the number of times your algorithm reached 100% accuracy on the training set\n",
    "\n",
    "\n",
    "plt.plot(hidden_layer_width, xor_success)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alaert alert-warning\" style=\"color:black\">\n",
    "    <h2>Activity 1.2: Visualising what the network is doing</h2>\n",
    "<ol><li>Run the training cell above until you get a successful run with four nodes - i.e. one ending with training set accuracy 100%. </li>\n",
    "    <li>Then run the cell below and make sure you understand what it is showing you.</li>\n",
    "</ol>\n",
    "<p>The top plot shows the output of the final node for different inputs.\n",
    "    <ul> <li>In this case we only have the four inputs marked by circles.</li>\n",
    "        <li> In use, we would apply a threshold of 0.5 to decide whether the output of the network was 1 or 0 <br>\n",
    "            So the orange lines represent the decision boundaries.</li></ul\n",
    "    \n",
    "<p> The bottom plot shows a visualisation of the network structure and weights: \n",
    "   <ul>\n",
    "        <li> The line thickness represents the magnitude of the weight</li>\n",
    "       <li> The line colour indicates the sign of the weight:<br>\n",
    "           Blue lines are <b>negative weights</b>, so signals down these connections will  suppress the output of the cell they lead to. <br>\n",
    "           Red lines are <b> positive weights</b>- so signals down these connections will  stimulate the node they lead to.</li>\n",
    "    </ul>     \n",
    "    You could repeat this with a network with more nodes (e.g. 10) or perhaps with an 'unsuccessfully trained'  network to see if you spot any patterns. \n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theMLP=xorMLP # change this line to reuse the code below for a different problem\n",
    "num_output_nodes = 1 # and this one for multi-class problems\n",
    "\n",
    "plotDecisionSurface(theMLP,train_X,train_y)\n",
    "\n",
    "\n",
    "#network_structure = np.hstack(([train_X.shape[1]], np.asarray(myMLP.hidden_layer_sizes), [train_y.shape[0]]))\n",
    "network_structure = np.hstack((2, np.asarray(theMLP.hidden_layer_sizes), 1))\n",
    "# Draw the Neural Network with weights\n",
    "network=VisNN.DrawNN(network_structure, theMLP.coefs_)\n",
    "network.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class = \"alert alert-warning\" style=\"color:black\"><h1> Part 2: Using MLP for multiclass problems:  Iris data</h1></div>\n",
    "<img src=\"cascading.png\" style=\"float:right\">\n",
    "\n",
    "So far we have used multilayer perceptrons for learning binary (two-class) problems.  \n",
    "Last week you should have discussed how you could solve a multi-class problem,  \n",
    "by 'cascading' binary classifiers. \n",
    "This is shown in the image for a three class problem.  \n",
    "Here the diamonds represent classifiers, each doing a \"this class or not\" decision.\n",
    "\n",
    "\n",
    "In this part we will introduce a different idea, which is to use a  parallel classifier using softmax and one-hot encoding.\n",
    "\n",
    "Not only is this simpler to manage, it  has the benefit that the classifiers can all share the feature creation done in previous layers\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to load the data\n",
    "\n",
    "\n",
    "irisX,irisy = load_iris(return_X_y = True)\n",
    "feature_names = ['sepal width','sepal_length','petal_width','petal_length']\n",
    "irisLabels = np.array(('setosa','versicolor','virginica'))\n",
    "# show what the labels look like\n",
    "print(irisy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming our label data to a format for training a MLP with three output nodes\n",
    "As you can see when you run the cell above, the labels is a 1-D array with labels of 0, 1, or 2.  \n",
    "This is fine for models like nearest neighbours, rule sets or decision trees.  \n",
    "However, (crudely speaking) the output from a neuron tends to be *off* (0) or *on*(1).  \n",
    "So if we want our network to make a choice of three predictions, then we need a node for each class.\n",
    "\n",
    "So there are two changes we make:\n",
    "1. We configure the network to have three output nodes  and use 'softmax' ('winner-takes-all') activation.  \n",
    "    i.e. Each node outputs a value, and we take as our final output the class whose node has the highest output signal\n",
    "2. We convert our labels tell the network what *each of the nodes* should ideally output for each training example.  \n",
    "   In other words:\n",
    "   - if the label is 0 the then output should be [1,0,0],  \n",
    "   - if the label is 1 it should be [0,1,0], and \n",
    "   - if it is 2 the output should be [0,0,1].\n",
    "\n",
    "Sklearn comes with a module sklearn.preprocessing.onehotencoder() to do this,   \n",
    "but the cell below does it explicitly to illustrate what is going on. \n",
    "\n",
    "I've made it generic so that you can easily reuse it for different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to create the one-hot version of the labels  we need for our MLP \n",
    "numcases = len(irisy)\n",
    "print(f'there are {numcases} training examples')\n",
    "thelabels = np.unique(irisy)\n",
    "numlabels = len(thelabels)\n",
    "print( f'there are {numlabels} labels: {thelabels}')\n",
    "\n",
    "# make a 2d array with numcases rows. and numlabels columns\n",
    "irisy_onehot = np.zeros((numcases,numlabels))\n",
    "\n",
    "\n",
    "# Now loop through the rows of the new array setting the appropriate column value to 1\n",
    "for row in range(numcases):\n",
    "    label = irisy[row]\n",
    "    irisy_onehot[row][label]= 1\n",
    "\n",
    "print('This is what  rows 45-55 of the one-hot version of the labels look like')\n",
    "print(irisy_onehot[44:55,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting our data into a training and a test set\n",
    "As you can see from the output of the cells above, the iris data has groups all the classes i.e. rows 0-49 are 'iris-setosa', 50-99 are 'iris versicolor'. and rows 100-149 are 'iris-virginica'.\n",
    "\n",
    "So if we want to train our network  and then estimate how well it will do on new data, we need to split this into a training and test set.  \n",
    "Again, we could do this manually:\n",
    "- first shuffling the rows so that we got a mixture of classes, \n",
    "- then taking the first part of the data for training and the second for testing.\n",
    "\n",
    "If the data are not so well organised, or the numbers of examples of different classes are not roughly equal, then that code gets trickier.  \n",
    "So the cell below shows how to do this using a method from sklearn.   \n",
    "The parameters are, in order:\n",
    "- the feature values (irisx)\n",
    "- the onehot-encoded set of labels (irisy_onehot)\n",
    "- what proportion of our data we holdback from training, so we can use it for test. We'll use 1/3rd ( test_size=0.33)\n",
    "- the array holding the labels that we want to be evenly represented in both our training and test sets. (stratify=irisy_onehot)\n",
    "\n",
    "This function returns the four different arrays - train and test, x and y.  \n",
    "Noe that this function also works if your data is not one-hot encoded - it figures that out for itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iris_train_X, iris_test_X, iris_train_y, iris_test_y = train_test_split(irisX,irisy_onehot, test_size=0.33, stratify=irisy_onehot )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-warning\" style = \"color:black\">\n",
    "    <h2>Activity 2.1 Training a MLP to learn the iris classification problem.</h2>\n",
    "<ol>\n",
    "    <li> Start by using the  settings for the MLPClassifier that we had before and just change the size of the hidden layer to five or ten </li>\n",
    "   <ul> \n",
    "       <li> You will probably see that the training stops making improvements before the problem has been fully learned.</li>\n",
    "       <li> This is an example of the backpropagation getting 'stuck' in a **local optimum** (we'll talk about these more next week). </li>\n",
    "        <li> It happens becuase the basic 'stochastic gradient descent' algorithm *'sgd'* is a local search method with only crude methods for getting out of 'traps'.</li> \n",
    "       <li> Try changing the solver to 'adam' and see if this gives better performance. </li>\n",
    "    </ul>\n",
    "    <p><b>Remember</b> to run a few times with each setting because this is a randomised algorithm and the random set of initial weights makes a huge difference.  </p>\n",
    "    <p><b>Question</b> What do you understand by <it>\"better\"</it> performance?</p><p></p>\n",
    "\n",
    "<li> Now try adding a second hidden layer - for example by changing that parameter in the constructor to <it>hidden_layer_sizes=(3,3)</it>.<br>  \n",
    "<li> Experiment with a few runs of each configuration to see if the network learns the problem more reliably with one hidden layer of 10 nodes or 2 layers of 5 nodes.</li>\n",
    "</ol>  \n",
    "</div>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an MLP object-  you will want to change the number of hidden nodes\n",
    "irisMLP =  MLPClassifier(hidden_layer_sizes=(5,), max_iter=1000, alpha=1e-4,\n",
    "                    solver='adam', verbose=0, \n",
    "                    learning_rate_init=.05)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "irisMLP.fit(iris_train_X, iris_train_y)\n",
    "print('number of output nodes = ' +str(irisMLP.n_outputs_))\n",
    "    \n",
    "lossplot=plt.plot(irisMLP.loss_curve_)    \n",
    "plt.xlabel(\"trainning epochs\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "# report how well it does on the training set\n",
    "training_accuracy = 100* irisMLP.score(iris_train_X, iris_train_y)\n",
    "print(f\"Training set accuracy: {training_accuracy} %\")\n",
    "\n",
    "\n",
    "# now how good is our network at predicting data it has never seen before\n",
    "test_accuracy = 100* irisMLP.score(iris_test_X, iris_test_y)\n",
    "print(f\"Estimated (Test set) accuracy: {test_accuracy}%\")\n",
    "\n",
    "# this bit of code prints a simple confusion matrix showing how the predicted labels correspond to the 'real' ones\n",
    "predictions=irisMLP.predict(iris_test_X)\n",
    "confusion = np.zeros((3,3))\n",
    "for row in range (predictions.shape[0]):\n",
    "    actual = np.argmax(iris_test_y[row])\n",
    "    predicted = np.argmax(predictions[row])\n",
    "    confusion [actual] [predicted] += 1\n",
    "\n",
    "print( '\\nPredicted->   Setosa  Versicolor  Virginica')\n",
    "print( 'Actual ')\n",
    "for i in range(3):\n",
    "    print( f'{irisLabels[i]:<10}       {confusion[i][0]:2.0f}       {confusion[i][1]:2.0f}       {confusion[i][2]:2.0f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"color:black\"><h2> Activity 2.2 Discussion</h2>\n",
    "Try to come up with answers to these questions. (these are the sorts of things you might be asked in an exam)\n",
    "<ol>\n",
    "    <li>Why is the test accuracy sometimes much lower than the training accuracy?</li>\n",
    "     <li>Why is it sometimes less reliable train a network with multiple hidden layers when learning the iris data?  <br>\n",
    "Hint: how many connections are you trying to learn?  <br>How much data have you got?</li>\n",
    "    </ol></div>\n",
    "\n",
    "<div class= \"alert alert-warning\" style=\"color:black\"><h2>Activity2.3 (stretch): Does it help if you normalise the data like we did in week 6?</h2>\n",
    "<p>In Activity 2.3 of the unsupervised learning tutorial (workbook6) we used a Minmax scaler so that each feature was transformed to the range (0,1).</p>\n",
    "<p>    Reusing snippets of code from that workbook,  try adding a few lines to the cell at the start of this section (Part 2), so that scaling gets applied to irisX before you make the call to train_test_split().<br>\n",
    "    Does this improve learning?</p>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"color:black\"><h1> Part 3: Learning to recognise hand-written digits:  MNIST</h1></div>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-warning\" style= \"color:black\"><h2>Activity 3.1: Loading and visualising the data</h2>\n",
    "<ol>\n",
    "    <li> Run <b>ONE</b> of the next two cells depending on whether you are using csctcloud or your own installation</li>\n",
    "    <li> Then run the third cell to visualise the data.</li>\n",
    "    </ol>\n",
    "</div>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the next cell if you are on csctcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Only  Run this cell if you are using the ccstcloud server\n",
    "# example code to run on the server using a copy of the data that I have already downloaded and made available.\n",
    "#label is column 0\n",
    "# pixel values are from 0-255 so need to be scaled to 0-1\n",
    "\n",
    "import numpy as np\n",
    "test = np.genfromtxt(\"/home/common/datasets/mninst/mnist_test.csv\",delimiter=',') \n",
    "X_test = test[:, 1:785] /255 \n",
    "y_test = test[ : , 0]\n",
    "\n",
    "train = np.genfromtxt(\"/home/common/datasets/mninst/mnist_train.csv\",delimiter=',')\n",
    "X_train = train[ : , 1:785]/255\n",
    "y_train = train[ : , 0]\n",
    "\n",
    "print(f\"X_train has {X_train.shape[0]} rows and {X_train.shape[1]} columns, y_train has {y_train.shape} entries,\\nX_test has shape {X_test.shape} y_test has {len(y_test)} entries.\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the next cell if you are NOT on csctcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only run this cell to load the data  if you are on your own machine\n",
    "\n",
    "# the data to download is about 33Mb \n",
    "# so I've put this code in its own cell so you can just do it once.\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True,cache=True,data_home=\"data\")\n",
    "\n",
    "X = X / 255.\n",
    "\n",
    "# rescale the data, use the traditional train/test split\n",
    "X_train, X_test = X[:60000], X[60000:]\n",
    "y_train, y_test = y[:60000], y[60000:]\n",
    "\n",
    "print('data loaded and saved locally')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell  shows us some example images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display ten random images from each class\n",
    "print(f'The test data has {X_test.shape[0]} images, each described as a {X_test.shape[1]} features (pixel values)')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for label in range(10):\n",
    "    imagesForLabel= np.empty((0,784))\n",
    "    examples=0\n",
    "    next=0\n",
    "    while(examples <5):\n",
    "        if (int(y_test[next])==int(label)):\n",
    "            imagesForLabel = np.vstack((imagesForLabel, X_test[next]))\n",
    "            examples +=1\n",
    "        next += 1\n",
    "    for col in range(5):\n",
    "        exampleplot = plt.subplot(10, 5, (label*5 +col+1) )\n",
    "        exampleplot.imshow(imagesForLabel[col].reshape(28, 28), \n",
    "                   cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class= \"alert alert-warning\" style = \"color:black\"> <h2>Activity 3.2 : Visualising what features the hidden layers learn to respond to.</h2> \n",
    "    <p>We will now configure a multilayer perceptron  and training it with all 60,000 images from the standard MNIST training set.</p>\n",
    "\n",
    "<p>The idea for you to learn here, is that each hidden node is effectively acting as a feature detector. <br>\n",
    "  <ol>\n",
    "      <li> So let's consider just one hidden layer node: \n",
    "          <ul>\n",
    "           <li> and a simple pattern where the weights from pixels in the top left and bottom right quadrant are all +1, </li>\n",
    "            <li> and the weights from pixels in the top-right and bottom-left quadrants are all -1.</li>\n",
    "          </ul> \n",
    "      </li>\n",
    "      <li> Now consider an input image that has some constant value for every pixel (feature) - i.e. is all the same colour. \n",
    "          <ul>\n",
    "             <li> When these inputs to the node  are multiplied by their weights and summed, they will cancel each other.</li>\n",
    "             <li> So the <b> weighted sum </b> will be zero,</li>\n",
    "            <li> and the <b>output</b> of the node  will be sigmoid(0) = 0.5, which we class as 0</li>\n",
    "          </ul>\n",
    "      </li>\n",
    "     <li> Next consider an the image  of a simple 'chequer' pattern with  white (255) in the top-left and bottom-right quadrants,  \n",
    "  and black (0)  in the other two.\n",
    "         <ul>\n",
    "          <li>In this case  the pattern of  pixel intensities (features) in the image  maches match the pattern in the weights.</li>\n",
    "             <li>So then the weighted sum will be at its maximum, and the <b>node will output +1.<b></li>\n",
    "         </ul>\n",
    "             </ol>\n",
    "         <p>So we can consider our hidden node is acting as a 'feature detector' for the checker pattern.<br>\n",
    "             And in general <b>each</b> hidden node is a feature detector that  <b>learns</b> to recognise useful patterns during training.<br>\n",
    "             And hidden nodes in the 2nd,3rd,...nth layers build complex features out of those recognised by the layer before.</p>\n",
    "<p>\n",
    "  <b>Run</b> the next set of cells to:<ul>\n",
    "<li> Set up and train the network with 16 nodes (a number chosen so we can visualise them neatly in a grid). </li>\n",
    "         <li> Then output the pattern  weights from each of the nodes as an image.</li></ul>\n",
    "      </p></div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> In year 2, the Machine Learning module will explain how this concept of feature detectors has been extended  in Deep Convolutional Networks. <br>\n",
    "In these features (called 'filters') can be a smaller size than the image and a process of Convolution (rather than straighforward multiplying) lets them detect small local features anywhere in the image.<br>  Convolutional Neural Networks have completely revolutionised the field of image processing and AI for visual tasks.</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up and train network\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "numHiddenNodes = 12\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(numHiddenNodes), max_iter=35, random_state=0,verbose=1)\n",
    "\n",
    "# this example won't converge because of CI's time constraints, so we catch the\n",
    "# warning and are ignore it here\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning,\n",
    "                            module=\"sklearn\")\n",
    "    mlp.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Training set accuracy: {100*mlp.score(X_train, y_train)}%\")\n",
    "print(f\"Test set accuracy: {100*mlp.score(X_test, y_test)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the weights from the input nodes to the first hidden layer\n",
    "coef = mlp.coefs_.copy()[0].T\n",
    "\n",
    "# find endpoints to use for scaling colour range\n",
    "scalemax = coef.max() *0.75\n",
    "scalemin = coef.min() *0.75\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "numRows=4\n",
    "numCols=4\n",
    "\n",
    "for i in range(numHiddenNodes):\n",
    "    l1_plot = plt.subplot(numRows, numCols, i + 1)\n",
    "    l1_plot.imshow(coef[i].reshape(28, 28), \n",
    "                   cmap=plt.cm.RdBu, vmin=scalemin, vmax=scalemax)\n",
    "    l1_plot.set_xticks(())\n",
    "    l1_plot.set_yticks(())\n",
    "    #l1_plot.set_xlabel('Hidden Node %i' % i)\n",
    "title= 'Learned weights from pixels to each hidden node which correspond to patterns the nodes have been trained to respond to.\\n'\n",
    "title=title+'Looking at a hidden node:\\n    Parts of the image where a node has weights coloured white (0.0) are ignored.\\n'\n",
    "title = title + '    Red indicates negative weights: signals from these pixels suppress the node.\\n'\n",
    "title=title+ '    Blue indicates positive weights; pixels from these pixels stimulate the  hidden node.'\n",
    "\n",
    "_=plt.suptitle(title,x=0.15,horizontalalignment='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-warning\" style=\"color:black\"><h2> Activity 3.3: Discussion / Thought exercuses </h2>\n",
    "Iris is a simple problems with only 4 features and three classes.\n",
    "\n",
    "MNIST is a much more complicated problem with 784 features and ten classes - some of which (e.g. 4s and sevens) can be drawn in completely different ways.\n",
    "<p>\n",
    "    <b>Questions:</b><ol>\n",
    "    <li>So how come the accuracy is roughly the same on these two problems?</li>\n",
    "    <li> The MNIST MLP you just trained and visualised has 10 nodes in its output layer, each receving numHiddenNodes (16) input signals. <br>\n",
    "        This means the hidden layer is effectively learning to  reducing a 784-Dimensional problems to a 16-dimensional one!<br>\n",
    "        How cool is that?<br>\n",
    "        From your observations of the visualisations, does it look like we even need 16 hidden nodes / dimensions/features?</li>\n",
    "    </ol></p>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-warning\" style=\"color:black\"> <h2>Activity 3.4: examining the effect of having less data.</h2>\n",
    "<p>The code in the cell below has a loop which trains a newtwork with different amounts of trainig data, and reports the training and test accuracy for each rin. </p>\n",
    "<p>  Run the cell below and make note of the train and test accuracy for each different sized training data.<br>\n",
    "    Make a hypothesis that explains the  patterns of changing training and test scores you see, and be ready to discuss this in class.</p></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trSetSize in (100,600,1000,6000,10000,50000):\n",
    "    split= trSetSize/60000\n",
    "    _,X_train_small,_,y_train_small = train_test_split(X_train,y_train, test_size=split,stratify=y_train)\n",
    "    smallMnistMLP = MLPClassifier(hidden_layer_sizes=(16), max_iter=25, alpha=1e-4,\n",
    "                    solver='sgd', verbose=0, random_state=10,\n",
    "                    learning_rate_init=.1)\n",
    "\n",
    "#put a loop of n runs here\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning,\n",
    "                            module=\"sklearn\")\n",
    "        smallMnistMLP.fit(X_train_small, y_train_small)\n",
    "    print(f'With a training set of {trSetSize} examples')\n",
    "    print(f\"    Training set accuracy: {100*smallMnistMLP.score(X_train_small, y_train_small)}%\")\n",
    "    print(f\"    Test set accuracy: {100*smallMnistMLP.score(X_test, y_test)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-warning\" style=\"color:black\"><h2> (Stretch) Activity 3.5: Gathering evidence for your hypothesis.</h2>\n",
    "<ol>\n",
    "    <li> Copy and paste the code from the previous cell into the cell below. </li>\n",
    "    <li> <b>Edit</b> the code by: <ul>\n",
    "        <li> Adding an array called results with 4 columns and 30 rows, initialised to zero (hint: np,zeros). </li>\n",
    "        <li> Adding a loop so that it runs the experiment for each training set size  5 times.</li>\n",
    "        <li> saving the training and test accuracy from each run into a seperate row in your new  array.</li>\n",
    "        <li> Store the training set size in results column 0, run number in column 1, training accuracy in column 2 and test accuracy in column 3.</li>\n",
    "     </ul>\n",
    "    <li> Use matplotlib to make a plot with training set size on the x-axis and accuracy on the y-axis</li>\n",
    "        <li> Plot your results as two different lines on your plot, with error bars for each.</li>\n",
    "        </ol>\n",
    "        <p> <b>  HINT: google is good to find code snippets to make plots with.</b></p.</div>\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\"> Please save your work (click the save icon) then shutdown the notebook when you have finished with this tutorial (menu->file->close and shutdown notebook</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"> Remember to download and save your work if you are not running this notebook locally.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
