{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dae6289d-998e-42ed-8ade-d9a55526d67b",
   "metadata": {},
   "source": [
    "# Workbook 4: Local Search in categorical and continuous spaces\n",
    "\n",
    "Overview of activities and objectives of this workbook:\n",
    "\n",
    "1. The first part of this workbook will implement *Local Search* for a simple *binary* problem (One-max).\n",
    "   - The One-max problem is simple; given an a random binary array, turn all values (i.e. 0's) to 1.  \n",
    "     The quality of a solution is the number of 1s it contains - so it is sometimes called *Counting Ones*.\n",
    "   - We will also evaluate the performance of the *Local Search* algorithm on the One-max problem with different complexities (number of values). <br><br>\n",
    "\n",
    "2. The second part of this workbook will adapt the binary One-max problem to use *continuous* decision variables.\n",
    "   - We will adapt the Local Search algorithm from part one to solve the continuous One-max problem.\n",
    "   - For local search with continuous variables we will explore two different methods for generating the neighbouring candidate solutions:\n",
    "       -  adding random (gaussian) noise, and\n",
    "       -  using *gradient* information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94ac125",
   "metadata": {},
   "source": [
    "<div style=\"background-color:black;width:100%;height:10px\"></div>\n",
    "\n",
    "# Part 1: Local Search for binary One-max\n",
    "\n",
    "Unlike the search algorithms covered in previous weeks, Local Search only considers the immediate neighbours of the current best candidate solution.  \n",
    "- If any of the neighbours are an improvement on the current candidate solution then that solution is selected\n",
    "- and the remaining neighbours are ignored/forgotten i.e. removed from the open list (see pseudocode below).\n",
    "\n",
    "You can think of Local Search like climbing a hill.   \n",
    "Each step you choose the direction that takes you higher towards the top and *not sideways or backwards*.   \n",
    " - So you stop if there are no uphill moves.\n",
    " - And you aren't allowed to backtrack by considering steps you *could* have taken 2 or 3 steps ago.\n",
    "\n",
    "To consider local search in terms of the One-max problem:\n",
    "\n",
    "1. If the random starting solution is [0, 1, 1, 0, 1], with quality 3\n",
    "\n",
    "2. We then generate several neighbouring solutions, e.g.:\n",
    "    - [0, 1, 1, 0, 0] with quality 2\n",
    "    - [0, 1, 1, 1, 0] with quality 3\n",
    "    - [0, 1, 1, 1, 1] with quality 4\n",
    "    - and so on, changing other values\n",
    "\n",
    "3. Then choose the best neighbour generated. In this case the 3rd one improves the quality so that is selected. The other two have either worse, or the same quality and so are discarded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd62e37-11a7-4d2c-b1be-2928935831e5",
   "metadata": {},
   "source": [
    "### Pseudocode for Local Search SelectAndMoveFromOpenList function\n",
    "\n",
    "<div style=\"background:#F0FFFF;font-size:18pt;color:black\">\n",
    "<p style=\"color:darkred;font-size:18pt;margin-bottom:0pt\"><em>SelectAndMoveFromOpenList</em></p>\n",
    "<dl style=\"font-size:18pt;margin-top:0pt\">\n",
    "    <dt>&nbsp;&nbsp;&nbsp;<b>IF</b> IsEmpty( open_list) <b>THEN</b> </dt>\n",
    "    <dd> RETURN None</dd>\n",
    "    <dt> &nbsp;&nbsp;&nbsp;<b>ELSE</b></dt>\n",
    "    <dd>bestChild &larr; <b>GetMemberWithHighestQuality</b>(openList)</dd>\n",
    "    <dd> <b>EMPTY</b>(openlist)&nbsp;&nbsp;&nbsp;&nbsp;<span style=\"background:pink\">This prevents backtracking</span></dd>\n",
    "    <dd>  <b>IF</b> BetterThan(bestChild, bestSoFar) <b>THEN</b> <br>\n",
    "        &nbsp;&nbsp;&nbsp;&nbsp;bestSoFar &larr; bestChild <br>\n",
    "        &nbsp;&nbsp;&nbsp;&nbsp;RETURN bestChild </dd>\n",
    "    <dd> <b>ELSE</b> <br>&nbsp;&nbsp;&nbsp;&nbsp; RETURN None</dd>\n",
    "</dl>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19af297-72bb-491a-996c-a0042b2d6f9b",
   "metadata": {},
   "source": [
    "<div style=\"background-color:black;width:100%;height:3px\"></div>\n",
    "<br>\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black\"><h2> Activity 1: Implementing local search for the binary One-max problem</h2>\n",
    "    <ol>\n",
    "    <li>Complete the second cell below which implements the <code>LocalSearch</code> class.</li>\n",
    "    <li>We have provided an <code>__init__()</code> method with over-rides the default behaviour and creates a random starting point</li>\n",
    "    <li>You need to complete the method <code>select_and_move_from_openlist()</code>. We have broken this down into <b>4</b> clearly marked small steps:</li>\n",
    "        <ul>\n",
    "            <li>Find the best candidate solution on the open list</li>\n",
    "            <li>Clear the open list</li>\n",
    "            <li>Check if the best candidate solution is a better solution</li>\n",
    "            <li>Return the best candidate solution if it is an improvement, else return <code>None</code></li>\n",
    "        </ul>\n",
    "    <li> Test your implementation by running the third cell which uses your implementation to solve the <em>oneMax</em> problem.\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a651ff3e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"color:black\"><b>Hints:</b>\n",
    "The first step (finding the best solution on the open list) is the same as <b>Best-first search</b> from the previous week.<br>\n",
    "    You can use <code>self.a_better_than_b()</code> to evaluate if one solutions quality is better than another.\n",
    "</div>\n",
    "\n",
    "**Run but do not edit the next cell** to import some relevant code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd49c91-ddef-4115-b6b9-1270fc90bf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU MUST RUN THIS CELL BUT DO NOT EDIT IT OR YOU WILL BREAK THE NOTEBOOK\n",
    "import sys, os\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "# Import from the common directory\n",
    "sys.path.append('../common')\n",
    "\n",
    "from candidatesolution import CandidateSolution\n",
    "from singlemembersearch import SingleMemberSearch\n",
    "from problem import Problem\n",
    "from onemaxproblem import OneMaxBinary, OneMaxContinuous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f542a66-9639-4ee7-9221-1585e79dd9c3",
   "metadata": {},
   "source": [
    "**Write your implementation where indicated in the cell below. Then run the cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b3cea1-eb18-43ff-b1ac-5f5b8c46b2f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LocalSearch(SingleMemberSearch):\n",
    "    \"\"\"Implementation of local search.\"\"\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\" return name\"\"\"\n",
    "        return \"local search\"\n",
    "    \n",
    "    def __init__( self,\n",
    "        problem: Problem,\n",
    "        constructive: bool = False,\n",
    "        max_attempts: int = 50,\n",
    "        minimise=True,\n",
    "        target_quality=1):\n",
    "        \"\"\" call super class \n",
    "        then change to random starting point\n",
    "        \"\"\"\n",
    "        super().__init__(problem,\n",
    "                         constructive=constructive,\n",
    "                         max_attempts=max_attempts,\n",
    "                         minimise=minimise,\n",
    "                         target_quality=target_quality)\n",
    "        \n",
    "        # over-ride default\n",
    "        arrays_of_rands = np.random.choice(my_binary_onemax.value_set, size=num_vars)\n",
    "        start_point =  self.open_list[0]\n",
    "        start_point.variable_values= list(arrays_of_rands)\n",
    "\n",
    "        # measure quality \n",
    "        start_point.quality = self.problem.evaluate(start_point.variable_values)\n",
    "        if start_point.quality == self.target_quality:\n",
    "            self.trials = 1\n",
    "            self.result = start_point.variable_values\n",
    "            self.solved = True\n",
    "\n",
    "    def select_and_move_from_openlist(self) -> CandidateSolution:\n",
    "        \"\"\"Pops best thing from list, \n",
    "        clears rest of list, \n",
    "        then returns best thing\n",
    "        relies on the presence of self.best_so_far\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        next working candidate (solution) taken from open list\n",
    "           **if it is an improvement**\n",
    "        None\n",
    "           IF list is empty OR next thing is worse than best so far\n",
    "        \"\"\"\n",
    "        next_soln = CandidateSolution()\n",
    "\n",
    "        # check the open list isn't empty\n",
    "        if len(self.open_list) == 0:\n",
    "            self.runlog += \"LS:empty open list\\n\"\n",
    "            return None\n",
    "\n",
    "        # get best child: start looking for it in position 0\n",
    "        self.runlog += f\"LS: {len(self.open_list)} children to examine\\n\"\n",
    "        best_index = 0\n",
    "        quality = self.open_list[0].quality\n",
    "        best_so_far: int = quality\n",
    "        \n",
    "        # ====> insert your code below to copy the best solution from the open list into next_soln\n",
    " \n",
    "        # <==== insert your code above to copy the best solution from the open list into next_soln\n",
    "\n",
    "        self.runlog += (\n",
    "            f\"\\t best child quality {best_so_far},\"\n",
    "            f\"\\n\\t best so far {self.best_so_far}\\n\"\n",
    "        )\n",
    "        \n",
    "        # ====> insert your code below here to clear the openlist\n",
    "\n",
    "        # <==== insert your code above here to clear the openlist\n",
    "\n",
    "        # always accept first move\n",
    "        improvement_found: bool \n",
    "        if self.trials == 1:\n",
    "            improvement_found = True\n",
    "        # otherwise there must be an improvement\n",
    "        else:\n",
    "            pass\n",
    "            # value will depend on whether next_soln.quality improves on self.best_so_far\n",
    "            # ====> insert your code below to set  values of  improvement_found and self.best_so_far\n",
    "  \n",
    "            # <==== insert your code above to set values of  improvement_found and self.best_so_far\n",
    "\n",
    "        \n",
    "        # return best offspring from open list or None if it doesn't improve on self.best_so_far\n",
    "        # ====> insert your code below to manage the return\n",
    "  \n",
    "        # <==== insert your code above manage the return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029c561d-1ee2-472f-abf9-64f4273cf893",
   "metadata": {},
   "source": [
    "**Run the next cell to test your code and produce feedback.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544a8024-6964-4821-a605-494e24d122de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define and create problem instance\n",
    "num_vars = 20\n",
    "my_binary_onemax = OneMaxBinary(N=num_vars)\n",
    "\n",
    "# Create search\n",
    "my_search = LocalSearch(my_binary_onemax,\n",
    "                       constructive = False,\n",
    "                       max_attempts= 500,\n",
    "                       minimise=False,\n",
    "                       target_quality=num_vars)\n",
    "\n",
    "# Record quality of random starting point \n",
    "starting_quality = my_search.open_list[0].quality\n",
    "\n",
    "#Run search\n",
    "success = my_search.run_search()\n",
    "\n",
    "#Feedback\n",
    "if success:\n",
    "    print(f'Run found the goal ({num_vars}) '\n",
    "          f'starting from point with quality {starting_quality} '\n",
    "          f'after examining {my_search.trials} solutions.')\n",
    "else:\n",
    "    print(f'Run failed to solve the problem in {my_search.max_attempts} trials\\n'\n",
    "          f'runlog is:\\n {my_search.runlog}')\n",
    "    completed_ok=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ff9ec1-c088-4958-a1a9-a6bd7c1ffb6a",
   "metadata": {},
   "source": [
    "<div style=\"background-color:black;width:100%;height:3px\"></div>\n",
    "<br>\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black\"><h2> Activity 2: Evaluating your implementation of Local Search</h2>\n",
    "    Once your code works and the cell above runs and finds a solution, it is time to evaluate its performance.<br>\n",
    "    Because it usually starts from a different random place every time, Local Search is a <b>stochastic</b> algorithm (the technical term for an algorithm that has a <b>random</b> element). <br>This means that to analyse its behaviour we should run it several times and report the <em>average</em> number of solutions it tries before it finds the goal.<br>\n",
    "    <br><b>How to get started:</b>\n",
    "    <ol>\n",
    "    <li>For each of the problem sizes (10, 15, 20, 25, 30) (i.e. num_vars) we will run the search 10 times and record the number of attempts needed to solve the problem.</li>\n",
    "    <li>Then plot your results as a curve of mean values (y-axis) vs num_vars (x-axis) with error bars showing the standard deviation. The cell below shows you first introduction to the graphics package <b>matplotlib</b>.</li>\n",
    "    <li>To do this you can use two nested loops. See hints below.\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9a5170",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"color:black\"><b>Hints:</b><br>\n",
    "    <ol>\n",
    "<li> If we <code>enumerate</code> a list, python gives us the index and value of each thing it contains.</li>\n",
    "<li>Given three arrays: <br>\n",
    "     - <code>sizes</code> to hold different values tested for the problem size (x-axis), e.g. problem sizes (10, 15, 20, 25, 30)<br>\n",
    "    -  <code>means</code>  and <code>std_deviations</code> to hold the average and variability  of attempts for each size (y-axis),  <br>\n",
    "then you can make a nice plot using the code snippets provided below.<br>\n",
    "\n",
    "You can automate finding the values for these arrays with two nested loops.<br>\n",
    "\n",
    "The first loop is <code>for idx, size in enumerate(sizes)</code> \n",
    "    <ul>\n",
    "        <li>Make an array called <code>attempts</code> full of zeros of size REPEATS (e.g. 10)</li>\n",
    "        <li>Then the second (inside) loop <code>for run in range(REPEATS)</code>:\n",
    "            <ul>\n",
    "                <li>make a new instance of the problem, of the appropriate size</li>\n",
    "                <li>make a new search object <code>my_search</code></li>\n",
    "                <li>call the <code>my_search.runsearch()</code> method</li>\n",
    "                <li>store the number of solutions it looked at <code>my_search.trials</code> in <code>attempts[run]</code></li>\n",
    "            </ul>\n",
    "    <li>Now you can use numpy's built in functions e.g.<code>np.mean(attempts)</code> and <code>np.std(attempts)</code> <br>\n",
    "    to calculate and store the mean and standard deviation of the number of attempts for this problem size <br>\n",
    "    in  <code>means[idx]</code>  and <code>std_deviations[idx]</code></li>\n",
    "    </ul>\n",
    "    Then move on to the next position in your <sizes> array.\n",
    "    </li></ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e3bddf-79e8-462e-9b0f-835b9fe8b280",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\" style=\"color:black\">\n",
    "<b>How to examine results when the algorithm contains randomness:</b><br>\n",
    "Lots of AI algorithms- both for search/optimisation and machine learning - use some form of randomness. <br>\n",
    "    This means that you might get a different result each time you run them on the same problem (or dataset).<br>\n",
    "    So to understand or compare results (scientists typically call these <i>observations</i>) we need to look at:\n",
    "<div>\n",
    "    <div style=\"float:right\">\n",
    "    <img src=\"https://curvebreakerstestprep.com/wp-content/uploads/2021/04/standard-deviation.png\" width=\"300\" height=\"300\" alt=\"Image of two curves with same mean but different standard deviations.\">\n",
    "    </div>\n",
    "    <ol> \n",
    "        <li>The average case behaviour.<br>\n",
    "        Normally we use the <b>mean</b>, which is calculated as the sum of the observed values, divided by the number of observations.</li>\n",
    "        <li>The amount of difference between observations.<br>\n",
    "        Usually we use the <b>Standard Deviation</b>, a measure of how much, on average, results differ from the mean (ignoring the sign of the difference).</li>\n",
    "        </ol>\n",
    "</div>\n",
    "To give a simple example, lets say you run a test in which 5 people score 10, and 5 people score 0.<br>\n",
    "The mean= (5*10 + 5*0)/10 = 5, but the standard deviation = 5 as well - since everyone gets a score 5 different from the mean. <br>\n",
    "    If we rerun the test but this time everyone gets 4 or 6. Now our mean is still 5 (5*4 + 5*6 = 50), but the standard deviation will be 1. \n",
    "    <br>So smaller values of standard deviation means the results are more similar to each other.\n",
    "<br><br> The image on the right shows frequency counts from two more realistic scenarios. <br>\n",
    "    Each has the same number of observations (area under the curve) and  a spread of scores  around the same mean. \n",
    "    <br>In the case with more variability the peak of the curve is lower - because more of the cases are spread away from the mean.\n",
    "</div>\n",
    "\n",
    "**Write your code where indicated in the cell below then run it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b69ef3-8c20-42a5-8e48-a2fc4e090f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max attempts 10000\n",
    "MAX_ATTEMPTS = 10000\n",
    "\n",
    "# number of repetitions\n",
    "REPEATS = 10\n",
    "\n",
    "#these are the three arrays to store your findings in\n",
    "sizes = [10, 15, 20, 25, 30, 40, 50]\n",
    "means = np.zeros(len(sizes))\n",
    "std_deviations=np.zeros(len(sizes))\n",
    "\n",
    "# ====> insert your code below here\n",
    "\n",
    "# <==== insert your code above here\n",
    "\n",
    "# for making the plots\n",
    "from matplotlib import pyplot as plt\n",
    "# plot results    \n",
    "plt.errorbar(sizes, means, yerr=std_deviations)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d48f64f",
   "metadata": {},
   "source": [
    "<div style=\"background:black;width:100%;height:10px\"></div>\n",
    "\n",
    "# Part 2: Local Search for continuous One-max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e1ed83-167e-45ed-a935-83243d0aca06",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\" style=\"color:black\">\n",
    "    <h2> Activity 3: Adapting local search for a continuous problem</h2>\n",
    "    <h3> This is a stretch activity for the more confident coders.</h3>\n",
    "    <p>For continuous problems you will need to adapt your local search class.</p>\n",
    "    <p>This requires adapting more of the methods from the single member search class</p>\n",
    "    <p>I've suggested code that changes the <code>__init__</code> method \n",
    "            to initialise with appropriate continuous values,\n",
    "        and stores the number of samples to take from the neighbourhood each iteration, and whether to use gradient-based search or not.</p>\n",
    "    <p> <b>So the things you need to do are:</b>\n",
    "    <ol>\n",
    "        <li> Over-ride the <code>select_and_move_from_openlist(self)</code> method\n",
    "        from your LocalSearch class so that it now:\n",
    "            <ul>\n",
    "            <li> Makes a copy of the best thing in the open list.</li>\n",
    "            <li> Updates <code>self.best_so_far</code> if appropriate.</li>\n",
    "            <li> Clears the open list</li>\n",
    "            <li> returns the copied solution</li> \n",
    "            </ul>\n",
    "        <li> Look through the new  <code>run_search()</code> method I have provided and make sure you understand it.<br>\n",
    "            Then <b>complete</b> the method where indicated to make it <b>elitist</b>. \n",
    "            In other words, it should keep the current working ccandidate if all its offsprintg are worse..<br>\n",
    "            <b>You can achieve this in one line of code!</b>\n",
    "            </li> \n",
    "        </ol>\n",
    "\n",
    " <h3> This version of the problem has a quality function that is the difference to  the target - so it needs to be minimised</h3>   \n",
    "    <ul>  \n",
    "        <li> Values are allowed to come from the range [0,2]  </li>\n",
    "        <li> The target/goal is the sequence [1.0,1.0,...1.0] (but you could change this)</li>\n",
    "        <li> The <em>cost</em> is 0.5 * the sum of the squared differences in each position <br>\n",
    "        We will see this again it Machine Learning, where it is called the <em> loss</em> function.<br>\n",
    "            It has the nice property of being <em> differentiable</em> i.e., we can calculate  the local <em>gradient</em><br>\n",
    "            - what direction to move from the current solution to reduce the errors/cost  fastest. </li>\n",
    "        <li>\\We  provide that <em>get_gradient() method</em> so you can try both approaches described in the lecture</li> \n",
    "        </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b4c34b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"color:black\"><b>Hints:</b>\n",
    "    <ol>\n",
    "        <li>The first step is very similar to the first activity. The key difference is we are now keeping candidate solutions that are equally as good as the current best.</li>\n",
    "        <li>For the second step, I've given you a simplified version of the <code>run_search()</code> method from the <code>SingleMemberSearch</code> class.</li>\n",
    "        <li>Note: this version of the problem has a quality function that is the difference to the target so it needs to be minimised.</li>\n",
    "</div>\n",
    "\n",
    "**Write your implementation where indicated in the code cell below then run it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280e72ed-36f5-4bde-a15a-f3c053b44c14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LocalSearchContinuous(SingleMemberSearch):\n",
    "    \"\"\"Implementation of local search for continuous problems.\n",
    "      Assumes the search mode is perturbative.\n",
    "      Extends single member search by doing explicit sampling of neighbourhood\n",
    "      and if not stopping if no improvment is  found in an iteration\n",
    "      Parameters\n",
    "      ---------\n",
    "      sample_size(int): \n",
    "          number of neighbours to generate each iteration\n",
    "          default 10\n",
    "      use_gradient(bool): \n",
    "          whether to use the gradient instead of random changes\n",
    "          if the problem supports it.\n",
    "          If set, assume sample_size is 1\n",
    "          default False\n",
    "      learning_rate(float)\n",
    "          multiplier for gradient if used\n",
    "          default 0.5\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"local search continuous\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        problem: Problem,\n",
    "        constructive: bool = False,\n",
    "        max_attempts: int = 50,\n",
    "        minimise:bool=True,\n",
    "        target_quality:float=1,\n",
    "        sample_size:int = 10,\n",
    "        use_gradient:bool=False,\n",
    "        learning_rate=0.5\n",
    "    ):   \n",
    "        super().__init__(problem, constructive=constructive,\n",
    "                       max_attempts=max_attempts,\n",
    "                       minimise=minimise,\n",
    "                       target_quality=target_quality)\n",
    "        print(f'self.target_quality is {self.target_quality}') \n",
    "        \n",
    "        #reinitialise to random continuous values in right range\n",
    "        self.num_vars  = len(self.open_list[0].variable_values)        \n",
    "        for decision in range(self.num_vars):\n",
    "            self.open_list[0].variable_values[decision]= self.rand_in_range()\n",
    "\n",
    "        #re-evaluate\n",
    "        quality = self.problem.evaluate(self.open_list[0].variable_values)\n",
    "        self.open_list[0].quality=quality    \n",
    "        self.trials=1\n",
    "        print(f'starting quality {quality}\\n')\n",
    "\n",
    "        #store the number of neighbours to examine each iteration \n",
    "        self.sample_size = sample_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        #does the problem support calculation of gradients\n",
    "        self.use_gradient= use_gradient\n",
    "        if self.use_gradient: \n",
    "            try:\n",
    "                _=self.problem.get_gradient()\n",
    "                self.sample_size = 1\n",
    "            except: #in case that doesn't work\n",
    "                self.use_gradient=False\n",
    "\n",
    "    def rand_in_range(self)->float:\n",
    "        \"\"\" generates a random number in the range\n",
    "        specified by the problem\n",
    "        \"\"\"\n",
    "        lowest_val = self.problem.value_set[0]\n",
    "        val_range = self.problem.value_set[1] - self.problem.value_set[0]\n",
    "        return np.random.random()*val_range +lowest_val\n",
    "    \n",
    "\n",
    "\n",
    "    def select_and_move_from_openlist(self) -> CandidateSolution:\n",
    "        \"\"\"Pops best thing from list, clears rest of list, then returns best thing\n",
    "        and updates  self.best_so_far\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        next working candidate (solution) taken from open list\n",
    "        None\n",
    "           IF list is empty r\n",
    "        \"\"\"\n",
    "        next_soln = CandidateSolution()\n",
    "\n",
    "        # edge cases\n",
    "        if len(self.open_list) == 0:\n",
    "            self.runlog += \"LS:empty open list\\n\"\n",
    "            return None\n",
    "\n",
    "        ## =====>insert your code below here to pop the best solution from the open_list into next_soln\n",
    "  \n",
    "        ## <====== insert your code above here to pop the best solution from the open_list into next_soln\n",
    "\n",
    "\n",
    "        \n",
    "        #===> insert your code below here to  update self.best_so_far\n",
    "\n",
    "        #<=== insert your code above here to  update self.best_so_far    \n",
    "        \n",
    "        #===> insert your code below here to   clear the openlist\n",
    "\n",
    "        #<=== insert your code above  here to  clear the open list\n",
    "        return next_soln\n",
    "        \n",
    "    def run_search(self) -> bool:\n",
    "        \"\"\"The main loop for single member search.\n",
    "        Returns True/False for success or failure.\n",
    "        \"\"\"\n",
    "\n",
    "        while self.trials < self.max_attempts and not self.solved:\n",
    "            working_candidate = self.select_and_move_from_openlist()\n",
    "            if working_candidate is None:\n",
    "                return False\n",
    "            \n",
    "            for neighbour in range(self.sample_size):\n",
    "                #GENERATE\n",
    "                neighbour = deepcopy(working_candidate)\n",
    "                if self.use_gradient:\n",
    "                    changes = self.problem.get_gradient() *self.learning_rate\n",
    "                else:\n",
    "                    changes= np.random.normal(size= self.num_vars) *self.learning_rate\n",
    "                for pos in range(self.num_vars):\n",
    "                    neighbour.variable_values[pos] += changes[pos]\n",
    "\n",
    "                # TEST\n",
    "                try:\n",
    "                    neighbour.quality = self.problem.evaluate(\n",
    "                    neighbour.variable_values\n",
    "                    )\n",
    "                    reason=''\n",
    "                except ValueError as e:\n",
    "                    reason=e\n",
    "\n",
    "                self.trials += 1\n",
    "                self.update_working_memory(neighbour,reason)\n",
    "                if self.solved:\n",
    "                    return True\n",
    "                \n",
    "            # end over loop of neighbors of working candidate\n",
    "            # ===> insert  your code below to stop search losing the best solution so far\n",
    " \n",
    "            # <=== insert your code above to stop search losing the best solution so far\n",
    "        # while loop has ended\n",
    "        if not self.solved:\n",
    "            self.runlog += \"failed to find solution to the problem in the time allowed!\"\n",
    "        return self.solved\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e916749-7bc6-4908-86d0-94185883664a",
   "metadata": {},
   "source": [
    "**Run the cell below to test your implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eab913-601a-4048-820a-2acc354a01b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define and create problem instance\n",
    "num_vars = 10\n",
    "continuous_onemax = OneMaxContinuous(N=num_vars)\n",
    "\n",
    "# search using option 1 from the lectures - adding Gaussian noise to create neighbours\n",
    "my_search2 = LocalSearchContinuous( \n",
    "                        continuous_onemax,\n",
    "                        constructive = False,\n",
    "                        max_attempts= 10000,\n",
    "                        minimise=True,\n",
    "                        learning_rate=0.05,\n",
    "                        target_quality=0.0\n",
    "                                 )\n",
    "success = my_search2.run_search()\n",
    "\n",
    "#Feedback\n",
    "if success:\n",
    "    print(f'Local Search solved the problem '\n",
    "          f'after {my_search2.trials} attempts.\\n'\n",
    "          f'solution {my_search2.result}\\n'\n",
    "          f'quality {my_search2.problem.evaluate(my_search2.result)}')\n",
    "else:\n",
    "    print(f'Failed to solve the problem in {my_search2.max_attempts} trials\\n'\n",
    "          #f'runlog is:\\n {my_search2.runlog}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66bbbcf-a69b-47cd-b860-004ba75461e5",
   "metadata": {},
   "source": [
    "### Comparing randomised vs gradient-based search.\n",
    "The cell below uses your code, and still works in a series of iterations, but does not make a set of neighbours by adding noise.  \n",
    "Instead, at each iteration  it makes a single neighbour, by estimating the **gradient** - the direction with the greatest slope towards the target -  and then taking a small step in that direction.\n",
    "\n",
    "**Run the cell below** and compare the run-times with randomised results you got before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72077101-54e9-4089-97b5-4d1e4dc67e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search using option 2 from the lectures - using the gradient information\n",
    "mysearch3 = LocalSearchContinuous(continuous_onemax,\n",
    "                        constructive = False,\n",
    "                        max_attempts= 500,\n",
    "                        minimise=True,\n",
    "                        target_quality=0.0,\n",
    "                        use_gradient=True,\n",
    "                        learning_rate=0.5)    \n",
    "\n",
    "success = mysearch3.run_search()\n",
    "if success:\n",
    "    print(f'Local Search solved the problem '\n",
    "          f'after {mysearch3.trials} attempts.\\n'\n",
    "          f'solution {mysearch3.result}\\n'\n",
    "          f'quality {mysearch3.problem.evaluate(mysearch3.result)}')\n",
    "else:\n",
    "    print(f'failed to solve the problem in {mysearch3.max_attempts} trials\\n'\n",
    "          f'runlog is:\\n {mysearch3.runlog}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979b3675-901a-4309-86cc-b8fd3e72d5d1",
   "metadata": {},
   "source": [
    "### It's  time to experiment\n",
    "- How do these two approaches behave as the size of the problem increases: try 5,10,15,20 variables?\n",
    "- How does the variability/reliability of the approaches change if you do repeat runs?\n",
    "- How **Robust**  are the methods to tweaking the parameters that change their behaviour?\n",
    "  - For the first version (`my_search2`) the `learning_rate` parameter controls the size of the random changes made to the working candidate. This will probably have  the biggest impact.\n",
    "  - Lots of research papers (including some of Jim's PhD.) have looked at how to control this effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2543dd5f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"color:black\"><b>Save and close Jupyter:</b>\n",
    "    <ol>\n",
    "        <li>Use the jupyterlab functions to download your work (ask your tutor if you need help with this) and save it somewhere sensible so you can find it easily.</li>\n",
    "        <li>Shutdown the notebook when you have finished with this tutorial (menu->file->close and shutdown notebook</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
