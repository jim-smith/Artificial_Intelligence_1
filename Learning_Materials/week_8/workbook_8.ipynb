{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black\">\n",
    "    <h2>THIS WORKBOOK IS ASSESSED</h2>\n",
    "    In activities 1 and 3 you will implement some standard Machine Learning workflows. They are worth 20 and 75 marks respectively, and the task descriptions describe  how marks are awarded.<br>\n",
    "    The remaining 5 marks will be for multiple-choice questions,  asked when you submit to the marking server.\n",
    "    As before, you will be submitting a file of python code for marking and  the notebooks use a two-stage process to help you.<br>\n",
    "    As you work through activities 1 and 3  in the workbook:\n",
    "    <ol>\n",
    "        <li> <em>Jupyter <code>%%writefile</code> cell magics</em>  saves your code to a file\"studentcode/student_wb8\" as you run cells after editing</li>\n",
    "        <li> Subsequent cells reload your code from that file to test the latest version .</li>\n",
    "    </ol>\n",
    "    When you are satisfied that you have completed the activities as required you should:\n",
    "    <ol>\n",
    "        <li> Restart the kernel and do a single run through of every cell in turn  so <code>studentcode/student_wb8.py</code> has no duplicated definitions.</li>\n",
    "        <li> Run the notebook <b>wb8_selfchecker.ipynb</b> to check for syntax errors when called from our marking code, and edit as needed</li>\n",
    "        <li>Download <code>studentcode/student_wb8.py</code> to your local machine. </li>\n",
    "        <li> Follow the links in the <i>Assessments</i> folder on Blackboard to submit it for automatic marking and feedback.<br>\n",
    "        </li>\n",
    "        <li><b>Read the feedback provided</b> and use it to improve your work.<br>\n",
    "            <b>You will have four attempts to submit each notebook.</b></li>\n",
    "    </ol>\n",
    "<h3> Important instructions about formatting your code cells</h3>\n",
    "<ol>\n",
    "   <li>Where you are asked to insert code, or to complete a code skeleton, <b>you must only put code where indicated</b>.<br>\n",
    "    Otherwise the marking server - or the plagiarism detection we will run later - will not be able to parse and accept them.<br>\n",
    "    So your code should only go\n",
    "       <ul> \n",
    "           <li><b>After</b> the lines marked <code># ====> insert your code below here </code></li>\n",
    "           <li><b>Before</b> the lines marked <code># <==== insert your code above here</code></li>\n",
    "      </ul>\n",
    "   </li>\n",
    "    <li>The marking server will reject any files containing imports except \"approvedimports\".<br>\n",
    "               The marking server has its own copy of that file, so there is no point editing the one we give you.</li>\n",
    "    <li>Even in comments you must not use any of these banned words: <em>system, read,open, import</li>\n",
    "</ol>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:black;width:100%;height:10px\"></div><br>\n",
    "\n",
    "# Workbook 8: Multi-layer Perceptrons (Neural Networks)\n",
    "\n",
    "Overview of activities and objectives of this workbook:\n",
    "\n",
    "1. The first part of this workbook will provide hands-on experience of how Perceptrons can be combined into Neural Networks/Multi-layer perceptrons (MLP) to learn complex models.\n",
    "    - You should gain understanding of the key differences between the two algorithms, such as:\n",
    "        - Non-linear activation functions.\n",
    "        - Using Backpropagation to update (learn) the weights.\n",
    "        - How different nodes learn different aspects of the problem.\n",
    "    - You should also gain practical experience of how network architecture and training parameters affects the outcome.  \n",
    "      This will help you understand the difference between:\n",
    "        - **hyper-parameters** - for example network architectures and learning parameters,   \n",
    "          that can have significant impact on the ability to learn a good model from a data set.\n",
    "        - **hyper-parameter tuning** to allow fair comparisons between different Machine Learning algorithms.\n",
    "    - **The first assessed activity** is producing code to **measure and visualise** the impact of network size on performance.<br><br>\n",
    "\n",
    "2. In the second part of this workbook you will implement a simple Machine Learning workflow - sometimes called a *pipeline*.  \n",
    "   - Workflows are a key part of any Data Science or AI project because they let us  do **fair**, **systematic** and **automated** comparisons between algorithms.\n",
    "    - You will learn how to format the *labels* in your dataset and configure the output layers of your MLP, for problems having more than two different labels.  \n",
    "      This type of problem- known as *multi-class learning* is very common- such as the Iris dataset.\n",
    "   - **The second assessed activity** is to implement a general purpose workflow for comparing different algorithms on a new dataset.<br><br>\n",
    "\n",
    "4. In the third part of this workbook you will learn how use an MLP to a classic image-based problems: learning to recognise hand-written digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:black;width:100%;height:10px\"></div><br>\n",
    "\n",
    "# Part 1: Artificial Neural Networks\n",
    "<img src=\"figures/ANN-2-Node.png\" style=\"float:right\" width= 500>\n",
    "\n",
    "As we have seen, Perceptrons are only capable of solving linearly separable problems.   \n",
    "To overcome this limitation, we can connect Perceptrons together into a network.  \n",
    "Each one becomes a Node in the network, and they are connected together into layers. \n",
    "\n",
    "In standard Artificial Neural Network (ANN) architecture there is one input, one or more hidden layers, and one or more nodes in the output layer.  \n",
    "  - Though the term *input layer* is a bit misleading, it doesn't actually do any computation, it is just the inputs to the network.\n",
    "  - So, outputs of hidden layers become the inputs to subsequent hidden layers, or the final output layer. \n",
    "  - Hidden nodes tend to learn different aspects of the problem space, building more complex decision boundaries and are therefore able to solve more complex problems.\n",
    "  - The common name **Multi-Layer Perceptrons (MLP)** describes this simple layered approach with information flowing from inputs to outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1: Specifying the *architecture* of a Neural Network\n",
    "We use the term architecture to mean the number of nodes and how they are connected in layers.  \n",
    "So the architecture defines how many connections (weights) there are in a given neural network.\n",
    "- **Input layer shape**:\n",
    "   The number of nodes in the input layer must equal the number of inputs/features in the data. \n",
    "- **Output layer shape**:\n",
    "   - One output node can discriminate between two classes (classification problems),  \n",
    "     or predict a value for one continuous variable (regression problems).\n",
    "  - If your data  has more than two classes (or variables to predict),  \n",
    "    the number of output nodes must equal the number of classes/regression variables. \n",
    "- **Between inputs and outputs:**\n",
    "  -   The number of hidden layers and nodes in the layers is arbitrary.\n",
    "  -   Making choices about numbers and sizes of hidden layers is part of building an ANN.<br><br>\n",
    "\n",
    "We use the term <b>capacity</b> to describe the ability of a model to learn complicated decision boundaries.<br>\n",
    "Another way of thinking about it is the model's <b>complexity</b> - the more complex a model, the greater its capacity to learn complicated things.<br>\n",
    "For a multi-layer perceptron this is largely determined  by the number of hidden layers and the <i>width</i> of each - how many nodes they contain. <br>\n",
    "\n",
    "In other words: **finding an architecture that gives good performance is a search  problem**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Neural Network Training Algorithm  \n",
    "\n",
    "For any architecture  we choose, the behaviour of the MLP is defined by the specific values of the connections (weights).  \n",
    "- Each different set of values for weights is a different model.\n",
    "- So learning  <=> finding the set of weights that gives the best performance for that architecture.\n",
    "- Therefore **training is an iterated search process**, moving between models to minimise the error on the training set<br><br>\n",
    "\n",
    "Similar to Perceptrons, for Neural Networks,  each iteration of this search - i.e. the move operator, has  two 'phases'.\n",
    "- The *forward pass*: where data is input into the network to produce an output. \n",
    "- The *backward pass*: where the error in output is used to update the weights using Backpropagation and Gradient Descent.\n",
    "  - note that to calculate what the sum of  inputs was going *in* to a node we apply the *sigmoid derivative* to the signal coming *out* of that node \n",
    "\n",
    "<img src=\"figures/ann-pseudocode.png\" style=\"float:center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:black:width:100%;height:5px\"></div>\n",
    "\n",
    "### 1.3: Worked example: Solving XOR\n",
    "As an introduction to the ANN algorithm, and to give you an intuition for how different nodes and layers in the network learn different aspects of the problem space, we are going to look at how a small network can solve the XOR problem.\n",
    "\n",
    "As there are only four cases, we do not have any test data for this problem - we are just looking at how reliably different sized networks can learn a simple problem.\n",
    "<ul>\n",
    "    <li> One epoch means that all the training data is shown to the system once and the weights are updated.</li>\n",
    "    <li> We know that <i>in theory</i> it should be able to learn XOR with 2 hidden nodes.<br>\n",
    "         But is there a difference between theory and what happens in practice? </li>\n",
    "    <li>Each time you run the cell it starts the whole process from a new set of random weights, so the error curve will be different and you might get different final accuracy scores.</li>\n",
    "    <li> Remember that Stochastic Gradient Descent is a form of local search - so what you are seeing here is the effect of the starting position!</li>\n",
    "</ul>\n",
    "\n",
    "Running the code will train an ANN to solve the XOR problem and produces a visualisation to show how different nodes have learned different aspects of the problem to create a more complex decision boundary (in this case different logical functions).\n",
    "\n",
    "- You do not need to understand *how* the graphs/visualisations are produced.\n",
    "\n",
    "- You should try and understand *what* the graphs/visualisations output means.\n",
    "\n",
    "When you run through the steps, the code will try and learn the XOR problem and show you a plot of how the error rate changes over *time* measured in epochs.\n",
    "\n",
    "\n",
    "\n",
    "**Run the cell below** to import the libraries and define the function that plots the decision surface.\n",
    "- It should not produce any output.\n",
    "- If it reports an error trying to import VisualiseNN, make sure you have downloaded the file VisualiseNN.py and it is in the same directory as this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basics for manipulating and outputting arrays etc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "## MLP specific stuff\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import VisualiseNN as VisNN\n",
    "\n",
    "# useful sklearn functions for preprocessing data and showing results\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Run the  cell below** to creates a dataset for the XOR problem, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell fits an MLP to the XOR problem once for a given network architecture \n",
    "# The code illustrates how to measure accuracy and make a plot\n",
    "\n",
    "# Define the data set - in this case XOR with two inputs\n",
    "train_X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "xor_y = np.array([0, 1, 1, 0])\n",
    "train_y = xor_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Step 2:Run the cell below** to **configure** an MLP (define its architecture), and create an instance with random initial weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "num_hidden_nodes = 3   # <== change this to configure the architecture\n",
    "# Create Multi-Layer Perceptron with one hidden layer of num_hidden_nodes neurons with logistic activation\n",
    "# and Stochastic Gradient Descent (backprop)\n",
    "xorMLP = MLPClassifier(\n",
    "    hidden_layer_sizes=(num_hidden_nodes,),\n",
    "    max_iter=1000,\n",
    "    alpha=1e-4,\n",
    "    solver=\"sgd\",\n",
    "    #verbose=0,\n",
    "    learning_rate_init=0.1,\n",
    "    #random_state=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: run the cell below** to train a model with this architecture on the XOR problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = xorMLP.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Run the cell below** to evaluate the trained model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# measure and print the accuracy\n",
    "# this also shows you how to access the point at which training stopped\n",
    "training_accuracy = 100 * xorMLP.score(train_X, train_y)\n",
    "print(f\"Training set accuracy: {training_accuracy}% after {xorMLP.n_iter_} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Run the cell below** to visualise the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Produce a plot of training loss (error) vs number of epochs\n",
    "fig, ax= plt.subplots(nrows=1, ncols=1)\n",
    "ax.plot(xorMLP.loss_curve_)\n",
    "ax.set_ylim((0.0, 1.0))\n",
    "ax.set_xlabel(\"training epochs\")\n",
    "ax.set_ylabel(\"error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6: Experiment** by re-running steps 2 - 4 with a few other values for the size of the hidden layer.  \n",
    "\n",
    "**Remember** that each model starts from a random set of weights.   \n",
    "- So you should do a few runs for each size (i.e. repeat steps 2-4 five times for each hidden layer size)   \n",
    "- The aim is to **explore** to get a feel for how capacity (hidden layer size) affects how reliably a network  learns a function that computes XOR.\n",
    "- In the next activity you will do this **systematically**\n",
    "\n",
    "This activity investigates this effect for the trivial xor problem, later you will try it for other datasets.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:black;width:100%;height:5px\"></div><br>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black\"><h2>Activity 1: Creating a reliability plot function</h2>\n",
    "    <h3><b>20 marks</b> - using the marking scheme below</h3>\n",
    "    <ul>\n",
    "        <li>0 marks if the code cell with the function <code>make_reliability_plot()</code> contains any text outside the function body.</li>\n",
    "        <li>0 marks if your code does not return the fig and axes objects as required.</li> \n",
    "        <li>10 marks for producing a matplotlib figure containing two matplotlib ax objects with titles and labels as specified below. And returning the objects (i.e. a figure and an array of axes).</li>\n",
    "        <li>5 marks each if the contents of the plots match the <i>reference version</i>. This means you <b>must</b> set the <i>random_state</i> hyperparameter for each run as described below.</li>\n",
    "    </ul>\n",
    "<p>    Complete the function <code>make_xor_reliability_plot()</code> in the cell below to <i>automate</i> the process of investigating the effect of the model <i>capacity</i> (as controlled by <code>hidden_layer_sizes</code> hyper-parameter) for an MLP with a single layer of hidden nodes on:\n",
    "    <ul>\n",
    "        <li>The <i>reliability</i> - as measured by the <i>success rate</i> i.e. the proportion of runs that achieve 100% training accuracy.</li>\n",
    "        <li>The <i>efficiency</i> - the mean number of training epochs per successful run.<br>\n",
    "    <b>Note:</b> to avoid <i>divide-by-zero</i> problems you should check if no runs are successful for a given value and report a value of 1000 in that case.</li>\n",
    "    </ul>\n",
    "    <b>Contents of plots:</b>\n",
    "    <ul>\n",
    "        <li>You must return two objects <i>fig</i> and <i>axs</i> produced by a call to <code>plt.subplots(1, 2)</code>. Axes should be an array of axes with shape (1,2).</li>\n",
    "        <li>The left hand plot should have a title \"Reliability\", y-axis label \"Success Rate\" and x-axis label \"Hidden Layer Width\".</li>\n",
    "        <li>The right hand plot should have a title \"Efficiency\", y-axis label \"Mean Epochs\" and x-axis label \"Hidden Layer Width\".</li>\n",
    "        <li>In both cases the width (number of neurons) of the single hidden layer should cover the range [1, 10] (inclusive) in steps of 1.</li>\n",
    "        <li>Each plot should contain an appropriate line illustrating the results of the experiment.</li> \n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"color:black\"><h2>How to get started</h2>\n",
    "<p>    In several of the stages below you will be adapting code from  above, where we demonstrated how to train an MLP to solve XOR, and 'steps' refer to comments and code snippets in that code cell.</p>\n",
    "    <ol>\n",
    "        <li> Declare a list <code>hidden_layer_width</code> holding the values 1 to 10 (inclusive) defining the model size.</li>\n",
    "        <li> Declare a 1-D numpy array filled with zeros of shape (10,) called <code>successes</code> to hold the number of successful runs for the different model sizes.</li>\n",
    "        <li> Declare a 2-D numpy array filled with zeros of shape (10, 10) called <code>epochs</code>.</li>\n",
    "        <li> Create two nested loops: one over all the values for a variable <code>h_nodes</code> from the list <code>hidden_layer_width</code>, and the other for a variable <code>repetition</code> between 0 and 9 (i.e. doing 10 repetitions).</li>\n",
    "        <li>Inside those loops:\n",
    "            <ol>\n",
    "                <li>Copy and edit code from  step 2 (above)) to create an MLP with one hidden layer containing the <code>h_nodes</code> nodes.<br>\n",
    "                <b>Note:</b> in the call to the MLP constructor you set the parameter <code>random_state</code> to be the run index so the results are the same as mine.</li>\n",
    "                <li>Copy and edit code from step 3 to <i>fit</i> the model to the training data.</li>\n",
    "                <li>Copy and edit code from Step 4 to measure it's accuracy.</li>\n",
    "                <li> If the accuracy is 100%:\n",
    "                    <ul>\n",
    "                        <li><i>Increment</i> the count in the successes array, i.e. <code>successes[h_nodes-1] += 1</code>.</li>\n",
    "                        <li><i>Store</i> the number of epochs taken in the epochs array, i.e. <code>epochs[h_nodes-1][repetition] = xorMLP.n_iter_</code>.</li>\n",
    "                </ul>\n",
    "            </ol>\n",
    "        <li> Declare a 1-D numpy array filled with zeros of shape (10,) called <code>efficiency</code> to hold either:\n",
    "            <ul>\n",
    "                <li> 1000 if no runs got 100% accuracy for that network size.</li>\n",
    "                <li> The mean number of epochs taken per successful run for that network size.</li>\n",
    "            </ul>\n",
    "        <li>Copy and edit the code from step 5 to make a figure containing two plots side-by-side as described in the task definition, set appropriate axis labels and title labels, and return the fig and axs objects.</li>\n",
    "    </ol>\n",
    "</div>\n",
    "\n",
    "**Edit the cell below where indicated** to complete your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile \"studentcode/student_wb8.py\"\n",
    "from approvedimports import *\n",
    "\n",
    "def make_xor_reliability_plot(train_x, train_y):\n",
    "    \"\"\" Insert code below to  complete this cell according to the instructions in the activity descriptor.\n",
    "    Finally it should return the fig and axs objects of the plots created.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_x: numpy.ndarray\n",
    "        feature values\n",
    "\n",
    "    train_y: numpy array\n",
    "        labels\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    fig: matplotlib.figure.Figure\n",
    "        figure object\n",
    "    \n",
    "    ax: matplotlib.axes.Axes\n",
    "        axis\n",
    "    \"\"\"\n",
    "    \n",
    "    # ====> insert your code below here\n",
    "\n",
    "    raise NotImplementedError(\"Complete the function\")\n",
    "    fig, ax = None, None # You will need to change the definition of fig and ax e.g. fig, ax = plt.subplots()\"\n",
    " \n",
    "    # <==== insert your code above here\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the cell below** to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load latest version of your code\n",
    "from sys import path\n",
    "if 'studentcode' not in path:\n",
    "    path.append('studentcode')\n",
    "from importlib import reload\n",
    "import student_wb8 \n",
    "reload(student_wb8)\n",
    "from student_wb8 import *\n",
    "\n",
    "# Redefine the data set to keep this cell contained in case you restart the noteboo\n",
    "train_X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "xor_y = np.array([0, 1, 1, 0])\n",
    "train_y = xor_y\n",
    "\n",
    "# now to test your code\n",
    "make_xor_reliability_plot(train_X, train_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background:black;width:100%;height:5px\"></div><br>\n",
    "\n",
    "\n",
    "## Visualising what the network is doing</h2>\n",
    "The cell below shows an example MLP with 4 hidden layer nodes being created and fitted to the xor problem follow by some simple visualisations.\n",
    "The top plot shows the output of the final node for different inputs.\n",
    "    - In this case we only have the four inputs marked by circles.\n",
    "    - In use, we would apply a threshold of 0.5 to decide whether the output of the network was 1 or 0, so the red/blue lines represent the decision boundaries.\n",
    "    \n",
    "The bottom plot shows a visualisation of the network structure and weights: \n",
    "<ul>\n",
    "    <li>The line thickness represents the magnitude of the weight</li>\n",
    "    <li> The line colour indicates the sign of the weight:<br>\n",
    "           Blue lines are <b>negative weights</b>, so signals down these connections will  suppress the output of the cell they lead to.<br>\n",
    "           Red lines are <b> positive weights</b>- so signals down these connections will  stimulate the node they lead to.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an MLP with 4 hidden nodes\n",
    "xorMLP= MLPClassifier(\n",
    "    hidden_layer_sizes=(4,), max_iter=1000,\n",
    "    alpha=1e-4, solver=\"sgd\",\n",
    "    learning_rate_init=0.1, random_state=5\n",
    ")\n",
    "\n",
    "# Fit the model to the data\n",
    "xorMLP.fit(train_X, train_y)\n",
    "\n",
    "fig3, ax3=plt.subplots()\n",
    "plt.set_cmap('coolwarm')\n",
    "disp = DecisionBoundaryDisplay.from_estimator(xorMLP, train_X, eps=0.1, alpha=0.5, ax=ax3)\n",
    "disp.ax_.scatter(train_X[:,0], train_X[:,1], c=train_y, edgecolor='k')\n",
    "\n",
    "\n",
    "network_structure = np.hstack((2, np.asarray(xorMLP.hidden_layer_sizes), 1))\n",
    "\n",
    "# Draw the Neural Network with weights\n",
    "network = VisNN.DrawNN(network_structure, xorMLP.coefs_)\n",
    "network.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"background-color:black;width:100%;height:10px\"></div><br>\n",
    "\n",
    "# Part 2: Using MLP for multi-class problems: Iris data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "So far we have used multilayer perceptrons for learning binary (two-class) problems.  \n",
    "Last week you should have discussed how you could solve a multi-class problem,  \n",
    "by 'cascading' binary classifiers. \n",
    "This is shown in the  image below for a three class problem.  \n",
    "Here the diamonds represent classifiers, each doing a \"this class or not\" decision.  \n",
    "<img src=\"./figures/cascading.png\" style=\"width:300px\" alt=\"architecture for cascading binary classifers or a 3 class problem\">  \n",
    "\n",
    "In this part we will introduce a different idea, which is to use a  parallel classifier using softmax and one-hot encoding.  \n",
    "Here we  have:\n",
    "- *num_classes* nodes in the output layer \n",
    "- each node predicts whether the input belongs to 'it's' class <br>\n",
    "<img src=\"./figures/mlp-3class.png\" style=\"width:300px\" alt=\"architecture for MLP with three outoutput nodes for a 3 class problem\"></div>\n",
    "\n",
    "The Softmax layer does two things:\n",
    "- it normalises the outputs so they represent probabilities\n",
    "- then a winner-takes-all outputs the class with the highest probability.\n",
    "   \n",
    "Not only is this simpler to manage, it  has the benefit that the classifiers can all share the feature creation done in previous layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 1: Loading the dataset\n",
    "**Run the cell below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "# Load the Iris data\n",
    "iris_data = load_iris(return_X_y=False)\n",
    "# Extract the data and labels, feature names, and label names\n",
    "irisX = iris_data.data\n",
    "irisy = iris_data.target\n",
    "feature_names = iris_data.feature_names\n",
    "label_names = iris_data.target_names\n",
    "\n",
    "print(f\"Iris has {irisX.shape[0]} samples and {irisX.shape[1]} features: {feature_names}\")\n",
    "print(f\"The labels array has shape {irisy.shape}, with values from the set {np.unique(irisy)}\")\n",
    "print(f\"Iris has 3 classes: {label_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Transforming our label data to a format for training a MLP with three output nodes\n",
    "As you can see when you run the cell above, the labels is a 1-D array with labels of 0, 1, or 2.  \n",
    "This is fine for models like nearest neighbours, rule sets or decision trees.  \n",
    "However, (crudely speaking) the output from a neuron tends to be *off* (0) or *on*(1).  \n",
    "So if we want our network to make a choice of three predictions, then we need a node for each class.\n",
    "\n",
    "So there are two changes we make:\n",
    "1. We configure the network to have three output nodes  and use 'softmax' ('winner-takes-all') activation.  \n",
    "    i.e. Each node outputs a value, and we take as our final output the class whose node has the highest output signal\n",
    "2. We convert our labels tell the network what *each of the nodes* should ideally output for each training example.  \n",
    "   In other words:\n",
    "   - if the label is 0 the then output should be [1, 0, 0],\n",
    "   - if the label is 1 it should be [0, 1, 0], and \n",
    "   - if it is 2 the output should be [0, 0, 1].\n",
    "\n",
    "Sklearn comes with a class [LabelBinarizer()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html) to do this, but the cell below does it explicitly to illustrate what is going on.\n",
    "\n",
    "I've made it generic so that you can easily reuse it for different datasets\n",
    "\n",
    "**Run the cell below to create one-hot encoded labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell to create the one-hot version of the labels we need for our MLP\n",
    "num_cases = len(irisy)\n",
    "iris_labels = np.unique(irisy)\n",
    "num_labels = len(iris_labels)\n",
    "print(f\"There are {num_cases} training examples with {num_labels} possible labels:\\n {iris_labels}\")\n",
    "\n",
    "# Make a 2D array with num_cases rows. and num_labels columns\n",
    "irisy_onehot = np.zeros((num_cases, num_labels))\n",
    "\n",
    "# Now loop through the rows of the new array setting the appropriate column value to 1\n",
    "for row in range(num_cases):\n",
    "    label = irisy[row]\n",
    "    irisy_onehot[row][label] = 1\n",
    "\n",
    "print(f\"The set of unique values in the  one-hot version of the labels now looks like this:\\n{np.unique(irisy_onehot, axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Pre-processing the data\n",
    "\n",
    "As for most machine learning algorithms, the problem becomes much easier if we don't have to worry about features having different ranges.\n",
    "\n",
    "A **MinMaxScaler** simply does this independently for each feature (column) *i* in the  data array *x*:\n",
    "- finds the min and max values for feature *i*: $min_i$ and $max_i$\n",
    "- scales each column to a standard range by\n",
    "  - subtracting the minimum column value $min_i$   so that the values now lie between 0 and ($max_i - min_i$)\n",
    "  - dividing by the range **so that the values lie between 0 and 1**  \n",
    "  $ x[row][i] = \\frac {x[row][i] - min_i} {max_i - min_i}$\n",
    "  \n",
    "We can do this in code, explicitly - to its a really common problem so we will reuse other people's work.\n",
    "\n",
    "**Run the cell below** which uses  a sklearn function to scale the data.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Print first 5 rows of irisX\n",
    "print(f\"First five Iris records before scaling:\\n{irisX[:5]}\\n\")\n",
    "\n",
    "# Scale the data\n",
    "iris_x = MinMaxScaler().fit_transform(irisX)\n",
    "\n",
    "# Print first 5 rows of iris_x\n",
    "print(f\"and after scaling:\\n{iris_x[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Splitting our data into a training and a test set\n",
    "\n",
    "As you can see from the output of the cells above, the iris data has groups all the classes i.e. rows 0-49 are 'iris-setosa', 50-99 are 'iris-versicolor'. and rows 100-149 are 'iris-virginica'.\n",
    "\n",
    "So if we want to train our network  and then estimate how well it will do on new data, we need to split this into a training and test set.  \n",
    "Again, we could do this manually:\n",
    "- first shuffling the rows so that we got a mixture of classes, \n",
    "- then taking the first part of the data for training and the second for testing.\n",
    "\n",
    "If the data are not so well organised, or the numbers of examples of different classes are not roughly equal, then that code gets trickier. So the cell below shows how to do this using a method from sklearn.\n",
    "The parameters are, in order:\n",
    "- the feature values (`irisX`)\n",
    "- the onehot-encoded set of labels (`irisy_onehot`)\n",
    "- what proportion of our data we holdback from training, so we can use it for test. We'll use 1/3rd (`test_size=0.33`)\n",
    "- the array holding the labels that we want to be evenly represented in both our training and test sets. (`stratify=irisy_onehot`)\n",
    "\n",
    "This function returns the four different arrays - train and test, x and y.   \n",
    "Note that this function also works if your data is not one-hot encoded - it figures that out for itself.\n",
    "\n",
    "**Run the cell below to make the train/test splits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make train/test split of datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "    irisX, irisy_onehot, test_size=0.33, stratify=irisy_onehot\n",
    ")\n",
    "\n",
    "print(f'Training set has {train_x.shape[0]} examples, test set has {test_x.shape[0]} examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:black;width:100%;height:5px\"></div><br>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black\"><h2>Activity 2: Training a MLP to learn the iris classification problem</h2>\n",
    "<ol>\n",
    "    <li> Start by using the  settings for the MLPClassifier that we had before and just change the size of the hidden layer to five or ten </li>\n",
    "   <ul> \n",
    "       <li> You will probably see that the training stops making improvements before the problem has been fully learned.</li>\n",
    "       <li> This is an example of the backpropagation getting 'stuck' in a <b>local optimum</b>.</li>\n",
    "        <li> It happens because the basic <em>Stochastic Gradient Descent</em> algorithm ('sgd') is a local search method with only crude methods for getting out of 'traps'.</li> \n",
    "       <li> Try changing the solver to 'adam' and see if this gives better performance.</li>\n",
    "    </ul>\n",
    "    <p><b>Remember</b> to run a few times with each setting because this is a randomised algorithm and the random set of initial weights makes a huge difference.</p>\n",
    "    <li> Now try adding a second hidden layer - for example by changing that parameter in the constructor to <code>hidden_layer_sizes=(3, 3)</code>.<br>  \n",
    "    <li> Experiment with a few runs of each configuration to see if the network learns the problem more reliably with one hidden layer of 10 nodes or 2 layers of 5 nodes.</li>\n",
    "    <li>Try to come up with answers to these questions. (these are the sorts of things you might be asked in an exam):\n",
    "        <ul>\n",
    "            <li>Why is the test accuracy sometimes much lower than the training accuracy?</li>\n",
    "            <li>What do you understand by <it>\"better\"</it> performance?</li>\n",
    "            <li>Why is it sometimes less reliable train a network with multiple hidden layers when learning the iris data?<br>\n",
    "            Hint: how many connections are you trying to learn?<br>\n",
    "            How much data have you got?</li>\n",
    "        </ul>\n",
    "</ol>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create an MLP object-  you will want to change the number of hidden nodes\n",
    "irisMLP = MLPClassifier(\n",
    "    hidden_layer_sizes=(5,),\n",
    "    max_iter=1000,\n",
    "    alpha=1e-4,\n",
    "    solver=\"sgd\",\n",
    "    verbose=0,\n",
    "    learning_rate_init=0.05,\n",
    ")\n",
    "\n",
    "# Fit the model to the data\n",
    "irisMLP.fit(train_x, train_y)\n",
    "# notice how it has determined the number of output nodes automatically from the data!\n",
    "print(\"number of output nodes = \" + str(irisMLP.n_outputs_))\n",
    "\n",
    "#make a figure and two axes for plotting\n",
    "fig4,axs4= plt.subplots(nrows=1,ncols=2,figsize=(12,6))\n",
    "# Query the model for its training history and accuracy and display them\n",
    "lossplot = axs4[0].plot(irisMLP.loss_curve_)\n",
    "axs4[0].set_xlabel(\"Training epochs\")\n",
    "axs4[0].set_ylabel(\"Error Rate\")\n",
    "\n",
    "# Report how well it does on the training set\n",
    "training_accuracy = 100 * irisMLP.score(train_x, train_y)\n",
    "print(f\"Training set accuracy: {training_accuracy} %\")\n",
    "\n",
    "\n",
    "# Now how good is our network at predicting data it has never seen before\n",
    "test_accuracy = 100 * irisMLP.score(test_x, test_y)\n",
    "print(f\"Estimated (Test set) accuracy: {test_accuracy}%\")\n",
    "\n",
    "#Plot the confusion matrix\n",
    "predictions = irisMLP.predict(test_x)\n",
    "confusion = ConfusionMatrixDisplay.from_predictions(np.argmax(test_y,axis=1), np.argmax(predictions, axis=1), display_labels=[0,1,2],ax=axs4[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:black;width:100%;height:5px\"></div><br>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black\"><h2>Activity 3: Creating a test workflow to fairly assess three different supervised learning algorithms on a dataset</h2>\n",
    "    <h3><b>75 marks</b> - using the marking scheme below</h3>\n",
    "    Complete the functions in the skeleton class <code>MLComparisonWorkflow</code> below with the following functionality:\n",
    "    <ol>\n",
    "        <li> The <code>__init__</code> method should read in and store a set of input examples and labels from two files (<code>datafilename</code>, and <code>labelfilename</code>) whose names are provided at run-time.<br>\n",
    "            <b>(10 marks)</b>.</li>\n",
    "        <li> The <code>preprocess()</code> method should perform any preprocessing of the stored input examples needed to ensure the comparison between algorithms is fair.<ul>\n",
    "            <li>Using a stratified 70:30 train:test split when separating your data.</li>\n",
    "            <li> Normalising the data so that each feature has the same scale.</li>\n",
    "            <li>Making separate one-hot coded versions of the train/test labels for the MLP if the dataset has 3 or more classes.</li>\n",
    "            <b>(3 x 5 marks)</b></ul></li>\n",
    "        <li> The <code>run_comparison()</code> method should do a fair comparison of the classifier versions of k-Nearest Neighbour, DecisionTree and MultilayerPerceptron algorithms, and store the best accuracy for each.<br>\n",
    "            <i>Fair</i> means doing hyper-parameter tuning for the combinations of values given below and storing each trained model. <br>\n",
    "            <b>(3 x 10 marks).</b><br></li>\n",
    "         <li>   Models should be saved by appending to a list held as the value in a dictionary <code>self.stored_model</code>(see below for details).<br>\n",
    "            You are encouraged to use the scikit-learn versions of all three algorithms as they have common interfaces which will make your coding easier.</li>\n",
    "        <li> The best comparison result for each algorithm, and the location (index) of the stored model, should be stored by creating and then adapting dictionaries called \n",
    "            <ul>\n",
    "                <li><code>self.best_model_index:dict = {\"KNN\":0, \"DecisionTree\":0, \"MLP\":0}</code></li>\n",
    "                <li><code>self.best_accuracy:dict = {\"KNN\":0, \"DecisionTree\":0, \"MLP\":0}</code></li>\n",
    "            </ul>\n",
    "            <b>(10 marks)</b>:\n",
    "        </li>\n",
    "        <li> The <code>report_best()</code> method should report the best performing model, in the format specified.<br>\n",
    "            <b>(10 marks)</b>.</li>\n",
    "    </ol>\n",
    "    <b>Parameters for each model:</b>\n",
    "    <ol>\n",
    "        <li>For the KNearestNeighbor algorithm you should try K values from the set {1,3,5,7,9}</li>\n",
    "        <li>For DecisionTreeClassifer you should try every combination of:\n",
    "            <ul>\n",
    "                <li><code>max_depth</code> from the set {1,3,5}</li>\n",
    "                <li><code>min_split</code> from the set {2,5,10}</li>\n",
    "                <li><code>min_samples_leaf</code> from the set {1,5,10}</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>For MultiLayerPerceptron you should try every combination of:\n",
    "            <ul>\n",
    "                <li>Number of nodes in the first hidden layer from the set {2,5,10}</li>\n",
    "                <li>Number of nodes in the second hidden layer from the set {0,2,5}</li>\n",
    "                <li><code>activation</code> from the set {\"logistic\",\"relu\"}</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ol>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black\"><p><b>How to get started:</b></p>\n",
    "<p>    This task builds heavily on the code in this notebook, and from workbooks 6 and 7, so make sure you have completed those before attempting this task.</p>\n",
    "    <ol>\n",
    "        <li>The examples and labels should be stored in arrays <code>data_x</code> and <code>data_y</code>. You can use <code>np.genfromtxt()</code>.</li>\n",
    "        <li>As your code creates and fits models of different types they should be appended to the relevant list in the <code>stored_models</code> dictionary i.e., each different MLP model gets appended to the list <code>self.stored_models[\"MLP\"]</code> after the call to <code>fit()</code></li>\n",
    "        <li>It probably makes sense to check and update the values held in <i>self.best_accuracy</i> and <i>self.best_model_index</i> as you test each model</li>\n",
    "        <li> It is acceptable to do only one run of each algorithm-hyperparameter combination.</li>\n",
    "        <li> Any code that takes a <code>random_state</code> parameter should be given the value 12345.</li>\n",
    "    <ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"color:black\"><b>Hints:</b> \n",
    "    <ul>\n",
    "        <li><b>Don't over-think this!</b><br> You have most of the code snippets you need and the hyper-parameter tuning is mostly a case of nested loops to run through combinations of values.</li>\n",
    "        <li>From the search topic you should be used to keeping track of 'best-so-far' as you go through options.</li>\n",
    "        <li>The point is that <b>your code should work for different datasets - so don't hard code things about the data</b></li>\n",
    "    </ul>\n",
    "    <p style=\"background:lightpink\">If you get syntax errors that don't seem to go away even after you've fixed them,<br> \n",
    "        this is probably because the cell below <em>appends</em> to the save file, so your buggy code is still there.<br> In that case try deleting the file <code>studentcode/student_wb8.py</code> then rerunning the cell below.<br> But do remember to do a clean-run-through of the whole note book when you're done so you submission file contains the code for both marked activities</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your implementation where indicated in the cell below** then run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a \"studentcode/student_wb8.py\"\n",
    "\n",
    "# make sure you have the packages needed\n",
    "from approvedimports import *\n",
    "\n",
    "#this is the class to complete where indicated\n",
    "class MLComparisonWorkflow:\n",
    "    \"\"\" class to implement a basic comparison of supervised learning algorithms on a dataset \"\"\" \n",
    "    \n",
    "    def __init__(self, datafilename:str, labelfilename:str):\n",
    "        \"\"\" Method to load the feature data and labels from files with given names,\n",
    "        and store them in arrays called data_x and data_y.\n",
    "        \n",
    "        You may assume that the features in the input examples are all continuous variables\n",
    "        and that the labels are categorical, encoded by integers.\n",
    "        The two files should have the same number of rows.\n",
    "        Each row corresponding to the feature values and label\n",
    "        for a specific training item.\n",
    "        \"\"\"\n",
    "        # Define the dictionaries to store the models, and the best performing model/index for each algorithm\n",
    "        self.stored_models:dict = {\"KNN\":[], \"DecisionTree\":[], \"MLP\":[]}\n",
    "        self.best_model_index:dict = {\"KNN\":0, \"DecisionTree\":0, \"MLP\":0}\n",
    "        self.best_accuracy:dict = {\"KNN\":0, \"DecisionTree\":0, \"MLP\":0}\n",
    "\n",
    "        # Load the data and labels\n",
    "        # ====> insert your code below here\n",
    "        raise NotImplementedError(\"Complete the function\")\n",
    "        # <==== insert your code above here\n",
    "\n",
    "    def preprocess(self):\n",
    "        \"\"\" Method to \n",
    "           - separate it into train and test splits (using a 70:30 division)\n",
    "           - apply the preprocessing you think suitable to the data\n",
    "           - create one-hot versions of the labels for the MLP if ther are more than 2 classes\n",
    " \n",
    "           Remember to set random_state = 12345 if you use train_test_split()\n",
    "        \"\"\"\n",
    "        # ====> insert your code below here\n",
    "\n",
    "        raise NotImplementedError(\"Complete the function\")\n",
    "        # <==== insert your code above here\n",
    "    \n",
    "    def run_comparison(self):\n",
    "        \"\"\" Method to perform a fair comparison of three supervised machine learning algorithms.\n",
    "        Should be extendable to include more algorithms later.\n",
    "        \n",
    "        For each of the algorithms KNearest Neighbour, DecisionTreeClassifer and MultiLayerPerceptron\n",
    "        - Applies hyper-parameter tuning to find the best combination of relevant values for the algorithm\n",
    "         -- creating and fitting model for each combination, \n",
    "            then storing it in the relevant list in a dictionary called self.stored_models\n",
    "            which has the algorithm names as the keys and  lists of stored models as the values\n",
    "         -- measuring the accuracy of each model on the test set\n",
    "         -- keeping track of the best performing model for each algorithm, and its index in the relevant list so it can be retrieved.\n",
    "        \n",
    "        \"\"\"\n",
    "        # ====> insert your code below here\n",
    "        raise NotImplementedError(\"Complete the function\")\n",
    "        # <==== insert your code above here\n",
    "    \n",
    "    def report_best(self) :\n",
    "        \"\"\"Method to analyse results.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        accuracy: float\n",
    "            the accuracy of the best performing model\n",
    "\n",
    "        algorithm: str\n",
    "            one of \"KNN\",\"DecisionTree\" or \"MLP\"\n",
    "        \n",
    "        model: fitted model of relevant type\n",
    "            the actual fitted model to be interrogated by marking code.\n",
    "        \"\"\"\n",
    "        # ====> insert your code below here\n",
    "        raise NotImplementedError(\"Complete the function\")\n",
    "        # <==== insert your code above here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the next cell** to test your code before submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load latest version of your code\n",
    "from sys import path\n",
    "if 'studentcode' not in path:\n",
    "    path.append('studentcode')\n",
    "from importlib import reload\n",
    "import student_wb8 \n",
    "reload(student_wb8)\n",
    "from student_wb8 import *\n",
    "\n",
    "\n",
    "# Dump iris data to file\n",
    "iris_x, iris_y = load_iris(return_X_y=True)\n",
    "np.savetxt(\"irisx.csv\", iris_x, delimiter=\",\")\n",
    "np.savetxt(\"irisy.csv\", iris_y, delimiter=\",\")\n",
    "\n",
    "# Run and test comparison constructor\n",
    "ml_comp = MLComparisonWorkflow(datafilename=\"irisx.csv\", labelfilename=\"irisy.csv\")\n",
    "\n",
    "# Check data has been stored correctly\n",
    "assert isinstance(ml_comp.data_x, np.ndarray), \"data_x not loaded\"\n",
    "assert len(ml_comp.data_x.shape) == 2, \"data_x not 2D\"\n",
    "assert isinstance(ml_comp.data_y, np.ndarray), \"data_y not loaded\"\n",
    "assert len(ml_comp.data_y.shape) == 1, \"data_y not 1D\"\n",
    "\n",
    "# Run and test preprocess method\n",
    "ml_comp.preprocess()\n",
    "\n",
    "# Check data has been split correctly\n",
    "assert isinstance(ml_comp.train_x, np.ndarray), \"train_x not created\"\n",
    "assert isinstance(ml_comp.test_x, np.ndarray), \"test_x not created\"\n",
    "assert isinstance(ml_comp.train_y, np.ndarray), \"train_y not created\"\n",
    "assert isinstance(ml_comp.test_y, np.ndarray), \"test_y not created\"\n",
    "\n",
    "assert len(ml_comp.train_x) == np.ceil(len(ml_comp.data_x) / 100 * 70), \"train_x wrong length, should be 70% of data\"\n",
    "assert len(ml_comp.train_y) == np.ceil(len(ml_comp.data_y) / 100 * 70), \"train_y wrong length, should be 70% of data\"\n",
    "\n",
    "# Run and test run_comparison method\n",
    "ml_comp.run_comparison()\n",
    "\n",
    "# Check stored models making sure there are the right number\n",
    "assert len (ml_comp.stored_models[\"KNN\"]) == 5,\"wrong number of stored knn models\"\n",
    "assert len (ml_comp.stored_models[\"DecisionTree\"]) == 27,\"wrong number of stored DT models\"\n",
    "assert len (ml_comp.stored_models[\"MLP\"]) == 18,\"wrong number of stored MLP models\"\n",
    "\n",
    "# Run and test report_best method\n",
    "acc, alg, model = ml_comp.report_best()\n",
    "\n",
    "# Check the accuracy is float, and the algorithm is one of the three\n",
    "assert isinstance(acc, float), \"accuracy not float\"\n",
    "assert acc > 0 and acc <= 100, \"accuracy percentage not between 0 and 100\"\n",
    "assert alg in [\"KNN\", \"DecisionTree\", \"MLP\"], \"algorithm not one of the three\"\n",
    "\n",
    "# Report the best model\n",
    "print(f\"Best test accuracy is {acc}, created by the {alg} algorithm with these hyper-parameters:\")\n",
    "for key, val in model.get_params().items():\n",
    "    print(f\"{key} : {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:black;width:100%;height:10px\"></div><br>\n",
    "\n",
    "# Part 3: An image-based example:Learning to recognise hand-written digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Loading and visualising the data\n",
    "\n",
    "For our example we will use the well-studied <a href=\"https://yann.lecun.com/exdb/mnist/\">MNIST</a>  dataset.\n",
    "\n",
    " **Edit then run** the next cell to specify the right path depending on whether you are using csctcloud or your own installation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find directory holding data depending on what machine you are on\n",
    "import socket\n",
    "import numpy as np\n",
    "if (socket.gethostname()=='csctcloud'): # on csctcloud\n",
    "    datapath = \"/home/common/datasets\"\n",
    "else:  # you will need to change this if you are using data on your local machine\n",
    "    datapath = \"/Users/j4-smith/GitHub/common/datasets/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the next cell to load the data**. It should output the  number of images loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Only  Run this cell if you are using the ccstcloud server\n",
    "# example code to run on the server using a copy of the data that I have already downloaded and made available.\n",
    "# label is column 0\n",
    "# pixel values are from 0-255 so need to be scaled to 0-1\n",
    "\n",
    "test = np.genfromtxt(datapath + \"mnist/mnist_test.csv\", delimiter=\",\")\n",
    "X_test = test[1:, 1:785] / 255\n",
    "y_test = test[1:, 0]\n",
    "\n",
    "train = np.genfromtxt(datapath + \"mnist/mnist_train.csv\", delimiter=\",\")\n",
    "X_train = train[1:, 1:785] / 255\n",
    "y_train = train[1:, 0]\n",
    "\n",
    "print(\n",
    "    f\"X_train has {X_train.shape[0]} rows and {X_train.shape[1]} columns, y_train has {y_train.shape} entries\"\n",
    "    f\"X_test has shape {X_test.shape} y_test has {len(y_test)} entries.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now run this cell** to display some example images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display ten random images from each class\n",
    "print(\n",
    "    f\"The test data has {X_test.shape[0]} images, each described as a {X_test.shape[1]} features (pixel values)\"\n",
    ")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for label in range(10):\n",
    "    imagesForLabel = np.empty((0, 784))\n",
    "    examples = 0\n",
    "    next = 0\n",
    "    while examples < 5:\n",
    "        if int(y_test[next]) == int(label):\n",
    "            imagesForLabel = np.vstack((imagesForLabel, X_test[next]))\n",
    "            examples += 1\n",
    "        next += 1\n",
    "    for col in range(5):\n",
    "        exampleplot = plt.subplot(10, 5, (label * 5 + col + 1))\n",
    "        exampleplot.imshow(imagesForLabel[col].reshape(28, 28), cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Visualising what features the hidden layers learn to respond to.</h2> \n",
    "We will now configure a multilayer perceptron  and training it with all 60,000 images from the standard MNIST training set.\n",
    "\n",
    "The idea for you to learn here is that each hidden node is effectively acting as a pattern detector.\n",
    "<ol>\n",
    "    <li> So let's consider just one hidden layer node: \n",
    "        <ul>\n",
    "            <li> and a simple pattern where the weights from pixels in the top left and bottom right quadrant are all +1, </li>\n",
    "            <li> and the weights from pixels in the top-right and bottom-left quadrants are all -1.</li>\n",
    "        </ul> \n",
    "    </li>\n",
    "    <li> Now consider an input image that has some constant value for every pixel (feature) - i.e. is all the same colour. \n",
    "        <ul>\n",
    "            <li> When these inputs to the node  are multiplied by their weights and summed, they will cancel each other.</li>\n",
    "            <li> So the <b> weighted sum </b> will be zero,</li>\n",
    "            <li> and the <b>output</b> of the node  will be sigmoid(0) = 0.5, which we class as 0</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li> Next consider an the image  of a simple 'chequer' pattern with  white (255) in the top-left and bottom-right quadrants,  \n",
    "  and black (0)  in the other two.\n",
    "        <ul>\n",
    "            <li>In this case  the pattern of  pixel intensities (features) in the image  matches the pattern in the weights.</li>\n",
    "            <li>So then the weighted sum will be at its maximum, and the <b>node will output +1.</b></li>\n",
    "        </ul>\n",
    "</ol>\n",
    "\n",
    "So we can consider our hidden node is acting as a 'feature detector' for the checker pattern.   \n",
    "And in general **each** hidden node is a feature detector that **learns** to recognise useful patterns during training.   \n",
    "Hidden nodes in the 2nd, 3rd,...nth layers build complex features out of those recognised by the layer before.\n",
    "\n",
    "**Run the next set of cells** to:\n",
    "<ul>\n",
    "    <li> Set up and train the network with 16 nodes (a number chosen so we can visualise them neatly in a grid).</li>\n",
    "    <li> Then output the pattern  weights from each of the nodes as an image.</li>\n",
    "</ul>\n",
    "\n",
    "<div class= \"alert alert-warning\" style = \"color:black\">\n",
    "In year 2, the Machine Learning module will explain how this concept of feature detectors has been extended in Deep Convolutional Networks.<br>\n",
    "In these features (called 'filters') can be a smaller size than the image and a process of Convolution (rather than straighforward multiplying) lets them detect small local features anywhere in the image.<br>\n",
    "<b>Convolutional Neural Networks have completely revolutionised the field of image processing and AI for visual tasks.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up and train network\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "num_hidden_nodes = 15\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(num_hidden_nodes), early_stopping=True, verbose=1)\n",
    "\n",
    "# this example won't converge because of CI's time constraints, \n",
    "# so we catch the warning and are ignore it here\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"sklearn\")\n",
    "    mlp.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Training set accuracy: {100*mlp.score(X_train, y_train)}%\")\n",
    "print(f\"Test set accuracy: {100*mlp.score(X_test, y_test)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the weights from the input nodes to the first hidden layer\n",
    "coef = mlp.coefs_.copy()[0].T\n",
    "\n",
    "print(coef[0].max(), coef[0].min())\n",
    "\n",
    "# find endpoints to use for scaling colour range\n",
    "scalemax = coef.max()  # *0.75\n",
    "scalemin = coef.min()  # *0.75\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "numRows = 4\n",
    "numCols = 5\n",
    "\n",
    "for i in range(num_hidden_nodes):\n",
    "    l1_plot = plt.subplot(numRows, numCols, i + 1)\n",
    "    l1_plot.imshow(\n",
    "        coef[i].reshape(28, 28), cmap=plt.cm.seismic, vmin=scalemin, vmax=scalemax\n",
    "    )\n",
    "    l1_plot.set_xticks(())\n",
    "    l1_plot.set_yticks(())\n",
    "    # l1_plot.set_xlabel('Hidden Node %i' % i)\n",
    "title = \"Learned weights from pixels to each hidden node which correspond to patterns the nodes have been trained to respond to.\\n\"\n",
    "title = (\n",
    "    title\n",
    "    + \"Looking at a hidden node:\\n    Parts of the image where a node has weights coloured white (0.0) are ignored.\\n\"\n",
    ")\n",
    "title = (\n",
    "    title\n",
    "    + \"    Blue [red] indicates negative [positive] weights: signals from these pixels suppress [stimulate] the node.\\n\"\n",
    ")\n",
    "title = (\n",
    "    title\n",
    "    + \"    so a sensitive (red) areas might have a blue border to mark whereit must have an edge\"\n",
    ")\n",
    "title = (\n",
    "    title\n",
    "    + \"\\n    Remember that each node could have positive or negative effect on each output node\"\n",
    ")\n",
    "\n",
    "_ = plt.suptitle(title, x=0.15, horizontalalignment=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"color:black\"><h2>Activity 4: MNIST vs Iris</h2>\n",
    "Iris is a simple problem with only 4 features and three classes.\n",
    "\n",
    "MNIST is a much more complicated problem with 784 features and ten classes - some of which (e.g. 4s and sevens) can be drawn in completely different ways.\n",
    "\n",
    "<ol>\n",
    "    <li>So how come the accuracy is roughly the same on these two problems?</li>\n",
    "    <li> The MNIST MLP you just trained and visualised has 10 nodes in its output layer. <br>\n",
    "        Each output node receives <code>num_hidden_nodes</code> (16) input signals. <br>\n",
    "     <li>   <b>This means the hidden layer is effectively learning to  reducing a 784-Dimensional problem to a 16-dimensional one!</b><br>\n",
    "    We call this an <em>embedding</em></li>\n",
    "    <li>From your observations of the visualisations, does it look like we even need 16 hidden nodes/dimensions/features?</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"color:black\"><b>Save and close Jupyter:</b>\n",
    "    <ol>\n",
    "        <li>Use the jupyterlab functions to download your work (ask your tutor if you need help with this) and save it somewhere sensible so you can find it easily.</li>\n",
    "        <li>Shutdown the notebook when you have finished with this tutorial (menu->file->close and shutdown notebook)</li>\n",
    "    </ol>\n",
    "</div"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "aienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
