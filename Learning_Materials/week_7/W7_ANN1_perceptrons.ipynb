{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Perceptrons : the basis of Artificial Neural networks\n",
    "\n",
    "## Artificial Intelligence 1 , week 7\n",
    "\n",
    "This week:  Artificial Neural Networks part 1\n",
    "1. Intro video\n",
    "2. Then taking you through how they developed, step by step\n",
    "  1. Logical calculus \n",
    "  1. perceptrons \n",
    "  1. multilayer perceptrons\n",
    "3. Biological metaphor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Video. https://www.youtube.com/watch?v=bxe2T-V8XRs\n",
    "[![Simple introduction to artificial neural networks](https://img.youtube.com/vi/v=bxe2T-V8XRs/0.jpg)](https://www.youtube.com/watch?v=bxe2T-V8XRs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> The development of neural networks </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 1: Linking logic with computational units <img src=\"figures/Perceptron.png\" style = \"float:right\" width=40%>\n",
    "- McCulloch and Pitts: the Logical Calculus:  \n",
    "  1940’s, predates high level programming languages\n",
    "- Link the logic of the “mind” with the functioning of the brain\n",
    "\n",
    "    UNIT with inputs \n",
    "\n",
    "    OUTPUT  ‘all or none’ output \n",
    "\n",
    "    CONNECTION - weighted  \n",
    "    \n",
    "    THRESHOLD = sum of all weighted inputs needs to meet this for unit to fire  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logical Calculus Units <img src=\"figures/Perceptron.png\" style = \"float:right\">\n",
    "\n",
    "- INPUT: 0 or 1\n",
    "- OUTPUT: 0 or 1\n",
    "- SYNAPSE = weighted connection:  \n",
    "  w<sub>1</sub>, w<sub>2</sub>, bias_weight are all either  +1,0 or -1\n",
    "- THRESHOLD = 0\n",
    "- BIAS INPUT – clamped at 1  \n",
    "  weighting this lets us change the effective threshold\n",
    "\n",
    "- Output = 1 if sum of inputs+bias >0  \n",
    "  Output = 0 if sum of inputs +bias  < 0\n",
    "  \n",
    "The code cell below implements this.  \n",
    "To be consistency with the format for models in sklearn, we will call the \n",
    "output() function predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class two_input_logical_calculus_unit:\n",
    "    def  __init__(self,weight1:int, weight2:int,biasweight:int):\n",
    "        \"\"\" parameters. are weights that define the unit's behavioour\"\"\"\n",
    "        valid = [-1,0,1]\n",
    "        if(weight1 not in valid or weight2 not in valid or biasweight not in valid):\n",
    "            return(\"Error,  weights can only be +1, -1 or 0\")\n",
    "        else:\n",
    "            self.weight1 = weight1\n",
    "            self.weight2 = weight2\n",
    "            self.biasweight = biasweight\n",
    "        \n",
    "        \n",
    "    def predict(self,input1:int,input2:int) -> int:\n",
    "        valid = [0,1]\n",
    "        if (input1 not in valid or input2 not in valid):\n",
    "            return(\"Errors, inputs must be 0 or 1\")\n",
    "        else:\n",
    "            summedInput = input1*self.weight1 +input2*self.weight2 + 1*self.biasweight\n",
    "            if summedInput >0:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How the logical calculus works\n",
    "\n",
    "Take the connectives from logic ( OR, AND, NOR, NAND, XOR)\n",
    "\n",
    "Look at their truth tables\n",
    "\n",
    "Input1 value | Input2 Value | OR(in1,In2) | AND (In1,In2) |NOR(In1,In2) | NAND( In1,In2) | XOR (In1,Inb2)\n",
    "-------------|--------------|-------------|---------------|-------------|----------------|---------------\n",
    "0            | 0            | 0           |    0          | 1           | 1              | 0\n",
    "0            | 1            | 1           | 0             | 0           | 1              | 1\n",
    "1            | 0            | 1           | 0             | 0           | 1              | 1     \n",
    "1            | 1            | 1           | 1             | 0           | 0              | 0     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hand-crafted examples\n",
    " <img src = \"figures/logical_calculus_or.png\" style=\"float:right\" width = 30%> \n",
    " \n",
    " OR has no bias,   \n",
    " so is off by default (zero inputs).    \n",
    " But a signal from  **either** input is enough to turn the output on.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " <img src=\"figures/logical_calculus_nor.png\" style=\"float:right\" width = 30%>\n",
    " \n",
    " NOR is the opposite: A bias of +1 turns it on with no inputs.  \n",
    " But weights from both inputs are negative, so a signal from **either**  is enough to turn it off\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " <img src = \"figures/logical_calculus_and.png\" style=\"float:right\" width = 30%>\n",
    " \n",
    " AND has a bias of -1,   \n",
    " so it is off unless there is a positive signal from **both** inputs.    \n",
    " **Note** bias weight is only difference between AND and OR.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "orUnit  = two_input_logical_calculus_unit( 1,1,0)\n",
    "andUnit = two_input_logical_calculus_unit( 1,1,-1)\n",
    "norUnit = two_input_logical_calculus_unit( -1,-1,1)\n",
    "\n",
    "for in1 in range(2):\n",
    "    for in2 in range(2):\n",
    "        inputs = \"inputs = {}{}:  \".format(in1,in2)\n",
    "        print (\"  {}myOR = {}, myAND = {}, myNOR = {}\".format(inputs,orUnit.predict(in1,in2),andUnit.predict(in1,in2),norUnit.predict(in1,in2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What about XOR ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- no set of weights makes **one** unit behave like XOR\n",
    "- but we can make it by combining others:    \n",
    "  XOR(in1,in2) = AND ( OR(in1,in2), NAND(in1,in2) )\n",
    "    \n",
    "## What's the big deal? \n",
    "Logical Calculus units demonstrate how **logic** (true/false) can be implemented by **computation**\n",
    "- and so you can express  logical structures in terms of networks of computational units\n",
    "\n",
    "Transistors can easily be configured to implement logical calculus units\n",
    "- and so we get modern \"Integrated Circuits\"  which may contain millions of transistors\n",
    " - Apple's new M1 chip has 16 billion transistors!  \n",
    "Hence the modern digital computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## These would be great for building models but…\n",
    "\n",
    "How do we choose the weights? \n",
    "\n",
    "Can we get the computer to do it for us?\n",
    " - Space would be all possible configurations of nodes connected by +/- 1 weights\n",
    " \n",
    "Rosenblatt claimed to have the answer in the 1960s…\n",
    "Based on a simple model of how a nerve cell works\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 2: Inspiration from biology\n",
    "What are computers good at?\n",
    "- calculation \n",
    "- memory.\n",
    "\n",
    "BUT traditional architecture is very poor at some tasks\n",
    "- tasks that we accomplish with ease. \n",
    "- Input: Recognition\n",
    "- Output: Control\n",
    "\n",
    "So can we use biological metaphors to guide engineering decisions, and make more useful tools?\n",
    "\n",
    "**Artificial Neural Networks** are algorithms and architectures inspired by the functioning of the biological nervous system.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic neurobiology <img src=\"figures/neuron.jpg\" style = \"float:right\">\n",
    "\n",
    "**Neurons** (nerve cells):  \n",
    "about 10^11 (100 billion) in humans  \n",
    "[chat-gpt4 estimated to have ~800million nodes](https://www.reddit.com/r/AskComputerScience/comments/15nq4aa/how_many_nodes_does_a_neural_network_need_to_run/?rdt=63837)\n",
    "\n",
    "**Dendrites** – incoming activity\n",
    "\n",
    "**Axon** – output of neuron\n",
    "\n",
    "\n",
    "**Synapses** – connection from axon of one neuron to dendrite of another \n",
    "- 100's to 1000’s per neurone\n",
    "- Each synapse has a weight\n",
    "\n",
    "**Action potential** - electrical signal.  \n",
    "In the soma the incoming electrical activity is summed over\n",
    "- space (many dendrites connected to other neurons)\n",
    "- time (not usually in ANN except 'spiking neural networks')\n",
    "-  if the sum exceeds a **threshold** then ‘all or none’ output,   travels down axon and then is connected via synapses to dendrites of other neurones  where it then adds to their inputs ...\n",
    "\n",
    "\n",
    "\n",
    "**The synaptic weights can be changed during learning (or forgetting)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Now answer the multiple choice questions\n",
    "\n",
    "electrical signals reach the cell body through the:   {soma| dendrites| axons | myelin sheath]\n",
    "\n",
    "number of cells in your brain is: [much less than | about the same as | much bigger than] the number of stars in our galaxy\n",
    "\n",
    "\n",
    "number of connections in your brain is: [much less than | about the same as | much bigger than] the number of stars in our galaxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Perceptron Research from the 50's & 60's, 1 min clip. https://www.youtube.com/watch?v=cNxadbrN_aI\n",
    "\n",
    "[![Perceptron Research from the 50's & 60's, clip](https://img.youtube.com/vi/v=cNxadbrN_aI/0.jpg)]( https://www.youtube.com/watch?v=cNxadbrN_aI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 3: The Rosenblatt Perceptron - the simplest Artificial Neural Network <img src=\"figures/Perceptron.png\" style=\"float:right\">\n",
    "\n",
    "Similar to the logical calculus BUT **weights can be any real number**. \n",
    "\n",
    "However only 1 'layer' : just inputs and units.   \n",
    "units cannot be connected to other units, so there is 1 unit per output.\n",
    "\n",
    "Code is just like above, but now weights can be floats not constrained [-1,0,1]\n",
    "\n",
    "### Rosenblatt proposed an algorithm to get the computer to choose the weights. \n",
    "- Once this was done, perceptrons can handle problems  e.g. *learning* image recognition (letters)\n",
    "- The idea is to **supervise** the procedure (sounds familiar?) \n",
    " - give the perceptron a list of inputs and DESIRED outputs. \n",
    " - the perceptron will learn from that and generalize to previously unseen inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Perceptron Training Law\n",
    "\n",
    "###    ∆ω_i = ε · i · α\n",
    "\n",
    "change in weight from input *i* = error X input *i* X learning rate.\n",
    "\n",
    "**Aside**: Comes from using a squared loss function then taking derivate with respect to the weight  \n",
    "as we discussed in week 4 for local search in continuous spaces.\n",
    "\n",
    "\n",
    "So this means that \n",
    "- Error = target-actual, can be negative \n",
    "- Weights only change when there is an error \n",
    "- Only active inputs are changed. \n",
    "- Inactive (x=0) inputs are not changed at all (which makes sense since they did not contribute to the error). \n",
    "\n",
    "See AI illuminated p297+ for a worked example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class two_input_perceptron:\n",
    "    def  __init__(self,weight1, weight2,biasweight, learningRate):\n",
    "        self.weight1 = weight1\n",
    "        self.weight2 = weight2\n",
    "        self.biasweight = biasweight\n",
    "        self.learningRate = learningRate ## <== this is new\n",
    "        \n",
    "    def predict(self,input1:int,input2:int) -> int:\n",
    "        summedInput = input1*self.weight1 +input2*self.weight2 + self.biasweight\n",
    "        if summedInput>0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def update_weights( self, in1,in2,target):\n",
    "        error = target - self.predict(in1,in2)\n",
    "        if(error != 0):\n",
    "            self.biasweight += error * 1 *self.learningRate # bias is always +1\n",
    "            if (in1>0):\n",
    "                self.weight1 += error * in1 * self.learningRate\n",
    "            if (in2>0):\n",
    "                self.weight2 += error * in2 * self.learningRate           \n",
    "            return 1\n",
    "        else:\n",
    "            return 0     ## <=let the calling fuinction know if it made the right prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Now answer these questions:\n",
    "\n",
    "Perceptrons are useful because they can learn rather than needing to be hand-coded [True| False]\n",
    "\n",
    "The perceptron's output is: \n",
    "- +1 if the sum of the inputs is more than 0\n",
    "- +1 if the weighted sum of the inputs is more than 0\n",
    "- equal to the weighted sum of the inputs\n",
    "- +1 if the sum of thwe weights is more than 0\n",
    "\n",
    "The weight from an input i to the perceptron is not changed when we present a data example ..\n",
    "- If the output for this example is correct\n",
    "- If the value of feature i for this case is 0\n",
    "- Both the above\n",
    "- Neither of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The promise of perceptrons\n",
    "- If the perceptron can handle a problem, then the perceptron is guaranteed to find an answer\n",
    "\n",
    "        The perceptron convergence theorem\n",
    "\n",
    "- Works for OR, AND, NOT and many demonstration problems…\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "# four rows of test cases,   third column is the right answer\n",
    "andData = [[0,0,0],[0,1,1],[1,0,1],[1,1,1]]\n",
    "\n",
    "# start with random weights\n",
    "w0=random()\n",
    "w1 = random()\n",
    "w2 = random()\n",
    "print(\"starting with initial random weights {:.4f}, {:.4f} and {:.4f}\".format(w1,w2,w0))\n",
    "\n",
    "myPerceptron = two_input_perceptron(w1,w2,w0,0.1)\n",
    "\n",
    "# just keep presenting the test cases nd updating until there are no errors\n",
    "for epoch in range(50):\n",
    "    errors = 0\n",
    "    for testcase in range(4):\n",
    "        errors += myPerceptron.update_weights(andData[testcase][0], andData[testcase][1],andData[testcase][2])\n",
    "    if(errors >0):\n",
    "        print(\"in epoch {} there were {} errors\".format(epoch,errors))\n",
    "    else:\n",
    "        print(\" Perceptron solved the learning problem in {} epochs\".format(epoch))\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Do you think perceptrons will be able to learn XOR?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The problem with perceptrons <img src=\"figures/linearly_seperable.png\" style= \"float:right\" width = 50%>\n",
    "The Minsky & Papert book 'perceptrons' showed in detail the limitations of perceptrons \n",
    "- It only deals with linearly separable tasks.  \n",
    "  **more on this next week**\n",
    "- So cannot deal with XOR… \n",
    "-  and pretty much all real world problems.\n",
    "\n",
    "Rosenblatt was aware of this but didn't know how to fix it …\n",
    "- neural network research went into a decline in the 1970s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The answer ? ... add layers\n",
    "\n",
    "You can solve XOR with logical calculus units\n",
    "XOR(a,b) = AND(  OR(a,b) , NAND(a,b). )\n",
    "\n",
    "So why not just do that for perceptrons?\n",
    "\n",
    "## The catch ...\n",
    "\n",
    "Training a single layer of perceptrons is easy\n",
    "- We can measure what the outputs actually are\n",
    "- We can apply the perceptron update rule because\n",
    "- We know what the outputs should be\n",
    "\n",
    "\n",
    "Training a multi-layer perceptron is harder\n",
    "- We can measure what the *outputs* actually are\n",
    "- so we can apply the perceptron update rule to thje last layer\n",
    " - But what should the output from the hidden layers be?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " 1:46 video clip with nice animations\n",
    " [![Video of simple neural netowrk from oolitionTech technologies](https://img.youtube.com/vi/v=gcK_5x2KsLA/0.jpg)](https://www.youtube.com/watch?v=gcK_5x2KsLA)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "- Logical Calculus units simulate logical operations but need hand-crafting\n",
    "- Nature offers us inspiration in the form of nerve cells\n",
    "- Perceptrons with training offer a way of automatically creating single-unit systems that can learn!\n",
    "- Multi-layer perceptrons offer a way of creating more complicated systems\n",
    "\n",
    "## Next Week:  Perceptons as linear classifiers, Multi-Layer Perceptrons\n",
    "<div style=\"float:right;background:lightgreen\" width=600><h1>don't panic!</h1></div>\n",
    "\n",
    "- Architecture\n",
    "- Mathematical foundations\n",
    "- Search/training algorithms  \n",
    "- How to design and use neural networks \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
