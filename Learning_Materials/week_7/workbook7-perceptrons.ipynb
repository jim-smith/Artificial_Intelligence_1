{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptrons - The basis of Artificial Neural Networks\n",
    "\n",
    "Perceptrons, invented by Frank Rosenblatt in the late 1950's,\n",
    "are a form of supervised machine learning algorithm inspired by neuron cells.\n",
    "In neurons, signals come in along the dendrites and out along the axon. \n",
    "A synapse is the connection between the axon of one cell and the dendrites of another.\n",
    "Crudely, input signals are 'summed' and if they reach a certain threshold the neuron 'fires'\n",
    "and sends a signal down the synapse to the connected cells.\n",
    "\n",
    "![Perceptron](figures/Perceptron.png \"Perceptron Image\")\n",
    "\n",
    "Perceptrons are an algorithmic approximation of this process and can learn to solve simple classification problems.\n",
    "Input values are multiplied by a learnable parameter called a *weight*.\n",
    "If the sum of the inputs $\\times$ weights is over a certain threshold the Perceptron 'fires' and generates an output.\n",
    "We use the *error* in the output to change the value of the *weights* by a small amount - the *learning rate*.\n",
    "The process is repeated until the error is 0, or as small as we can get it.\n",
    "\n",
    "**Note:** The threshold which determines if the Perceptron produces an output is determined by its *activation function*.\n",
    "For Perceptrons this is often a step function which outputs a 1 or 0 i.e. 'fires' or not. However, it can also be a\n",
    "non-linear function such as sigmoid, which will always produce a real numbered output in the range 0 to 1.\n",
    "\n",
    "### Perceptron - Algorithm\n",
    "\n",
    "1. Set weights to random values in range [-0.5, 0.5]\n",
    "\n",
    "2. Set learning rate to a small value, usually less than 0.5\n",
    "\n",
    "3. For each training example in the dataset i.e one 'epoch'\n",
    "\n",
    "    A. Calculate output (activation)\n",
    "    \n",
    "    $sum = \\sum\\limits_{i=0}^{n} w_i \\times x_i$\n",
    "      \n",
    "    $if\\ sum >\\ 0 \\\\ \\;\\;\\;activation = 1 \\\\ \\\\else \\\\ \\;\\;\\;activation = 0$\n",
    "       \n",
    "    B. Calculate error\n",
    "    \n",
    "    $error = target \\, output - activation$\n",
    "\n",
    "    C. Update each of the weights values\n",
    "    \n",
    "    $change \\, in \\, weight = error \\times input \\times learning \\, rate$\n",
    "\n",
    "\n",
    "4. Repeat from step 3 until error is 0 (or as close as possible), or for the number of training epochs.\n",
    "\n",
    "### Perceptrons - Logical Operators\n",
    "\n",
    "We are going to use binary data to show that Perceptrons can learn to represent logical functions,\n",
    "though you could also think about it as a prediction/classification problem\n",
    "i.e. for a given set of inputs what is the correct output.\n",
    "A truth table can be used as the Perceptrons *training* data, with each row representing an input example.\n",
    "Each training example has two inputs (*features*) and one output (*label*).\n",
    "\n",
    "| Input 1| Input 2| AND | OR  | XOR |\n",
    "|:------:|:------:|:---:|:---:|:---:|\n",
    "| 0      | 0      | 0   | 0   | 0   | \n",
    "| 0      | 1      | 0   | 1   | 1   |\n",
    "| 1      | 0      | 0   | 1   | 1   |\n",
    "| 1      | 1      | 1   | 1   | 0   |\n",
    "\n",
    "First we will import some python modules and then create the training data.\n",
    "\n",
    "**Note:** Input data is often denoted as X and labels/target outputs with Y.\n",
    "Here we are going to use **inputs**, but the target outputs have been labeled **AND**, **OR** and **XOR**.\n",
    "This is so we can be clear about what the outputs should be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"color:black\"><h2> Activity 1: How the perceptron learns simple binary functions</h2>\n",
    "Work through the first few cells ‘Logical Operators’, ‘Implementation’ and ‘Linear Decision Boundary’ sections and try to familiarise yourself with the algorithm and how the process builds a model by learning values for the weights.\n",
    "<ul>\n",
    "    <li>How well does it perform on different logical functions?</li>\n",
    " <li>You do not need to remember the equation for the calculating the decision boundary.</li>\n",
    "    <li> But you should try and understand how this decision boundary relates to the Perceptrons output (and why it can only be straight).</li>\n",
    "    </ul>\n",
    "    </div>\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random\n",
    "# Create input and target output data\n",
    "inputs = [[0, 0],\n",
    "          [1, 0],\n",
    "          [0, 1],\n",
    "          [1, 1]] \n",
    "print(f\"Input data: {inputs} \" )\n",
    "\n",
    "target_outputs_AND = [0, 0, 0, 1]\n",
    "print(f\"target_outputs_AND: {target_outputs_AND}\" )\n",
    "\n",
    "target_outputs_OR = [0, 1, 1, 1]\n",
    "print(f\"target_outputs_OR: {target_outputs_OR} \")\n",
    "\n",
    "target_outputs_XOR = [0, 1, 1, 0]\n",
    "print(f\"target_outputs_XOR: {target_outputs_XOR}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Implementation\n",
    "\n",
    "Now lets write a function to build and train a Perceptron.\n",
    "This is just an implementation of the algorithm above, except we are going to train one **step** or one **epoch** at a time.\n",
    "This allows us to see what the algorithm is doing more clearly.\n",
    "\n",
    "- A training **step** applies the algorithm to just one input example (A, B and C above).\n",
    "- An **epoch** repeats the training step for all input examples in the data (so in this case 4).\n",
    "\n",
    "First we define the learning rate and model - run the next cell to define the class we used in the lectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class two_input_perceptron:\n",
    "    def  __init__(self,weight1, weight2,biasweight, learningRate):\n",
    "        self.weight1 = weight1\n",
    "        self.weight2 = weight2\n",
    "        self.biasweight = biasweight\n",
    "        self.learningRate = learningRate \n",
    "        \n",
    "    def predict(self,input1:int,input2:int) -> int:\n",
    "        ## step 1 multiply each input by its weight and sum them\n",
    "        summedInput = input1*self.weight1 +input2*self.weight2 + self.biasweight\n",
    "        ## step 2 compare sum to thrreshold (0) to decide output\n",
    "        if summedInput>0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def update_weights( self, in1,in2,target):\n",
    "        # calculate error: difference between target output and actual output\n",
    "        # error could be -1, 0 or 1\n",
    "        error = int (target - self.predict(in1,in2))\n",
    "        # for each input and bias, apply the update rule \n",
    "        # update = error *  input  * learningRate\n",
    "        if(error != 0):\n",
    "            self.biasweight += error * 1 *self.learningRate # bias is always +1\n",
    "            self.weight1 += error * in1 * self.learningRate\n",
    "            self.weight2 += error * in2 * self.learningRate           \n",
    "            return 1\n",
    "        else:\n",
    "            return 0     ## <=let the calling function know if it made the right prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now run the cell below to:\n",
    "- create a perceptron instance, \n",
    "- set it's  **learning_rate** variable determines how large a change we will make to the weights each time they are updated.\n",
    "\n",
    "- run the model for the and data\n",
    "\n",
    "As it trains, in each epoch you will be told when it makes a prediction error, and what the updated weights are you should see output for the current inputs and target outputs,\n",
    "training step, epoch and total error for that epoch.\n",
    "\n",
    "<div class=\"alert alert-warning\" style=\"color:black\" ><h3> Activity 1.1: Run the cell below once and make sure you understand which weights are being updated at each step, and why.</h3></div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### from random import random\n",
    "\n",
    "target = target_outputs_AND\n",
    "# start with random weights\n",
    "w0=random()\n",
    "w1 = random()\n",
    "w2 = random()\n",
    "print(f\"\\t\\t\\t\\t\\tInitial random weights:   w1 {w1:.4f}, w2 {w2:.4f} biasweight {w0:.4f}\")\n",
    "\n",
    "myPerceptron = two_input_perceptron(w1,w2,w0,0.1)\n",
    "\n",
    "\n",
    "# just keep presenting the test cases nd updating until there are no errors\n",
    "for epoch in range(50):\n",
    "    errors = 0\n",
    "    for testcase in range(4):\n",
    "        # get the inputs to present\n",
    "        input1 = inputs[testcase][0]\n",
    "        input2 = inputs[testcase][1]\n",
    "        # make the prediction\n",
    "        prediction = myPerceptron.predict(input1,input2)\n",
    "        # print changes\n",
    "        messageString = f\"Input {input1} {input2}:\"\n",
    "        messageString  += f\"target {target[testcase]}, predicted {prediction} so error = {(target[testcase]-prediction):2d}.   \"\n",
    "\n",
    "        thiserror = myPerceptron.update_weights(input1,input2,target[testcase])\n",
    "        errors += thiserror\n",
    "        \n",
    "        messageString += f\"After updates:   w1 {myPerceptron.weight1:.4f}, w2 {myPerceptron.weight2:.4f} biasweight {myPerceptron.biasweight:.4f}\"\n",
    "        print(messageString)\n",
    "\n",
    "\n",
    "    if(errors >0):\n",
    "        print(f\"Overall in epoch {epoch} there were {errors} errors\\n\")\n",
    "    else:\n",
    "        print(f\" Perceptron solved the learning problem in {epoch} epochs\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-warning\" style=\"color:black\"><h3> Activity 1.2: Checking your understanding</h3>\n",
    "<ol>\n",
    " <li>Now make a prediction about whether you will see exactly the same thing when you run the cell again.<br>\n",
    "     Be honest: Write down your answer, and your reasoning *before* you run the cell :)</li>\n",
    " <li>Run the cell above once more.  Was your answer right?  If not, why not?</li>\n",
    " <li> Change line 3 of the code so that the target is now target_outputs_OR . <br> \n",
    "      Again make a prediction about what you expect to see before you run the code.</li>\n",
    " <li> Change line 3 of the code so that the target is nowtarget_outputs_XOR. <br>  \n",
    "     Again make a prediction about what you expect to see before you run the code.</li>\n",
    "    <li>Finally run the next cell and answer the questions in it.</li>\n",
    "    </ol>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import workbook8_mcq\n",
    "display(workbook8_mcq.Q1)\n",
    "\n",
    "display(workbook8_mcq.Q2)\n",
    "\n",
    "display(workbook8_mcq.Q3)\n",
    "\n",
    "display(workbook8_mcq.Q4)\n",
    "\n",
    "display(workbook8_mcq.Q5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Perceptrons learn Straight Decision Boundaries!\n",
    "<img src=\"figures/straightLine.png\" width=\"300\" style=\"float:right\">\n",
    "\n",
    "To give you an intuition for what the Perceptron is doing, consider the equation for a straight line:\n",
    "\n",
    "$y = mx + c$\n",
    "\n",
    "a and c are coefficients just like the learned weights and bias in the Perceptron.\n",
    "\n",
    "Now lets think about when the perceptron's behaviour (output) changes as the inputs vary.\n",
    "- We know that the output depends on whether the sum of the weighted inputs is greater than zero (output 1) or not (output 0).\n",
    "- But if we are using the perceptron to make predictions,   \n",
    "  then saying that *the behaviour changes when ...*    \n",
    "  is the same as saying: *the decision boundary is when ...*\n",
    "  \n",
    "- In other words **the decision boundary for a perceptron** is when $y =0$ where      \n",
    "$y = input1 \\times weight1 \\;\\; + input2 \\times weight2\\;\\; + \\;\\;bias\\_weight$  \n",
    "\n",
    "\n",
    "Setting $y = 0$ and re-arranging the equation in terms of the two inputs gives:\n",
    "\n",
    "$input2 = - \\frac{weight1}{weight2} \\times input1 - bias \\times \\frac{weight}{weight2}$\n",
    "\n",
    "Which is the same form as the equation for a straight line where:\n",
    "- the ratio of the bias to weight2 defines the intercept  \n",
    "  i.e., the critical value of input2 when input1 = 0\n",
    "- the ratio of weights 1 and 2 defines the slope/gradient of the line (a)  \n",
    "  i.e., how much the critical value of input2 changes each time input1 changes by +1 \n",
    "\n",
    "So for any given value of input1, we can use this equation to tell us the critical value of input2\n",
    "- above that the output is 1,  below that, the output is 0\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\" style=\"color:black\"><h3> Activity 1.3 interactively changing weights to mimic automatic learning</h3>\n",
    "    Run the code cell below to create an interactive widget that allows you to manually adjust the weights, and see how the decision boundary moves.<br>\n",
    "    You don't need to understand the python in the first cell- it sets up the sliders, the radio buttons and the plot widget.\n",
    "    <ul>\n",
    "        <li> When you click to show different functions, the colour (target) of the dots at (0,0),(0,1),(1,0) and (1,1) change.</li>\n",
    "        <li> The sliders let you manually control the values of the perceptron weights </li>\n",
    "        <li> The red line shows the decision boundary calculated from the weight values </li>\n",
    "        <li> When the weights are correct (i.e. the perceptron will correctly predict) that function:<br>\n",
    "             red dots should be 0 (below the line), <br>\n",
    "            green dots should 1 (above the line </li>\n",
    "    </ul>\n",
    "    <b>Tasks</b>    <ol>\n",
    "        <li> Try different functions, and see if you can manually tweak the slider values so that the red line separates the red and green dots </li>\n",
    "    <li> You might find it helpful to  go around the four points (00,10,01,11) in turn,<br>\n",
    "        looking at whether they are on the wrong side of the line, <br>\n",
    "        and if so moving the appropriate sliders a little  by hand, to mimic what the update mechanism does automatically</li>\n",
    "    <li> Then answer the questions in the second cell.   </li>\n",
    "    </ol>\n",
    "    </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "weight1 = widgets.FloatSlider(value=-0.5,min = -1,max = 1)\n",
    "weight2 = widgets.FloatSlider(value=0.5,min = -1,max = 1)\n",
    "biasweight = widgets.FloatSlider(value=-0.5,min = -1,max = 1)\n",
    "funcToModel = widgets.RadioButtons(options=['OR','AND','XOR'])\n",
    "output=interact(workbook8_mcq.showPerceptron, w1=weight1,w2=weight2,bias=biasweight,func = funcToModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(workbook8_mcq.Q6)\n",
    "\n",
    "display(workbook8_mcq.Q7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class =\"alert alert-warning\" style=\"color:black\">\n",
    "    <h2> Activity 2: Learning from  examples with  continuous features </h2>\n",
    "    \n",
    "The aim of this task is to illustrate how Perceptrons can be applied to a larger, more realistic, dataset with real numbered features.  \n",
    "\n",
    "We'll use the term **epoch** to mean one presentation of each of the training set cases.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Perceptrons - Training and Testing on Real-Fake Data\n",
    "\n",
    "Truth table data and logical functions are a good way to learn the Perceptron algorithm but the data isn't very realistic.\n",
    "\n",
    "Most problems are much more complex and cannot be represented with binary data or solved with only 4 training examples.\n",
    "We were also only training for one **step** (one input example) or one **epoch** (all input examples) at a time, so that we\n",
    "could see what the algorithm was doing.\n",
    "\n",
    "In supervised learning, generally we want to train for a fixed number of epochs, or until there is no improvement in\n",
    "the error on the training data. Once training is finished we apply the model (trained weights) to some test data and\n",
    "measure its performance. This gives us an indication of how well it would perform on new data it has not 'seen' before.\n",
    "\n",
    "Next we will train and then test a Perceptron on a larger, real numbered dataset so that we can see the process of \n",
    "applying machine learning in practice.\n",
    "\n",
    "As before, we will first import some python modules and then randomly generate some training and test data.\n",
    "This time the features of the data will be real numbers but there are still only 2 classes/labels, 0 and 1.\n",
    "\n",
    "\n",
    "- In the first cell,  The make_blobs function generates a random dataset.\n",
    "  The 'centers' variable determines how many classes/label are in the data,  \n",
    "  'n_features' is the number of features each example has, and  \n",
    "  cluster_std is the 'spread' of each class i.e. how randomly scattered they are from each other.\n",
    "- The other call in the first cell splits the data created  into train and testsets. \n",
    "  - this is described more fully later on in this workbook.\n",
    "\n",
    "- you **don't** have to understand how the visualisation code works\n",
    "\n",
    "It's also helpful to plot the data so that we can see how it is distributed so we'll just use 3D data as we can visualise that\n",
    "\n",
    "<div class=\"alert alert-warning\" style=\"color:black\">\n",
    "    <h3> Activity 2:1 Create, split, and visualise the data.</h3>\n",
    "    Run the next two cells to:\n",
    "    <ul> \n",
    "        <li>generate the data,</li>\n",
    "        <li>split it  into training and test sets,</li>\n",
    "        <li>then visualise the data in 3D</li>\n",
    "        </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import some needed modules\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Generate random dataset\n",
    "num_samples = 150\n",
    "features, labels = make_blobs(n_samples=num_samples, centers=2, n_features=3, cluster_std=2.0, random_state=0)\n",
    "\n",
    "# Split data to training and test data, 2/3 for training and 1/3 for testing\n",
    "train_X, test_X, train_y, test_y = train_test_split(features, labels, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Print some information about the data\n",
    "print(f\"Shape of training data: {train_X.shape} \")\n",
    "print(f\"Shape of test data: {test_X.shape}\" )\n",
    "print(\"First 5 rows of training data:\")\n",
    "print(train_X[: 5, :])\n",
    "print(\"First 5 labels of training data:\")\n",
    "print(train_y[:5])\n",
    "\n",
    "#use the blob labels to set a value for the colours of the markers in our plots\n",
    "# boring bit of numpy manipluation here :)\n",
    "trlabels = train_y.astype(str)\n",
    "trlabels = np.where(trlabels=='0','red',trlabels)\n",
    "trainlabels = np.where(trlabels=='1','blue',trlabels)\n",
    "telabels = test_y.astype(str)\n",
    "telabels = np.where(telabels=='0','red',telabels)\n",
    "testlabels = np.where(telabels=='1','blue',telabels)\n",
    "\n",
    "\n",
    "\n",
    "# Plot the training and test data side by side \n",
    "fig=plt.figure(figsize=(12,6))\n",
    "ax1 = fig.add_subplot(121, projection='3d') \n",
    "ax2 = fig.add_subplot(122,projection='3d')\n",
    "    \n",
    "ax1.scatter(train_X[:,:1], train_X[:,1:2],train_X[:,2:3],c=trainlabels)\n",
    "ax1.title.set_text(\"Train\")\n",
    "ax2.scatter(test_X[:,:1], test_X[:,1:2],test_X[:,2:3],c=testlabels)\n",
    "ax2.title.set_text(\"Test\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###  A more generic perceptron \n",
    "The class below extends the two-input perceptron class from above, so that it:\n",
    "1. Is more generic - and  can cope with any number of inputs.   \n",
    "   To achieve that we move the weight initialisation from the constructor to the fit() function.  \n",
    "   This lets it  deal with data with any number of features by querying the data's *shape* attribute  \n",
    "2. Has a **fit()** method that run the training for a number of epochs,  instead of having a loop outside the class.  \n",
    "  - This also means it will match the sklearn models way of doing things.\n",
    "  - Training will stop:\n",
    "    - when  the whole training set is presented (one epoch) with no prediction errors, or  \n",
    "    - after a fixed number of epochs have been run - because we can't assume 100% accuracy is achievable!\n",
    "3. Implements a **predict()** method that  takes a set of items to predict as a parameter.  \n",
    "   This method can be used  to estimate the performance of our model on a held back test set.  \n",
    "   It just loops over the cases calling a methgod PredictOneCase()\n",
    "4. Implements a method **predictOneCase()** that presents the  set of feature values corresponding to one new item/case to the network and returns the network's output value\n",
    "\n",
    "<div class = \"alert alert-warning\" style=\"color:black\">\n",
    "    <h2> Activity 2.2 Explore the effect of training parameters on learning</h2>  \n",
    "    <b>Read through, then run, the two cells below.</b>   If you don't understand it,  ask your tutor or one of the others in your class!<br><br>\n",
    "    <b>Experiment</b> with changing the \n",
    "    <ul>\n",
    "        <li>maxEpochs - how many iterations of model adaptation happen, and</li>\n",
    "        <li>learningRate - how big is step-size of each adaptation</li>\n",
    "    </ul>to explore what  effect these have on how well the perceptrons learns the training data.<br> Remember to run the second cell a few times since the perceptron starts with a random model (set of weights).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class n_input_perceptron:\n",
    "    def  __init__(self, learningRate,debug=True):\n",
    "        # The only wieght we  set in the constructor is a random initial  bias weight\n",
    "        self.biasweight = random()\n",
    "        self.learningRate = learningRate \n",
    "        self.debug = debug\n",
    "        \n",
    "    def fit(self, data:np.ndarray, targets=[], maxEpochs=10):\n",
    "        ## start by setting up the perceptron\n",
    "        \n",
    "        #ask the data how many columns it has\n",
    "        self.numInputs= data.shape[1]\n",
    "        \n",
    "        # then create that many weights and randomise them\n",
    "        self.weights = np.zeros(self.numInputs)\n",
    "        for i in range(self.numInputs):\n",
    "            self.weights[i] = random()\n",
    "        \n",
    "        # set the maximum training time\n",
    "        self.maxEpochs = maxEpochs\n",
    "\n",
    "        # now the training loop - this is exactly the same code as in the earlier cell\n",
    "        # just tweaked to match the size of the data our perceptron is asked to model\n",
    "        # 1 epoch <=> present each trainig case once\n",
    "   \n",
    "        numTrainingCases = data.shape[0]\n",
    "        # run for at most a fixed number of epochs\n",
    "        for epoch in range(self.maxEpochs):\n",
    "            errors=0\n",
    "            correct=0\n",
    "            # within each epoch present each trainig item\n",
    "            for traincase in range(numTrainingCases):\n",
    "                # get the inputs to present- now it is a 1d array\n",
    "                inputValues = data[traincase]\n",
    "        \n",
    "                # make the prediction\n",
    "                prediction = self.predictOneCase(inputValues)\n",
    "            \n",
    "                #  update weights  if there were errors\n",
    "                if (prediction != targets[traincase]):\n",
    "                    self.update_weights(inputValues,targets[traincase], prediction)\n",
    "                    errors = errors+ 1\n",
    "                else:\n",
    "                    correct = correct +1\n",
    "                    \n",
    "            # Now all the training cases have been presented in this epoch\n",
    "            # if there were no errors, say so and  quit training, \n",
    "            # otherwise print a message        \n",
    "            if(errors ==0):\n",
    "                if(self.debug):\n",
    "                    print(f\" Perceptron solved the learning problem after {epoch+1} epochs\")\n",
    "                break\n",
    "            else:\n",
    "                if(self.debug):\n",
    "                    print(f\"in epoch {epoch} there were {errors} errors and {correct} correct\")\n",
    "                pass\n",
    "        \n",
    "\n",
    "    # the predict() method will now accept a set of cases to make predictions for\n",
    "    # it just runs a loop repeatedly calling a function to predict for one case\n",
    "    def predict(self,data):\n",
    "        #ask the data how many rows it has\n",
    "        numToPredict = data.shape[0]\n",
    "        predictions = []\n",
    "        for newCase in range(numToPredict):\n",
    "            predictions.append (self.predictOneCase(data[newCase]) )\n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "    # this is the old predict method, changed to take an array of inputs instead of just two\n",
    "    def predictOneCase(self,inputValues) -> int:\n",
    "        summedInput =  self.biasweight  * 1.0 # since bias always has the value 1.0\n",
    "        for i in range(self.numInputs):\n",
    "            summedInput += inputValues[i] * self.weights[i]\n",
    "        if summedInput>0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # slightly changed the update method to take both the target and the prediction    \n",
    "    def update_weights( self, inputValues,target, prediction):\n",
    "        error = int (target - prediction)\n",
    "        \n",
    "        # no updates if there were no errors!\n",
    "        if(error != 0):\n",
    "             # bias is always +1 so biasweight is always changed\n",
    "            self.biasweight += error * 1 *self.learningRate\n",
    "            \n",
    "            # loop through the weights updating them if necessary\n",
    "            for i in range (self.numInputs):\n",
    "                # only updating the weights if the corresponding input was 1\n",
    "                if (inputValues[i]>0):\n",
    "                    self.weights[i] += error * inputValues[i] * self.learningRate          \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thePerceptron = n_input_perceptron(learningRate=0.05)\n",
    "thePerceptron.fit(train_X,targets=train_y, maxEpochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class = \"alert alert-warning\" style=\"color:black\">\n",
    "    <h2>Activity 2.3: Testing the trained perceptron on unseen data</h2>\n",
    "\n",
    "Run the two cells below to: \n",
    " <ul>\n",
    " <li> Test the trained model by making predictions on the test data.<br> \n",
    "       Note that now we just call <b>predict()</b> but we dont update the weights.</li>\n",
    "  <li> Calculate the decision boundary by putting the weights from the trained perceptron into the formula from activityt 1.3</li>\n",
    "    <li> Plot the training and test data again, and show where the decision boundary is.</li>\n",
    "    </ul>\n",
    "    </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ask the trained perceptron to predict labels some unseen data\n",
    "\n",
    "#remind ourselves how much test data we have\n",
    "print( f\"the test data and label arrays have sizes {test_X.shape} and {len(test_y)}\")\n",
    "numTestCases = test_X.shape[0]\n",
    "\n",
    "#ask the trained perceptron to make predictions\n",
    "predictions = thePerceptron.predict(test_X)\n",
    "\n",
    "# compare those to the trie values to see how well the perceptron did\n",
    "correct = 0\n",
    "for case in range (numTestCases):\n",
    "    if (predictions[case] == test_y[case]) :\n",
    "        correct += 1\n",
    "print (f\"On the unseen test data the perceptron made {correct} correct predictions and {numTestCases - correct} errors so the accuracy is {100*correct/numTestCases}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the next cell to get a visualisation of the learned decision boundary\n",
    "You dont have to go into the matplotlib details of  how the plots are created.   \n",
    "The decision boundary is created by querying the perceptron's learned weights and putting them into ther formula from activity 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def show_perceptron_decision_boundary(train_X,train_y,test_X,test_y,thePerceptron):\n",
    "    \"\"\"method to visualise a decision boundary and train/test sets\"\"\"\n",
    "\n",
    "    # Create the range of values for decision boundary\n",
    "    x_range = np.linspace(train_X[:,0].min(), train_X[:,0].max(), 10)\n",
    "\n",
    "    # query the weights learned by the model\n",
    "    biasweight = thePerceptron.biasweight\n",
    "    weight1 = thePerceptron.weights[0]\n",
    "    weight2 = thePerceptron.weights[1]\n",
    "\n",
    "    # apply the formula from activy 1.3 to get the y-value for the decision boundary\n",
    "    # at each x-point we just made\n",
    "    y_range = [((-weight1/weight2) * x) + (-biasweight/weight2) for x in x_range]\n",
    "\n",
    "    # Plot the training  and test data with the decision boundary\n",
    "    figure, ax = plt.subplots(1,2,figsize=(16, 8))\n",
    "    ax[0].scatter(x=train_X[:,0], y=train_X[:,1], c=trainlabels)\n",
    "    ax[0].plot(x_range, y_range, color='r')\n",
    "    ax[0].set_title(\"Training data\")\n",
    "    ax[1].scatter(x=test_X[:,0], y=test_X[:,1], c=testlabels)\n",
    "    ax[1].plot(x_range, y_range, color='r')\n",
    "    ax[1].set_title(\"test data\")\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_perceptron_decision_boundary(train_X,train_y,test_X,test_y,thePerceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"color:black\"><h2> Activity 3: Learning from data with more  features and classes </h2></div\n",
    "\n",
    "We will use the Iris data to illustrate how to do this.   \n",
    "This data set has 4 features and 3 classes.  \n",
    "<img src=\"figures/cascading.png\" style=\"float:right\">    \n",
    "Since perceptrons can only make two-way distinctions so we have a choice of options:\n",
    "\n",
    "1. (simplest) we create three classifiers - one to recognise each class. \n",
    "  - This requiresa way of specfying how to combine  their votes,\n",
    "  - and what to do if all three say \"not in class\".\n",
    "\n",
    "\n",
    "    \n",
    "2. (slightly more complex) use a cascade approach (shown in image)\n",
    "- first train a network to predict if a training item items is setosa or not. \n",
    "- then use the training items that are predicted 'not-setosa'   \n",
    "  to train a second perceptron that predicts   versicolor-or virginica\n",
    "    \n",
    "\n",
    "    \n",
    "In the next few cells we show how to create a cascading classifier\n",
    "    \n",
    "As usual we begin by loading the data set and checking we have split it correctly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "#useful functions\n",
    "from  sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "#the iris data\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "irisX,irisy = load_iris(return_X_y = True)\n",
    "feature_names = ['sepal width','sepal_length','petal_width','petal_length']\n",
    "irisLabels = np.array(('setosa','versicolor','virginica'))\n",
    "# show what the data and labels looks like\n",
    "print(f\" item 0   feature values are {irisX[0]} and label is {irisy[0]}\")\n",
    "print(f\" item 50  feature values are {irisX[50]} and label is {irisy[50]}\")\n",
    "print(f\" item 100 feature values are {irisX[100]} and label is {irisy[100]}\")\n",
    "\n",
    "\n",
    "numcases = len(irisy)\n",
    "print(f'there are {numcases}  training examples')\n",
    "thelabels = np.unique(irisy)\n",
    "numlabels = len(thelabels)\n",
    "print( f'there are {numlabels}  labels {thelabels}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting our data into a training and a test set\n",
    "As you can see from the output of the cells above, the iris data has groups all the classes i.e. rows 0-49 are 'iris-setosa', 50-99 are 'iris versicolor'. and rows 100-149 are 'iris-virginica'.\n",
    "\n",
    "So if we want to train our network  and then estimate how well it will do on new data, we need to split this into a training and test set.  \n",
    "\n",
    "The cell below shows how to do this using a method from sklearn.   \n",
    "The parameters are, in order:\n",
    "- the feature values (irisx)\n",
    "- the  set of labels (irisy)\n",
    "- what proportion of our data we holdback from training, so we can use it for test. We'll use 1/3rd ( test_size=0.33)\n",
    "- the array holding the labels that we want to be evenly represented in both our training and test sets. (stratify=irisy_onehot)\n",
    "\n",
    "This function returns the four different arrays - train and test, x and y.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_train_X, iris_test_X, iris_train_y, iris_test_y = train_test_split(irisX,irisy, test_size=0.33, stratify=irisy )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to set up  version of the labels which just treat the data as setosa (0) or not (1),\n",
    "by setting the values 2 (virginica) to 1\n",
    "and the same thing to make  some versicolor labels\n",
    "\n",
    "- look up numpy.where() if you have not come across this function before\n",
    "- **put your cursor inside the () of a method then click shift+tab to get help onscreen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original labels are 0=setosa, 1=versicolor 2 = virginica\n",
    "\n",
    "\n",
    "# to make setosa (1) vs not-setosa (0) labels\n",
    "# we first turn all the 2's to 1's\n",
    "setosa_train_Labels = np.where(iris_train_y==2,1, iris_train_y)\n",
    "setosa_test_Labels = np.where(iris_test_y==2,1, iris_test_y)\n",
    "# then do 1- current values ot swap 0s for 1s and vice-versa\n",
    "setosa_train_Labels = 1- setosa_train_Labels\n",
    "setosa_test_Labels = 1- setosa_test_Labels\n",
    "\n",
    "# to make versicolor be 1 not not verrsicolor be 0 we jsut turn 2's to 1's\n",
    "versicolor_train_Labels = np.where(iris_train_y==2,0, iris_train_y)\n",
    "versicolor_test_Labels = np.where(iris_test_y==2,0, iris_test_y)\n",
    "\n",
    "print(\"first 25 original  labels {}\".format(iris_test_y[:25]))\n",
    "print(\"first 25 setosa    labels {}\".format(setosa_test_Labels[:25]))\n",
    "print(\"first 25 versicol. labels {}\".format(versicolor_test_Labels[:25]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will train a perceptron to do the *setosa*: *not-Setosa* recognition task.  \n",
    "Then we will make predictions  and get the id's of the training items classified as *not-setosa*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSetSize = iris_train_X.shape[0]\n",
    "\n",
    "# Step 1: Train a perceptron for the setosa : not setosa task\n",
    "print(\"training the first stage classifier\")\n",
    "setosaClassifier =  n_input_perceptron(learningRate=0.05,debug=True)\n",
    "setosaClassifier.fit(iris_train_X,targets=setosa_train_Labels, maxEpochs=20)\n",
    "\n",
    "setosaPredictions= setosaClassifier.predict(iris_train_X)\n",
    "\n",
    "#we can easily count how many we predicted as setosa like this\n",
    "num_setosa_predictions = np.array(setosaPredictions).sum()\n",
    "num_not_setosa_predictions = iris_train_X.shape[0]- num_setosa_predictions\n",
    "\n",
    "print(f\"the first stage made {num_setosa_predictions}:{num_not_setosa_predictions} setosa:not_setosa predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Collect subsets of training data  not predicted to be setosa, \n",
    "#.  and the labels for versicolor-verginica task\n",
    "\n",
    "#we found the number of items being passed through in the last cell\n",
    "#use this to allcoate new arrays\n",
    "not_setosa__X = np.empty((num_not_setosa_predictions,4))\n",
    "not_setosa_y = np.empty(num_not_setosa_predictions)\n",
    "\n",
    "#now loop through making copes of every training item not predicted to be setosa\n",
    "# but this time taking the versicolor labels as our targets\n",
    "new_index = 0\n",
    "for i in range(trainSetSize):\n",
    "    if (setosaPredictions[i]==0):\n",
    "        not_setosa__X[new_index] = iris_train_X[i]\n",
    "        not_setosa_y[new_index] = versicolor_train_Labels[i]\n",
    "        new_index += 1\n",
    "        \n",
    "#check we got them all\n",
    "assert new_index == num_not_setosa_predictions\n",
    "\n",
    "#print(not_setosa_y)\n",
    "print (f\"shape of data and labels being passed to second classifier are {not_setosa__X.shape} and {len(not_setosa_y)}\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Training a perceptron for the versicolor(1) -- virginica(0) task\n",
    "print(\"training the second stage classifier\")\n",
    "versicolorClassifier =  n_input_perceptron(learningRate=0.05,debug=True)\n",
    "versicolorClassifier.fit(not_setosa__X,targets=not_setosa_y, maxEpochs=20)\n",
    "\n",
    "##### YOU MAY WANT TO RUN THIS CELL A FEW TIMES TO GET A GOOD RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Put together our two-stage classifier\n",
    "def CascadePredict(newItem,setosaClassifier,versicolorClassifier ):\n",
    "    \n",
    "    thePrediction = -1\n",
    "    firstStagePrediction = setosaClassifier.predictOneCase(newItem)\n",
    "    if (firstStagePrediction==1):#classifiers says setosa\n",
    "        thePrediction = 0\n",
    "    else:\n",
    "        secondStagePrediction = versicolorClassifier.predictOneCase(newItem)\n",
    "        if (secondStagePrediction==1):\n",
    "            thePrediction= 1 #versicolor\n",
    "        else:\n",
    "            thePrediction = 2 #virginica\n",
    "\n",
    "    return thePrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets see how it does!\n",
    "\n",
    "twoStagePredictions = []\n",
    "twoStageCorrect = 0\n",
    "\n",
    "for i in range (iris_test_X.shape[0]):\n",
    "    pred = CascadePredict( iris_test_X[i], setosaClassifier, versicolorClassifier )\n",
    "    twoStagePredictions.append(pred)\n",
    "    if ( pred == iris_test_y[i]):\n",
    "        twoStageCorrect += 1\n",
    "        \n",
    "print(f\" Final outcome{twoStageCorrect} out of {iris_test_X.shape[0]} correct test predictions from Cascaded classifier\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "cm = confusion_matrix(iris_test_y, twoStagePredictions)\n",
    "CMPlot=ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=(\"setosa\",\"versicolor\",\"virginica\"))\n",
    "CMPlot.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\"> Please save your work (click the save icon) then shutdown the notebook when you have finished with this tutorial (menu->file->close and shutdown notebook</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"> Remember to download and save your work if you are not running this notebook locally.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "py10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
