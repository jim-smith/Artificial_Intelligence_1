{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Artificial Intelligence Topic 2 Machine Learning\n",
    "\n",
    "## Week 5: Introduction, Ethics and types\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This Week:\n",
    "###  Background\n",
    "- What is ML: recap\n",
    "- Types of ML\n",
    "- Ethical Considerations: Creating and Using Data\n",
    "\n",
    "###  Types of Machine Learning\n",
    "- Unsupervised Learning: K-Means as an example\n",
    "- Reinforcement Learning\n",
    "\n",
    "\n",
    "### Next few weeks: \n",
    "- Supervised Machine Learning, \n",
    "- Artificial Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap:<img style=\"float:right\" width=300 src=\"./figures/basic-model-fitting.png\">\n",
    "- Machine Learning is:\n",
    " - the application of inductive logic to a dataset\n",
    " -  to create useful predictive models.  \n",
    " - So it is about solving  modelling problems\n",
    " \n",
    "\n",
    "- In week 1 we learned that problem solving is what you do when one of  \n",
    "  Input -> Model -> &  Output     is missing\n",
    "- In the third topic we will look at how we **manually** create models encoding human expertise  \n",
    "- Machine Learning is about how you **automatically** create models  from data (inputs and outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## So it’s all about the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Yes!\n",
    "\n",
    "The aim is to build ML systems that can be used to do things when data or scenarios arise.  \n",
    "So we need data to: \n",
    "- train them on,  \n",
    "- choose between models, \n",
    "- Know (estimate) how well they are going to do when we start using them\n",
    "\n",
    "We may not always have an output for every input \n",
    "- Because they’re not possible to capture\n",
    "  - e.g. data from scientific experiments such as genomics, astrophysics,...\n",
    "- Because sometimes we have to wait a while e.g.,\n",
    "  - game playing\n",
    "  - finding human volunteers to label images/ caption videos,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Types of Machine Learning\n",
    "Type | Inputs | Outputs | Feedback | What drives search? | Examples\n",
    "-----|--------|---------|----------------|---------------------|---------------------\n",
    "**Supervised** | Data |Predictions for each case | Correct labels | Accuracy of predictions made | **Recognition**  speech, images, actions, **Forecasting**\n",
    "**Reinforcement**| Scenarios | Actions to take in different states | Periodic Rewards | Expected future feedback | Learning game strategy\n",
    "**Unsupervised** | Data| Groupings of similar items | None | Statistics about cluster *coherence* and separation | Recommender systems, search engines\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Ethics: *Can* we use the data?\n",
    "- GDPR,  Privacy policies affect how it is collected\n",
    "- The law is very clear that we have to give people the right to:\n",
    "  - Provide Informed Consent about  how we are going to use their personal data at the time we collect it\n",
    "  - Find out what information we hold about them\n",
    "  - Withdraw their data (e.g. “right to forget”)\n",
    "\n",
    "- Examples of unethical use:    \n",
    " - Cambridge Analytica,   \n",
    " - targeting of fake news, propaganda on social media\n",
    "\n",
    "- Nowadays there should be clear collaboration agreements describing who is the data controller and who is the data processor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ethics: *Should* we use the data? <img src=\"figures/algorithms-of-oppression.png\" style=\"float:right\" width = 100> \n",
    "<img style=\"float:right\" src=\"figures/protected-characteristics.png\" width = 600>\n",
    "\n",
    "- ML is only as good as the data we give it\n",
    "- So we have to be very careful that the data is representative\n",
    "- Examples of problems:\n",
    "  - Microsoft's (abandoned) Tay Bot\n",
    "  - <a href=\"https://aibusiness.com/document.asp?doc_id=767688\">Google to remove gender labels from image recognition tool 25/2/21</a>\n",
    "  - <a href=\"https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/\">Predictive policing algorithms are racist. They need to be dismantled.</a>\n",
    "  -  <a href=\"https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G\">Amazon scraps secret AI recruiting tool that showed bias against women</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example of bias in Image-Net\n",
    "Image net is the standard dataset used by many AI teams and researchers.  \n",
    "'Synsets' are sets of words with common meaning e.g. \"manager\", \"teacher\", \"cleaner\", \"nurse\", ...\n",
    "\n",
    "These plots show the bias.  For example, if gender bias did  not exist, the left plot would show a flat green-blue border  at approximately 50% \n",
    "![Series of plots showing how almost all occupations show bias in terms of gender, skin colour and age.](https://www.image-net.org/static_files/figures/demographcs_distribution.png)\n",
    "\n",
    "Image from https://www.image-net.org/static_files/figures/demographcs_distribution.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Quick Video from Cog-x\n",
    "\n",
    "[![AI Ethics with Dong Nguyen, The Alan Turing Institute | CogX17 Highlight | CogX](https://img.youtube.com/vi/v=M-ko82Y0GUQ/0.jpg)](https://www.youtube.com/watch?v=M-ko82Y0GUQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsupervised Learning : finding groups in data\n",
    "- We don’t have labels ... but we still want to find useful groups<img src=\"figures/clustering.png\" style=\"float:right\" width=400>\n",
    "- All data is defined in terms of values for features\n",
    "  - Numbers,  \n",
    "  - categories (_colour_, _name_, _Uni course_),  \n",
    "  - binary ( _present_,_absent_)\n",
    "- So we define distance measure _d(a,b)_ between two data items _a_ and _b_.\n",
    "  - Hamming Distance (number of features where _a_ and _b_ differ)\n",
    "  - Euclidean (straight line) distance for continuous numbers\n",
    "- Typically in clustering we look for a way of putting the data items into _k_ clusters  \n",
    "- As a search problem we are seeking models that maximise *Quality of Clustering*\n",
    "  - Minimise Intercluster Distance:  \n",
    "   max ( _d(a,b)_ ) for all _a,b_, in **same** cluster\n",
    "  - Maximise Intracluster distance:\n",
    "    min(_d(a,c)_) for _a_ and _c_ in **different** clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K Means: probably the best known clustering algorithm\n",
    "\n",
    "Basic Idea:  \n",
    "\n",
    "- clusters defined by a set of *centroids* (mid-points)\n",
    "  - centroids probably not \"real\" data items\n",
    "- data items assigned to the cluster with the closest centroid\n",
    "\n",
    "Algorithm\n",
    "- start with randomly picked items as  centroids\n",
    "- Loop until no changes:\n",
    "  - assign items to clusters\n",
    "  -  move each centroids to the new mid-point of all the items in the cluster\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-Means : Pseudocode- (not search based version)\n",
    "```\n",
    "#Step 0: init\n",
    "Pick K data points at random to be cluster “centroids” C_k k=1,...,K\n",
    "Set Converged = False\n",
    "  \n",
    "# Main loop\n",
    "WHILE Converged= False:  \n",
    "  ```\n",
    "\n",
    "```   \n",
    "    #Step 1: Assign items to clusters  \n",
    "    For each data point i:  \n",
    "      For each cluster k:\n",
    "        Calculate distance d(i,C_k) \n",
    "      Assign i to  cluster with smallest value of d(i,C_k) \n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "    #Step 2: Check to see if the algorithm has converged\n",
    "    If no datapoints have moved cluster:\n",
    "       Converged = True\n",
    "    Else:\n",
    "       Remember new cluster membership\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "``` \n",
    "    #Step 3 Update cluster centroids\n",
    "    Foreach cluster k in (1...K):\n",
    "      Set new cluster centroid C_k =  mean position of  points in cluster\n",
    "      Update Cluster metrics\n",
    "      \n",
    "IF (converged==True):\n",
    "  return cluster centroids\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### K-means as a search problem in *N* dimensions\n",
    "- CandidateSolution.variable_values:  \n",
    "   list of *N* x *K* values specifying the co-ordinates of *K* centroids\n",
    "- KMEANSproblem: \n",
    "    - reads in the data set when initialised\n",
    "    - quality  is calculated as sum of inter- and intra- cluster distances **for a given set of centroids**  \n",
    "      (i.e. a candidate solution)\n",
    "    - if we define the quality function carefully, we can calculate **gradient**  \n",
    "       - how to move each centroid to improve quality  \n",
    "- Solve KMeans by using gradient as the move operator in a local search algorithm.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start by importing some modules we will use\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import the apples-oranges-bananas data set from the first week\n",
    "\n",
    "# columns in X are Red,Green,Blue,Width,Height,Weight,Type\n",
    "\n",
    "# read in all the data from the apples-oranges-bananas dataset\n",
    "alldata = np.genfromtxt('data/fruit_values.csv', delimiter=',')\n",
    "\n",
    "#select the first two  feature values to make visualisation easier\n",
    "X = alldata[:,:2]\n",
    "\n",
    "numItems = X.shape[0]\n",
    "\n",
    "numFeatures  = X.shape[1]\n",
    "print(f\" The shape of X is {X.shape} so there are {numItems} items, each with {numFeatures} features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def AssignItemsToClusters(X, centres):\n",
    "    '''returns an array with index of the closest cluster for each item'''\n",
    "\n",
    "    # X is 2d array of data items, centres is 2d array of cluster centres\n",
    "    \n",
    "    # X.shape[0] and centres.shape[0] tell us the number of items and clusters\n",
    "    # X.shape[1] and centres.shape[1] tell us how many columns (features) they have\n",
    "    \n",
    "    assert X.shape[1] == centres.shape[1] #make sure they have same number of features\n",
    "    \n",
    "    #make 1D numpy array to hold an (integer) cluster label for each item\n",
    "    clusterLabels = np.zeros(X.shape[0],dtype=int)    \n",
    "    \n",
    "    # loop over all rows in the data array\n",
    "    for thisItem in range(X.shape[0]):\n",
    "        \n",
    "        # start by guessing  first cluster is closest\n",
    "        clusterLabels[thisItem] = 0\n",
    "        closestClusterDist = getDistance( X[thisItem], centres[0])\n",
    "        \n",
    "        #then loop over the other clusters looking for one closer\n",
    "        for thisCluster in range(1, centres.shape[0]):\n",
    "            clusterDist = getDistance( X[thisItem], centres[thisCluster])\n",
    "            if(clusterDist < closestClusterDist):\n",
    "                clusterLabels[thisItem] = thisCluster\n",
    "                closestClusterDist = clusterDist\n",
    "        \n",
    "    return clusterLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getDistance(a,b):\n",
    "    ''' gets the Euclidean (straight line) distance between two items a and b'''\n",
    "    ''' this is just Pythagoras' theorem in N-dimensions'''\n",
    "    #a and b must have same number of dimensions/feastures\n",
    "    assert a.shape[0] == b.shape[0]\n",
    "    distance=0.0\n",
    "    for feature in range( a.shape[0]):\n",
    "        difference = a[feature] - b[feature]\n",
    "        distance= distance + difference*difference\n",
    "    return math.sqrt(distance)          \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def PlotCluster(X,labels,centres, oldcentres,iteration):\n",
    "    fig, ax= plt.subplots(figsize=(7.5, 7.5))\n",
    "    \n",
    "    # show a scatter plot  coloured by labels\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=labels)\n",
    "    #put the new centres on\n",
    "    for k in range(centres.shape[0]):\n",
    "        ax.plot(oldcentres[k][0],oldcentres[k][1], marker = '*',color='r',markersize=12)\n",
    "        \n",
    "    #draw lines to show movements\n",
    "    for k in range (centres.shape[0]):\n",
    "        ax.annotate(\"\", xy=centres[k], xycoords='data',xytext= oldcentres[k],textcoords='data',arrowprops=dict(arrowstyle=\"->\",\n",
    "                            connectionstyle=\"arc3\",color='red'),)\n",
    "    ax.set_xlim((40,120))\n",
    "    ax.set_ylim((40,120))\n",
    "\n",
    "    plotTitle= \"Cluster membership and centroids: iteration \" + str(iteration) \n",
    "    ax.set_title(plotTitle)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def runKMEANS(X):\n",
    "    \n",
    "    #Step 0  pick K random data items as the initial cluster centres\n",
    "    #idx = np.linspace(start= 0,stop=X.shape[0],num=K,endpoint=False,dtype=int)\n",
    "    idx=index = np.random.choice(X.shape[0], K, replace=False)  \n",
    "    centres = copy.deepcopy(X[idx,:] ) #important that we make a separate copy rather than a reference\n",
    "    clusterLabels = AssignItemsToClusters(X,centres)\n",
    "\n",
    "    \n",
    "    for iteration in range(10):                    #MAIN LOOP\n",
    "        oldcentres= copy.deepcopy(centres) #remember for plotting\n",
    "        \n",
    "        # TEST Step 1: assign items to  clusters\n",
    "        newLabels = AssignItemsToClusters(X,centres) \n",
    " \n",
    "        #TEST step 2:  see if algorithm has converged\n",
    "        numMoved = np.sum(newLabels != clusterLabels)\n",
    "        if( (iteration>0) and (numMoved==0)):\n",
    "            print(\"converged after {} iterations\".format(iteration))\n",
    "            break\n",
    "    \n",
    "        #GENERATE step 3: find new cluster centroids\n",
    "        for k in range(K): # loop through each cluster\n",
    "            num_in_cluster = 0\n",
    "            for feature in range(X.shape[1]):\n",
    "                centres[k][feature]= 0.0\n",
    "            for item in range(numItems):\n",
    "                if (newLabels[item]==k) :\n",
    "                    num_in_cluster +=1\n",
    "                    centres[k] += X[item]\n",
    "            if(num_in_cluster >0):\n",
    "                centres[k] /= num_in_cluster\n",
    "        #plot     \n",
    "        PlotCluster(X,clusterLabels,centres,oldcentres, iteration+1) \n",
    "        # move cluster centroids\n",
    "        clusterLabels=newLabels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "K = 3\n",
    "runKMEANS(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Of course you wouldn't normally write your own version...\n",
    "\n",
    "We could easily do this within the framework from topic 1.\n",
    "\n",
    "BUT Highly optimised versions available in well-established frameworks e.g. Weka (Java), scikit-learn (python).\n",
    "\n",
    "`class sklearn.cluster.KMeans(n_clusters=8, *, init='k-means++', n_init=10, max_iter=300, tol=0.0001,...)`  \n",
    " - defult number of clusters, variety of \"smart\" initialisation schemes\n",
    " - n_init: number of repeats it does before returning the best\n",
    " \n",
    "Object attributes include: \n",
    "- `cluster_centers` :ndarray of shape (n_clusters, n_features)\n",
    "- `labels`  : ndarray of shape (n_samples,)\n",
    "- `inertia` : float (Sum of squared distances of samples to their closest cluster center.)\n",
    " \n",
    "Methods include: \n",
    "- `fit(X[, y, sample_weight])` : Compute k-means clustering.\n",
    "or this estimator.\n",
    "- `predict(X[, sample_weight])` : Predict the closest cluster each sample in X belongs to.\n",
    "- `fit_predict(X[, y, sample_weight])` : Compute cluster centers and predict cluster index for each sample.\n",
    "- `get_params([deep])` : Get parameters of the model - including the cluster centroids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-Means Strengths and weaknesses \n",
    "<img src=\"figures/kmeans_clustering_examples.png\" style=\"float:right\">\n",
    "\n",
    "### PROS: \n",
    "- fast, \n",
    "- lots of implementations\n",
    "\n",
    "### CONS:\n",
    "- need right value of K, \n",
    "- results depend on starting points\n",
    "\n",
    "### Assumptions:\n",
    "- all features are relevant, \n",
    "- data is \"globular\" with respect to the current features\n",
    "\n",
    "### How could we fix the counter-example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reinforcement Learning\n",
    "Five minute video.\n",
    "\n",
    "[![Reinforcement learning for bar-tenders](https://img.youtube.com/vi/v=m2weFARriE8/0.jpg)](https://www.youtube.com/watch?v=m2weFARriE8)\n",
    "https://www.youtube.com/watch?feature=oembed&v=m2weFARriE8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reinforcement Learning <img src=\"figures/RL.png\" style=\"float:right\" width=400>\n",
    "\n",
    "Q learning was best known initial algorithm\n",
    "\n",
    "Basic idea is that you have a *Reward* table R\n",
    "- which tells you what reward you get if you are in state s and take action a\n",
    "- for a multi-step problem the immediate rewards might be zero for many states  \n",
    "  e.g. finding your way out of a maze, playing tic-tac-toe (noughts and crosses)   \n",
    "\n",
    "Uses repeated trials to learn a Quality table Q: *s* rows and *a* actions  \n",
    "\n",
    "Start exploring, and build up a history of states (*s<sub>1</sub>*, *s<sub>2</sub>*, ..., *s<sub>t</sub>*) and actions (*a<sub>1</sub>*, *a<sub>2</sub>*, ..., *a<sub>t</sub>*)\n",
    "\n",
    "If at time  *t* you get a reward *r* then:  \n",
    "Q[s<sub>t</sub>][a<sub>t</sub>]  is increased by _r_    \n",
    "and the previous steps get a 'discounted' reward too: Q[s<sub>t-n</sub>][a<sub>t-n</sub>] is increased by  0.9<sup>*n*</sup> * _r_  \n",
    "\n",
    "\n",
    "Over time the Q table learns the best sequences of moves to take => use it to pick the next move\n",
    "    \n",
    "Problems with scalability as numbers of  states and actions increase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Reinforcement Learning  <img src=\"figures/AlphaGoZero.png\" style=\"float:right\" width=300>\n",
    "- neural network rather than table\n",
    "- tends to learn “end-to-end”  rather than a Q table and a policy table\n",
    "  E.g. Alpha Go, Atari simulator\n",
    "- Relies on lots of data:  \n",
    "  e.g. Unity: ‘learning brain’ from ml-agents toolkit links out to tensorflow model\n",
    " \n",
    "- Alpha Go Zero:   \n",
    "  learned by playing itself!\n",
    "  image from https://medium.com/syncedreview/alphago-zero-approaching-perfection-d8170e2b4e48\n",
    " \n",
    " Really nice explanation of Q-learning here: http://mnemstudio.org/path-finding-q-learning-tutorial.htm\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Learning\n",
    "Basic idea: Models divide up “decision space” into regions\n",
    "\n",
    "\n",
    "Search for model is driven by minimising error\n",
    "\n",
    "Form depends on what the ouputs can be\n",
    "- Two class:  0/1 loss (i.e. %age of wrong predictions)\n",
    "- Many class: Cross entropy \n",
    "- Continuous: mean squared error\n",
    "\n",
    "Types of models we’ll look at:\n",
    "- K Nearest Neighbours: make predictions based on *nearby* points\n",
    "- Greedy Rule Induction: use local search to learn a set of rules \n",
    "  - that don't make mistakes and correctly predict as many examples as possible\n",
    "- Decision Trees\n",
    "- Artificial Neural Nets\n",
    "\n",
    "**Classification** algorithms put labels on regions\n",
    "\n",
    "**Regression** algorithms compute a function in regions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The basic supervised learning process\n",
    "1. Choose features according to what kind of data you have available\n",
    "2. Decide what types of model might be appropriate (**CandidateSoliution** in search framework)\n",
    " - human readable?,   \n",
    " - type and amount of data?    \n",
    "3. Initialise Model \n",
    "4.  While not finished:\n",
    "  - See how well it does on training set (**Test** in our search framework)\n",
    " - Adapt model to try and reduce error on training set (**Generate** in our search framework)\n",
    "5.  Try to estimate how good it is (more accurate **Test**)\n",
    "\n",
    "Often do steps 3-5 above in parallel with different types of model or metaparameters\n",
    "E.g. max number of rules, max depth of trees, value of k in kNN, learning rates in ANN   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How we use our data <img src=\"figures/using_data.png\" style=\"float:right\" width=300>\n",
    "\n",
    "### Unsupervised Learning: \n",
    "- Estimate of quality is based on whole dataset,  \n",
    "  so use it all for training\n",
    "\n",
    "### Reinforcement learning:\n",
    "- Problem is usually lack of data  \n",
    "  compared to size of state-action space.  \n",
    "- Because data is only generated by using the algorithm!\n",
    "- Alternate periods of:\n",
    "  - training (explore state-action-reward space to improve model)\n",
    "  - testing (choose current max predicted reward in each state) \n",
    "  \n",
    "### Supervised learning:\n",
    "Most commonly work in *off-line* or  *batch* mode \n",
    "- Random split of the data into separate test set, \n",
    "  training set, and sometimes validation set\n",
    "- Final model then built using all the data available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary: you need to know and understand\n",
    "- When it is legal and ethical to use data\n",
    "- The basic workflow of **preprocess** &rarr; **train** &rarr; **test**\n",
    "- The difference between **Unsupervised**, **Reinforcement**, and **Supervised Learning**\n",
    "- kMeans as a typical unsupervised clustering algorithm\n",
    "  - stochastic, distance-based\n",
    "- The basic idea (but not the details) of reinforcement learning \n",
    "  - immediate rewards for taking action *a* in state *s*\n",
    "  - build up a Q-table predicting future reward for taking action *a* in state *s*\n",
    "- Supervised learning:\n",
    "  - Model represents a set of  *decision boundaries* that divide  feature space into regions\n",
    "  - *Classification*: each region has a label\n",
    "   - *Regression*: each region calculates a number\n",
    "  - adapts (optimises) boundaries to minimise error on the *training set*\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
