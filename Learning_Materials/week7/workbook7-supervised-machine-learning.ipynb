{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workbook 7: Supervised Machine Learning\n",
    "\n",
    "## Description and aims\n",
    "\n",
    "This tutorial is designed to give you your first experience of machine learning in practice by implementing a simple nearest-neighbour classifier.\n",
    "\n",
    "The learning outcomes are:\n",
    "- experience of implementing the K Nearest Neighbours classification algorithm\n",
    "- experience of using the sklearn DecisionTree classification algorithm\n",
    "-  experience of working through different preprocessing steps to try and improve the performance of your classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"color:black\">\n",
    "    <h1>Activity 1: Loading and Visualising Data</h1>\n",
    "   We will start by importing and visualising the  Iris dataset used  in the lecture.\n",
    "<ul>\n",
    "    <li><b>Run the 2 code cells below</b> to load and display the iris dataset</li>\n",
    "            </ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import week7_utils as W7utils\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris flowers <img src=\"figures/Iris-image.png\" style=\"float:right\">\n",
    "- classic Machine Learning Data set\n",
    "- 4 measurements: sepal and petal width and length\n",
    "- 50 examples  from each 3 sub-species for iris flowers\n",
    "- three class problem:\n",
    " - so for some types of algorithm have to decide whether to make  \n",
    "   a 3-way classifier or nested 1-vs-rest classifers\n",
    "- most ML classifiers can get over 90%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "irisX,irisy = sklearn.datasets.load_iris(return_X_y=True)\n",
    "title=\"Scatterplots of 2D slices through the 4D Iris data\"\n",
    "\n",
    "iris_features= (\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\")\n",
    "iris_names= ['setosa','versicolor','virginica']\n",
    "W7utils.show_scatterplot_matrix(irisX,irisy,iris_features,title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"color:black\">\n",
    "    <h1>Activity 2: Implementing K-Nearest Neighbours</h1>\n",
    "</div>\n",
    "            \n",
    "Basic process for predicting the label of a new point from the trainig set\n",
    "1. Measure distance to new point from every member of the training set\n",
    "2. Find the K Nearest Neighbours  \n",
    "   in other words, the K members of the training set with the smallest distances  (*calculated in step 1*)\n",
    "3. Count the labels that were provided for those K training items,  \n",
    "   and return the most common one as the predicted label.\n",
    "\n",
    "Below is a figure illustrating the start and first two steps of process.  \n",
    "It is followed by a code cell with a simple implementation of a class for 1-Nearest neighbours. \n",
    "\n",
    "<b>Read through the code  to get a sense for how it implements the algorithm. </b><br>\n",
    "Your tutor will discuss it with you in the lab sessions.\n",
    "<img src=\"figures/kNN-steps.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for K = 1 \n",
    "\n",
    "class simple_1NN:\n",
    "\n",
    "    def __init__(self, verbose = True):\n",
    "        #we'll use straight line distance -code from week6 reproduced in this week's utils file\n",
    "        self.distance = W7utils.euclidean_distance\n",
    "        # this version only looks at the single nearest neighbour\n",
    "        self.K=1\n",
    "        \n",
    "        #just affects prints to screen\n",
    "        self.verbose= verbose\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        # ask the data how big it is and store that info\n",
    "        self.numTrainingItems = X.shape[0]\n",
    "        self.numFeatures = X.shape[1]\n",
    "        # store a copy of the data (X) and the labels (y)\n",
    "        self.modelX = X\n",
    "        self.modelY = y\n",
    "        self.labelsPresent = np.unique(self.modelY) # list the unique values found in the labels provided\n",
    "        if (self.verbose):\n",
    "            print(f\"There are {self.numTrainingItems} training examples, each described by values for {self.numFeatures} features\")\n",
    "            print(f\"So self.modelX is a 2D array of shape {self.modelX.shape}\")\n",
    "            print(f\"self.modelY is a list with {len(self.modelY)} entries, each being one of these labels {self.labelsPresent}\")\n",
    "        \n",
    "    def predict(self,newItems):\n",
    "        # read how many  newitems there are\n",
    "        numToPredict = newItems.shape[0]\n",
    "        # make an empty list to hold their predicted labels\n",
    "        predictions = np.empty(numToPredict)\n",
    "        \n",
    "        #loop through each new item each one\n",
    "        for item in range(numToPredict):\n",
    "            # predicting its label\n",
    "            thisPrediction = self.predict_new_item ( newItems[item])\n",
    "            # adding that prediction to our list\n",
    "            predictions[item] = thisPrediction\n",
    "        return predictions\n",
    "    \n",
    "    def predict_new_item(self,newItem):\n",
    "        \n",
    "        # Step 1: measure and store distance to each training item\n",
    "        distFromNewItem = np.zeros((self.numTrainingItems)) # array with one entry for each training set item, intialised to zero\n",
    "        for stored_example in range (self.numTrainingItems):\n",
    "            distFromNewItem[stored_example] = self.distance(newItem,  self.modelX[stored_example])\n",
    "  \n",
    "        # Step 2: find the one closest training example: This is K=1, \n",
    "        closest = 0\n",
    "        for stored_example in range (0, self.numTrainingItems):\n",
    "            if  ( distFromNewItem[stored_example] < distFromNewItem[closest] ):\n",
    "                closest=stored_example\n",
    " \n",
    "        # step 3: count the votes - because this is for K=1 so we don't need to take a vote\n",
    "        labelOfClosest = self.modelY[closest]\n",
    "        return labelOfClosest\n",
    "    \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-warning\" style=\"color:black\" >\n",
    "<h2> Activity 2.1</h2>\n",
    "    <b>Run the code provided below</b> for K=1 with the two datasets and make sure you understand the outputs and how they are produced\n",
    "<ul>\n",
    "    <li>For the marks dataset this creates a plot to show a decision surface<br>\n",
    "    (you do not need to understand how the PlotDecisionSurface() methods works)</li>\n",
    "    <li>For the  iris data set this uses a confusion matrix <br> (ask the internet what a confusion matrix is if you're not sure)</li>\n",
    "    </ul>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Iris dataset - illustrating a confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train/test split \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "irisX,irisy = load_iris(return_X_y = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(irisX, irisy, test_size=0.33,stratify=irisy)\n",
    "\n",
    "\n",
    "myKNNmodel = simple_1NN()\n",
    "myKNNmodel.fit(X_train,y_train)\n",
    "y_pred = myKNNmodel.predict(X_test)\n",
    "print(y_pred.T) #.t turns column to row so it sghows onscreen better \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ( (y_test==y_pred))\n",
    "accuracy = 100* ( y_test == y_pred).sum() / y_test.shape[0]\n",
    "print(f\"Overall Accuracy = {accuracy} %\")\n",
    "\n",
    "confusionMatrix = np.zeros((3,3),int)\n",
    "for i in range(50):\n",
    "    actual = int(y_test[i])\n",
    "    predicted = int(y_pred[i])\n",
    "    confusionMatrix[actual][predicted] += 1\n",
    "print(confusionMatrix)\n",
    "\n",
    "#and here's sklearn's built-in method\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred,display_labels= iris_names )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The iris data set - illustrating the decision surface\n",
    "We will only use the two petal features so we can visualise it in 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petals = X_train[:,2:4]\n",
    "myKNNmodel.fit(petals,y_train)\n",
    "y_pred = myKNNmodel.predict(X_test[:,2:4])\n",
    "accuracy = 100* ( y_test == y_pred).sum() / y_test.shape[0]\n",
    "print(f\"Overall Accuracy in 2D = {accuracy} %\")\n",
    "\n",
    "title= \"1-Nearest Neighbour on petal features\"\n",
    "W7utils.PlotDecisionSurface(petals,y_train,myKNNmodel, title, iris_features[2:4],stepSize= 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"color:black\" >\n",
    "<h2> Activity 2.3: Create your own implementation of K-Nearest Neighbours</h2>\n",
    "    Using the code above,  extend the predict method for the class simple_1NN  to use the votes from K>1 neighbours.\n",
    "\n",
    "\n",
    "<ul>\n",
    "    <li>I have started you off by creating an empty class called Simple_KNN</li>\n",
    "    <li> Then I  copied in the pseudo-code as comments starting ##</li>\n",
    "    <li>I have also helped you by copying in the code from the simple_1NN class into the relevant places</li>\n",
    "    <li> I have marked everywhere you need to write code with #==></li>\n",
    "    <li> <b>it is a total of 18 lines, most are very obvious</b></li>\n",
    "    </ul>\n",
    "    <p><b> It's often helpful to put in some print() statements to show what is going on as you develop your code</b><br>\n",
    "        And if you can write your code  so that it runs in 'partially completed' state then you can build it up in bits.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode for KNearest Neighbours\n",
    "**init()**  :  \n",
    "SPECIFY function to calculate distance metric d(i,j) for any two items *i* and *j*     \n",
    "  e.g. Euclidean (continuous variables) or Hamming (categorical)  \n",
    "SET value of K\n",
    "\n",
    "**fit(trainingData)** :  \n",
    "\n",
    "SET numExemplars = READ(number of rows in training data)  \n",
    "SET numFeatures = READ(number of columns in training data) \n",
    "\n",
    "*#Just store a local copy of the training data as two arrays:*   \n",
    "CREATE_AND_FILL(X_train of shape (numExemplars , numFeatures)).     \n",
    "CREATE_AND_FILL(y_train of shape( numExemplars))\n",
    "  \n",
    "**predict(newItems)** :  \n",
    "SET numToPredict = READ(number of rows in newItems)  \n",
    "SET predictions = CREATE_EMPTYARRAY( numToPredict)\n",
    " \n",
    "FOREACH item in (0...,numToPredict-1)    \n",
    "...SET predictions[item] = predictNewItem ( newItems[item]) \n",
    " \n",
    "RETURN predictions  \n",
    "\n",
    "\n",
    "**predictNewItem(newItem)**:\n",
    "\n",
    "*Step 1:   Make 1D array distances from newItem to each trainig set item*   \n",
    "FOREACH exemplar in (0,...,numExemplars -1  \n",
    "...SET distFromNewItem [exemplar] = d (newItem , X_train[exemplar] )   \n",
    "\n",
    "*Step 2: Get indexes of the k nearest neighbours for our new item*        \n",
    "SET closestK = GET_IDS_OF_K_CLOSEST(K,distFromNewItem)\n",
    " \n",
    "  \n",
    "*Step 3: Calculate most popular of the m possible labels*     \n",
    "SET labelcounts = CREATE(1D array with m zero values)  \n",
    "\n",
    "FOREACH  k in (0,...K-1)   \n",
    "... SET thisindex = closestK[k]   \n",
    "... SET thislabel = y_train[thisindex]  \n",
    "... INCREMENT labelcounts[thislabel]  \n",
    "\n",
    "SET thisPrediction = READ(index of labelcounts with highest value)    \n",
    "\n",
    "RETURN thisPrediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get_ids_of_k_closest(distFromNewItem, K):**\n",
    "\n",
    "SET closestK= CREATE(1D array with K values)  \n",
    "SET arraySize = len(distFromNewItem)  \n",
    "\n",
    "FOR k in (0,...,K-1)  \n",
    "... SET thisClosest=0  \n",
    "... FOR exemplar in (0,...,arraySize -1)  \n",
    "......IF ( distFromNewItem[exemplar] < distFromNewItem[thisClosest]  )  \n",
    "......... SET thisClosest = exemplar  \n",
    "... SET closestK[k] = thisClosest # store this id  \n",
    "... SET distFromNewItem[thisClosest] = BigNumber # so we don't pick it again in next loop\n",
    "\n",
    "RETURN closestK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your KNN class code here\n",
    "\n",
    "class simple_KNN:\n",
    "\n",
    "    def __init__(self, verbose = True):\n",
    "        \"\"\"init function, Needs adapting to take an argument K with default 1\"\"\"\n",
    "        \n",
    "        ## SPECIFY function to calculate distance metric d(i,j) for any two items *i* and *j*\n",
    "        self.distance= W7utils.euclidean_distance\n",
    "        ## SET value of K\n",
    "        #===> change line below to take K from an argument to this init() method <====\n",
    "        self.K=1\n",
    "        \n",
    "        #just affects prints to screen\n",
    "        self.verbose= verbose     \n",
    "\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        \"\"\"stores the dataset values X and labels y. Same code as 1-NN\"\"\"\n",
    "        \n",
    "        ##SET numExemplars = READ(number of rows in training data)  \n",
    "        self.numTrainingItems = X.shape[0]\n",
    "        \n",
    "        ##SET numFeatures = READ(number of columns in training data) \n",
    "        self.numFeatures = X.shape[1]\n",
    "        \n",
    "        # Just store a local copy of the training data as two arrays:*   \n",
    "        ## CREATE_AND_FILL(X_train of shape (numExemplars , numFeatures)).     \n",
    "        self.modelX = X\n",
    "        ## CREATE_AND_FILL(y_train of shape( numExemplars))\n",
    "        self.modelY = y\n",
    "        \n",
    "        #additional reporting -  not part of algorithm\n",
    "        self.labelsPresent = np.unique(self.modelY) # list the unique values found in the labels provided\n",
    "        if (self.verbose):\n",
    "            print(f\"There are {self.numTrainingItems} training examples, each described by values for {self.numFeatures} features\")\n",
    "            print(f\"So self.modelX is a 2D array of shape {self.modelX.shape}\")\n",
    "            print(f\"self.modelY is a list with {len(self.modelY)} entries, each being one of these labels {self.labelsPresent}\")\n",
    "\n",
    "\n",
    "  \n",
    "      def predict(self,newItems):\n",
    "        \"\"\" make a prediction for each new item - same code as 1-NN\"\"\"\n",
    "        \n",
    "        ## SET numToPredict = READ(number of rows in newItems) \n",
    "        numToPredict = newItems.shape[0]\n",
    "        \n",
    "        ## SET predictions = CREATE_EMPTYARRAY( numToPredict)\n",
    "        predictions = np.empty(numToPredict)\n",
    "        \n",
    "        ##FOREACH item in (0...,numToPredict-1) \n",
    "        for item in range(numToPredict):\n",
    "        \n",
    "            ##...SET predictions[item] = predictNewItem ( newItems[item]) \n",
    "            thisPrediction = self.predict_new_item ( newItems[item])\n",
    "            predictions[item] = thisPrediction\n",
    "            \n",
    "            \n",
    "        ## RETURN predictions    \n",
    "        return predictions:  \n",
    " \n",
    "\n",
    " \n",
    "    def predict_new_item(self,newItem):\n",
    "        \"\"\"make prediction for single item. Step 1 is same as 1-NN steps 2 and 3 need writing\"\"\"\n",
    "\n",
    "        ## Step 1:   \n",
    "        ## Make 1D array distances from newItem to each training set item*   \n",
    "        distFromNewItem = np.zeros((self.numTrainingItems)) \n",
    "\n",
    "        ## FOREACH exemplar in (0,...,numExemplars -1  \n",
    "        for stored_example in range (self.numTrainingItems):\n",
    "            ## ...SET distFromNewItem [exemplar] = d (newItem , X_train[exemplar] )   \n",
    "            distFromNewItem[stored_example] = self.distance(newItem,  self.modelX[stored_example])\n",
    "  \n",
    "\n",
    "        ## Step 2: Get indexes of the k nearest neighbours for our new item    \n",
    "    \n",
    "        ## SET closestK = GET_IDS_OF_K_CLOSEST(K,distFromNewItem)\n",
    "        #closestK is array with K elements  \n",
    "        #===> add one line of  code  to call the new function <===       \n",
    "\n",
    " \n",
    "        ## Step 3: Calculate most popular of the m possible labels* \n",
    "    \n",
    "        ## SET labelcounts = CREATE(1D array with m zero values)  \n",
    "        #==> add one line of code using numpy.zeros to do this.  <===\n",
    "        #remember that in fit() we created self.labelsPresent\n",
    "        # so m = len(self.labelsPresent) \n",
    "      \n",
    "       ##    FOREACH  k in (0,...K-1)  \n",
    "       #==> add line of code putting in a for() loop here <===\n",
    "    \n",
    "           ##... SET thisindex = closestK[k] \n",
    "           #==> add line of code to do this\n",
    "            \n",
    "           ##... SET thislabel = y_train[thisindex]  \n",
    "           #==> add line of code to do this\n",
    "            \n",
    "           ##... INCREMENT labelcounts[thislabel] \n",
    "           #==> add line of code to do this\n",
    "\n",
    "           ##SET thisPrediction = READ(index of labelcounts with highest value)    \n",
    "           #==> add one or two lines of code to do this\n",
    "           # suggest you google \"python most frequent value in numpy array\" \n",
    "        \n",
    "    ##RETURN thisPrediction   \n",
    "    return thisPrediction\n",
    "    \n",
    "    \n",
    "  \n",
    "                \n",
    "                \n",
    "def get_ids_of_k_closest(np.array:distFromNewItem,int: K)->np.array:\n",
    "    \"\"\"new function that returns array containing indexes of K closest items\"\"\"\n",
    "    \n",
    "    # Several way of doing this.  \n",
    "    #This one just does K iterations of the loop from 1-NN that found the sigble closest \n",
    "\n",
    "    ## SET closestK= CREATE(1D array with K values) \n",
    "    #==> add line of code to do this using np.empty(k,dtype=int)  <==\n",
    "\n",
    "    ##SET arraySize = len(distFromNewItem)  \n",
    "    #==> add line of code to do this, \n",
    "    #distFromNewItem is a numpy array so you use its .shape[0] attribute <===\n",
    "    \n",
    "    ## FOR k in (0,...,K-1)  \n",
    "    #==> add line of code to do this\n",
    "    # look at 1-NN predict_new_item() for inspiration for the contents of this loop\n",
    "\n",
    "        ##... SET thisClosest=0\n",
    "        #==> add line of code to do this\n",
    "    \n",
    "        ##... FOR exemplar in (0,...,arraySize -1) \n",
    "        #==> add line of code to do this\n",
    "\n",
    "            ##......IF ( distFromNewItem[exemplar] < distFromNewItem[thisClosest]  )  \n",
    "            #==> add line of code to do this\n",
    "\n",
    "                ##......... SET thisClosest = exemplar\n",
    "                #==> add line of code to do this\n",
    "\n",
    "                ##... SET closestK[k] = thisClosest  \n",
    "                #===> add line of code to do this\n",
    "                \n",
    "        ##... SET distFromNewItem[thisClosest] = BigNumber \n",
    "        # so we don't pick it again in next loop\n",
    "        #==>add line of code to do this, you could use 100000 for bignum\n",
    "\n",
    "    ##RETURN closestK\n",
    "    #==> add line of code to do this\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"color:black\">\n",
    "<h2> Activity 2.4: Test your implementation on the iris dataset</h2>\n",
    "Use the toolbar to copy and paste the two cells from activity 2.1 below here. <br>\n",
    "Then edit them so that they create and use objects of your new class, instead of the class simple_1NN\n",
    "\n",
    "Start with K=1 - this should produce the same results as you got in activity 2.1, then try with K = {3,5,7}\n",
    "<ul>\n",
    "    <li>Make  <b>qualititative</b> judgements : how does the decision surface change?</li>\n",
    "    <li>Make <b>quantitative</b> judgements :  how does the confusion matrix change?</li>\n",
    "    </ul>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"color:black\" >\n",
    "<h1> Activity 3: Decision Trees</h1></div>\n",
    "\n",
    "In the lecture notebook we illustrated how the decision tree is created by a process of expanding nodes.\n",
    "\n",
    "We often want to control how we learn a model (in this case, grow a tree) h to avoid a phenomenon call **over-fitting**.\n",
    "\n",
    "- This is where the model is capturing fine-details of the training set and so failing to generalise from the training set to the real world.\n",
    "- like in the images where all the dogs faced left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"color:black\">\n",
    "<h2> Activity 3.1: exploring how to control tree-growth to prevent over-fitting</h2>\n",
    "The aim of this activity is for you to experiment with what happens when you change three parameters that affect how big and complex the tree is allowed to get.\n",
    "<ul>\n",
    "    <li> max_depth</li>\n",
    "    <li>min_samples_split, (default value is 2)</li>\n",
    "    <li>min_samples_leaf, (default value is 1)</li>\n",
    "    </ul>\n",
    "\n",
    "\n",
    "Experiment with the Iris data set below to see if you can work out what each of these parameters does, and how it affects the tree \n",
    "<ul>\n",
    "<li> Each time you run the  cell below, it will give you a different train-test split of the Iris data.<br>\n",
    "    Does this affect what tree you get? </li>\n",
    "    <li> Is there a combination of values that means you consistently get similar trees?</li>\n",
    "    <li>    What is a good way of judging 'similarity?</li>\n",
    "    </ul>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "# load iris dataset and split into train:test\n",
    "iris = sklearn.datasets.load_iris()\n",
    "irisX = iris.data\n",
    "irisy = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(irisX, irisy, test_size=0.33,stratify=irisy)\n",
    "\n",
    "\n",
    "\n",
    "## Experiment with changing these values\n",
    "depth= 1 #  try 2,3,4,5\n",
    "minsplit = 2 #try 3,4,5\n",
    "minleaf=1 #try 3,4,5\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=1234, max_depth=depth,min_samples_split=minsplit,min_samples_leaf=minleaf)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "CMPlot=ConfusionMatrixDisplay.from_predictions(y_test,y_pred, display_labels=iris_names)\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "_ = tree.plot_tree(model, feature_names=iris.feature_names,  class_names=iris.target_names, filled=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"color:black\" > <h1> Activity 4: (stretch)</h1></div>\n",
    "Using the code from last week,  apply a StandardScaler to the Iris data set and evaluate the effect this has on the accuracy.\n",
    "\n",
    "Because there is a random element in how  the data set is split into training / test split,  it is not valid just to split the data once then compare the results with / without scaling.\n",
    "\n",
    "Instead  you will need to do ten repeats  of:\n",
    "- Use the sklearn method to split the data into 66:34 train/test sets\n",
    "- Construct,  train, and test,  an instance of your kNN model on the unscaled data and store its accuracy \n",
    "- Create an instance of the standard scaler and then:\n",
    "  - call its fit() method to set its parameters from the training set.\n",
    "  - call its transform() method for both the traing and test sets\n",
    "  - Construct,  train, and test,  an instance of your kNN model on this scaled data and store its accuracy \n",
    "\n",
    "That should gives you ten pairs of values (one per repeat) for the scaled and raw data accuracy.  \n",
    "Use an online statistical tool (e.g. https://www.graphpad.com/quickcalcs/ttest1.cfm) that lets you copy your data in the perform a 'paired t-test\" to find out the probability that normalising the data improves prediction accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\"> Please save your work (click the save icon) then shutdown the notebook when you have finished with this tutorial (menu->file->close and shutdown notebook</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"> Remember to download and save your work if you are not running this notebook locally.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
